{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 5:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1014, 1: 1014, 2: 952, 3: 1016, 4: 997, 5: 1025, 6: 980, 7: 977, 8: 1003, 9: 1022}\n",
      "First 20 Labels: [1, 8, 5, 1, 5, 7, 4, 3, 8, 2, 7, 2, 0, 1, 5, 9, 6, 2, 0, 8]\n",
      "\n",
      "Example of Image 1000:\n",
      "Image - Min Value: 28 Max Value: 228\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 7 Name: horse\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLN7C3VdJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x4d0f240>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 5\n",
    "sample_id = 1000\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    x_min = x.min(axis=(1, 2), keepdims=True)\n",
    "    x_max = x.max(axis=(1, 2), keepdims=True)\n",
    "    \n",
    "    x = (x - x_min)/(x_max-x_min)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    x = np.array(x)\n",
    "    # set the number of rows and columns in the new array\n",
    "    n_row = x.size\n",
    "    n_col = 10\n",
    "    # make a new array full of zeros\n",
    "    ohe = np.zeros((n_row, n_col))\n",
    "    # the ones columns are just at the given locations\n",
    "    ohe[np.arange(n_row), x] = 1\n",
    "    return ohe\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    "If you're finding it hard to dedicate enough time for this course a week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) to build each layer, except \"Convolutional & Max Pooling\" layer.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    "If you would like to get the most of this course, try to solve all the problems without TF Layers.  Let's begin!\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    image_shape = list(image_shape)\n",
    "    shape = [None] + image_shape\n",
    "    return tf.placeholder(tf.float32, shape=shape, name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    shape = [None, n_classes]\n",
    "    return tf.placeholder(tf.float32, shape=shape, name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    shape = None\n",
    "    return tf.placeholder(tf.float32, shape=shape, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "Note: You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer.  You're free to use any TensorFlow package for all the other layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # input shape\n",
    "    in_shape = x_tensor.get_shape().as_list()\n",
    "    # set up filter weights and bias\n",
    "    filter_W = tf.Variable(tf.truncated_normal((conv_ksize[0], \n",
    "                                                conv_ksize[1], \n",
    "                                                in_shape[3], \n",
    "                                                conv_num_outputs), \n",
    "                                               stddev=.05))\n",
    "    filter_b = tf.Variable(tf.zeros(conv_num_outputs,))\n",
    "    # set up filter strides and padding\n",
    "    strides = [1, conv_strides[0], conv_strides[1], 1]\n",
    "    padding = 'SAME'\n",
    "    \n",
    "    # convolutional layer\n",
    "    conv_layer = tf.nn.relu(tf.nn.conv2d(x_tensor, filter_W, strides, padding) + filter_b)\n",
    "    # max pooling layer\n",
    "    ksize = [1, pool_ksize[0], pool_ksize[1], 1]\n",
    "    stride = [1, pool_strides[0], pool_strides[1], 1]\n",
    "    max_layer = tf.nn.max_pool(conv_layer, ksize, stride, padding)\n",
    "    \n",
    "    return max_layer \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). You can use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    shape = x_tensor.get_shape().as_list()\n",
    "    img_size_flatten =   shape[1] * shape[2] * shape[3]\n",
    "    \n",
    "    return tf.reshape(x_tensor, (-1, img_size_flatten))\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). You can use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    shape = x_tensor.get_shape().as_list()\n",
    "    weight = tf.Variable(tf.random_normal([shape[1], num_outputs]))\n",
    "    bias = tf.Variable(tf.random_normal([num_outputs]))\n",
    "    \n",
    "    fc = tf.add(tf.matmul(x_tensor, weight), bias)\n",
    "    fc = tf.nn.relu(fc)\n",
    "    \n",
    "    return fc\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). You can use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer.\n",
    "\n",
    "Note: Activation, softmax, or cross entropy shouldn't be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    shape = x_tensor.get_shape().as_list()\n",
    "    weight = tf.Variable(tf.random_normal([shape[1], num_outputs]))\n",
    "    bias = tf.Variable(tf.random_normal([num_outputs]))\n",
    "    \n",
    "    output = tf.add(tf.matmul(x_tensor, weight), bias)\n",
    "    \n",
    "    return output\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv = conv2d_maxpool(x_tensor=x, \n",
    "                          conv_num_outputs=32, \n",
    "                          conv_ksize=(2, 2), \n",
    "                          conv_strides=(1, 1),\n",
    "                          pool_ksize=(2, 2), \n",
    "                          pool_strides=(2, 2))\n",
    "\n",
    "    \n",
    "    # second convolutional layer\n",
    "    conv = conv2d_maxpool(x_tensor=conv, \n",
    "                          conv_num_outputs=64, \n",
    "                          conv_ksize=(2, 2), \n",
    "                          conv_strides=(1, 1),\n",
    "                          pool_ksize=(2, 2), \n",
    "                          pool_strides=(2, 2))\n",
    "    \n",
    "    # third convolutional layer\n",
    "    #conv = conv2d_maxpool(x_tensor=x, \n",
    "    #                      conv_num_outputs=128, \n",
    "    #                      conv_ksize=(2, 2), \n",
    "    #                      conv_strides=(1, 1),\n",
    "    #                      pool_ksize=(2, 2), \n",
    "    #                      pool_strides=(2, 2))\n",
    "    \n",
    "    # dropout layer\n",
    "    conv = tf.nn.dropout(x=conv,\n",
    "                         keep_prob=keep_prob)\n",
    "    \n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    flat = flatten(conv)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    fc = fully_conn(x_tensor=flat, \n",
    "                    num_outputs=128)\n",
    "    \n",
    "    # dropout layer\n",
    "    #fc = tf.nn.dropout(x=fc, \n",
    "    #                   keep_prob=keep_prob)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    out = output(x_tensor=fc, \n",
    "                 num_outputs=10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, \n",
    "                                      y: label_batch, \n",
    "                                      keep_prob: keep_probability})\n",
    "    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    \n",
    "    loss, accuracy = session.run([cost, accuracy],\n",
    "                                 feed_dict={x: valid_features,\n",
    "                                            y: valid_labels,\n",
    "                                            keep_prob: 1.0})\n",
    "    print('loss: ' + str(loss) + '   ' + 'accuracy: ' + str(accuracy))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 256\n",
    "keep_probability = .6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss: 2.34285   accuracy: 0.169\n",
      "\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss: 2.16566   accuracy: 0.212\n",
      "\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss: 2.08918   accuracy: 0.2584\n",
      "\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss: 2.02726   accuracy: 0.2806\n",
      "\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss: 2.0033   accuracy: 0.2886\n",
      "\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss: 2.06393   accuracy: 0.2858\n",
      "\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss: 1.98867   accuracy: 0.3118\n",
      "\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss: 1.84719   accuracy: 0.3554\n",
      "\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss: 1.82681   accuracy: 0.3628\n",
      "\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss: 1.79915   accuracy: 0.3678\n",
      "\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss: 1.82555   accuracy: 0.359\n",
      "\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss: 1.74635   accuracy: 0.3922\n",
      "\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss: 1.73113   accuracy: 0.3914\n",
      "\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss: 1.67934   accuracy: 0.4216\n",
      "\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss: 1.67168   accuracy: 0.411\n",
      "\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss: 1.63255   accuracy: 0.426\n",
      "\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss: 1.62498   accuracy: 0.4342\n",
      "\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss: 1.62436   accuracy: 0.432\n",
      "\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss: 1.60518   accuracy: 0.4342\n",
      "\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss: 1.57583   accuracy: 0.4422\n",
      "\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss: 1.55235   accuracy: 0.4494\n",
      "\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss: 1.54333   accuracy: 0.4592\n",
      "\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss: 1.61149   accuracy: 0.4366\n",
      "\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss: 1.54123   accuracy: 0.462\n",
      "\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss: 1.53154   accuracy: 0.4622\n",
      "\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss: 1.5254   accuracy: 0.4558\n",
      "\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss: 1.58627   accuracy: 0.4452\n",
      "\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss: 1.48936   accuracy: 0.4804\n",
      "\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss: 1.47498   accuracy: 0.4832\n",
      "\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss: 1.47324   accuracy: 0.4756\n",
      "\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss: 1.46308   accuracy: 0.482\n",
      "\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss: 1.40473   accuracy: 0.5124\n",
      "\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss: 1.39519   accuracy: 0.5168\n",
      "\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss: 1.37874   accuracy: 0.5162\n",
      "\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss: 1.4331   accuracy: 0.4958\n",
      "\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss: 1.39449   accuracy: 0.506\n",
      "\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss: 1.37741   accuracy: 0.5188\n",
      "\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss: 1.36053   accuracy: 0.5222\n",
      "\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss: 1.36294   accuracy: 0.5206\n",
      "\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss: 1.34016   accuracy: 0.5354\n",
      "\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss: 1.36006   accuracy: 0.5242\n",
      "\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss: 1.3379   accuracy: 0.5384\n",
      "\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss: 1.34086   accuracy: 0.5386\n",
      "\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss: 1.34716   accuracy: 0.5306\n",
      "\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss: 1.3533   accuracy: 0.53\n",
      "\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss: 1.33636   accuracy: 0.5326\n",
      "\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss: 1.31124   accuracy: 0.55\n",
      "\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss: 1.30703   accuracy: 0.5448\n",
      "\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss: 1.38759   accuracy: 0.522\n",
      "\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss: 1.31484   accuracy: 0.5496\n",
      "\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss: 1.36457   accuracy: 0.5334\n",
      "\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss: 1.30452   accuracy: 0.5462\n",
      "\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss: 1.35719   accuracy: 0.5406\n",
      "\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss: 1.2798   accuracy: 0.5578\n",
      "\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss: 1.30264   accuracy: 0.5484\n",
      "\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss: 1.28359   accuracy: 0.5568\n",
      "\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss: 1.28725   accuracy: 0.558\n",
      "\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss: 1.30513   accuracy: 0.5526\n",
      "\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss: 1.29592   accuracy: 0.5626\n",
      "\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss: 1.2631   accuracy: 0.5684\n",
      "\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss: 1.29197   accuracy: 0.5578\n",
      "\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss: 1.30411   accuracy: 0.5578\n",
      "\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss: 1.30335   accuracy: 0.5588\n",
      "\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss: 1.2423   accuracy: 0.575\n",
      "\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss: 1.27601   accuracy: 0.563\n",
      "\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss: 1.28803   accuracy: 0.5624\n",
      "\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss: 1.28933   accuracy: 0.565\n",
      "\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss: 1.25186   accuracy: 0.5786\n",
      "\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss: 1.24665   accuracy: 0.575\n",
      "\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss: 1.24184   accuracy: 0.5768\n",
      "\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss: 1.26347   accuracy: 0.582\n",
      "\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss: 1.24471   accuracy: 0.5848\n",
      "\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss: 1.2635   accuracy: 0.5826\n",
      "\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss: 1.24187   accuracy: 0.5824\n",
      "\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss: 1.23391   accuracy: 0.5884\n",
      "\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss: 1.2167   accuracy: 0.5906\n",
      "\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss: 1.23302   accuracy: 0.5926\n",
      "\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss: 1.26007   accuracy: 0.59\n",
      "\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss: 1.25601   accuracy: 0.5836\n",
      "\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss: 1.2342   accuracy: 0.5904\n",
      "\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss: 1.23621   accuracy: 0.5874\n",
      "\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss: 1.21302   accuracy: 0.5952\n",
      "\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss: 1.25234   accuracy: 0.5834\n",
      "\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss: 1.21978   accuracy: 0.596\n",
      "\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss: 1.22196   accuracy: 0.5944\n",
      "\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss: 1.23063   accuracy: 0.5922\n",
      "\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss: 1.23657   accuracy: 0.5938\n",
      "\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss: 1.23293   accuracy: 0.5966\n",
      "\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss: 1.24655   accuracy: 0.5856\n",
      "\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss: 1.26148   accuracy: 0.5884\n",
      "\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss: 1.2361   accuracy: 0.5922\n",
      "\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss: 1.27111   accuracy: 0.5878\n",
      "\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss: 1.29734   accuracy: 0.5828\n",
      "\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss: 1.22781   accuracy: 0.595\n",
      "\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss: 1.26336   accuracy: 0.5878\n",
      "\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss: 1.25485   accuracy: 0.5874\n",
      "\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss: 1.23822   accuracy: 0.592\n",
      "\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss: 1.25091   accuracy: 0.5894\n",
      "\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss: 1.25659   accuracy: 0.5908\n",
      "\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss: 1.24612   accuracy: 0.5908\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss: 2.59002   accuracy: 0.145\n",
      "\n",
      "Epoch  1, CIFAR-10 Batch 2:  loss: 2.25345   accuracy: 0.207\n",
      "\n",
      "Epoch  1, CIFAR-10 Batch 3:  loss: 2.13035   accuracy: 0.2412\n",
      "\n",
      "Epoch  1, CIFAR-10 Batch 4:  loss: 2.02855   accuracy: 0.2788\n",
      "\n",
      "Epoch  1, CIFAR-10 Batch 5:  loss: 2.03273   accuracy: 0.2766\n",
      "\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss: 1.96105   accuracy: 0.3\n",
      "\n",
      "Epoch  2, CIFAR-10 Batch 2:  loss: 1.91007   accuracy: 0.336\n",
      "\n",
      "Epoch  2, CIFAR-10 Batch 3:  loss: 1.89675   accuracy: 0.3344\n",
      "\n",
      "Epoch  2, CIFAR-10 Batch 4:  loss: 1.86598   accuracy: 0.3324\n",
      "\n",
      "Epoch  2, CIFAR-10 Batch 5:  loss: 1.81071   accuracy: 0.3714\n",
      "\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss: 1.76896   accuracy: 0.3846\n",
      "\n",
      "Epoch  3, CIFAR-10 Batch 2:  loss: 1.75299   accuracy: 0.385\n",
      "\n",
      "Epoch  3, CIFAR-10 Batch 3:  loss: 1.71122   accuracy: 0.3884\n",
      "\n",
      "Epoch  3, CIFAR-10 Batch 4:  loss: 1.71852   accuracy: 0.381\n",
      "\n",
      "Epoch  3, CIFAR-10 Batch 5:  loss: 1.64276   accuracy: 0.4188\n",
      "\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss: 1.65453   accuracy: 0.412\n",
      "\n",
      "Epoch  4, CIFAR-10 Batch 2:  loss: 1.62236   accuracy: 0.418\n",
      "\n",
      "Epoch  4, CIFAR-10 Batch 3:  loss: 1.61177   accuracy: 0.4202\n",
      "\n",
      "Epoch  4, CIFAR-10 Batch 4:  loss: 1.57351   accuracy: 0.441\n",
      "\n",
      "Epoch  4, CIFAR-10 Batch 5:  loss: 1.54696   accuracy: 0.453\n",
      "\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss: 1.5683   accuracy: 0.446\n",
      "\n",
      "Epoch  5, CIFAR-10 Batch 2:  loss: 1.55725   accuracy: 0.4462\n",
      "\n",
      "Epoch  5, CIFAR-10 Batch 3:  loss: 1.52557   accuracy: 0.464\n",
      "\n",
      "Epoch  5, CIFAR-10 Batch 4:  loss: 1.52062   accuracy: 0.4576\n",
      "\n",
      "Epoch  5, CIFAR-10 Batch 5:  loss: 1.49021   accuracy: 0.4766\n",
      "\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss: 1.46984   accuracy: 0.4848\n",
      "\n",
      "Epoch  6, CIFAR-10 Batch 2:  loss: 1.49072   accuracy: 0.467\n",
      "\n",
      "Epoch  6, CIFAR-10 Batch 3:  loss: 1.48091   accuracy: 0.4716\n",
      "\n",
      "Epoch  6, CIFAR-10 Batch 4:  loss: 1.42176   accuracy: 0.501\n",
      "\n",
      "Epoch  6, CIFAR-10 Batch 5:  loss: 1.41792   accuracy: 0.497\n",
      "\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss: 1.42073   accuracy: 0.4976\n",
      "\n",
      "Epoch  7, CIFAR-10 Batch 2:  loss: 1.47102   accuracy: 0.4752\n",
      "\n",
      "Epoch  7, CIFAR-10 Batch 3:  loss: 1.41563   accuracy: 0.5012\n",
      "\n",
      "Epoch  7, CIFAR-10 Batch 4:  loss: 1.37769   accuracy: 0.518\n",
      "\n",
      "Epoch  7, CIFAR-10 Batch 5:  loss: 1.37146   accuracy: 0.5114\n",
      "\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss: 1.35643   accuracy: 0.5246\n",
      "\n",
      "Epoch  8, CIFAR-10 Batch 2:  loss: 1.40519   accuracy: 0.4934\n",
      "\n",
      "Epoch  8, CIFAR-10 Batch 3:  loss: 1.37138   accuracy: 0.5202\n",
      "\n",
      "Epoch  8, CIFAR-10 Batch 4:  loss: 1.35026   accuracy: 0.5298\n",
      "\n",
      "Epoch  8, CIFAR-10 Batch 5:  loss: 1.33968   accuracy: 0.5294\n",
      "\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss: 1.35392   accuracy: 0.5188\n",
      "\n",
      "Epoch  9, CIFAR-10 Batch 2:  loss: 1.34221   accuracy: 0.5166\n",
      "\n",
      "Epoch  9, CIFAR-10 Batch 3:  loss: 1.31723   accuracy: 0.5394\n",
      "\n",
      "Epoch  9, CIFAR-10 Batch 4:  loss: 1.29987   accuracy: 0.551\n",
      "\n",
      "Epoch  9, CIFAR-10 Batch 5:  loss: 1.28055   accuracy: 0.552\n",
      "\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss: 1.31883   accuracy: 0.536\n",
      "\n",
      "Epoch 10, CIFAR-10 Batch 2:  loss: 1.32348   accuracy: 0.5236\n",
      "\n",
      "Epoch 10, CIFAR-10 Batch 3:  loss: 1.30694   accuracy: 0.5464\n",
      "\n",
      "Epoch 10, CIFAR-10 Batch 4:  loss: 1.27515   accuracy: 0.5576\n",
      "\n",
      "Epoch 10, CIFAR-10 Batch 5:  loss: 1.29249   accuracy: 0.5498\n",
      "\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss: 1.28232   accuracy: 0.557\n",
      "\n",
      "Epoch 11, CIFAR-10 Batch 2:  loss: 1.29604   accuracy: 0.5402\n",
      "\n",
      "Epoch 11, CIFAR-10 Batch 3:  loss: 1.26625   accuracy: 0.5644\n",
      "\n",
      "Epoch 11, CIFAR-10 Batch 4:  loss: 1.23356   accuracy: 0.571\n",
      "\n",
      "Epoch 11, CIFAR-10 Batch 5:  loss: 1.27324   accuracy: 0.5538\n",
      "\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss: 1.25359   accuracy: 0.5636\n",
      "\n",
      "Epoch 12, CIFAR-10 Batch 2:  loss: 1.29002   accuracy: 0.5412\n",
      "\n",
      "Epoch 12, CIFAR-10 Batch 3:  loss: 1.24489   accuracy: 0.5724\n",
      "\n",
      "Epoch 12, CIFAR-10 Batch 4:  loss: 1.22284   accuracy: 0.573\n",
      "\n",
      "Epoch 12, CIFAR-10 Batch 5:  loss: 1.22295   accuracy: 0.5732\n",
      "\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss: 1.23275   accuracy: 0.5748\n",
      "\n",
      "Epoch 13, CIFAR-10 Batch 2:  loss: 1.30218   accuracy: 0.5436\n",
      "\n",
      "Epoch 13, CIFAR-10 Batch 3:  loss: 1.22118   accuracy: 0.5762\n",
      "\n",
      "Epoch 13, CIFAR-10 Batch 4:  loss: 1.19358   accuracy: 0.5886\n",
      "\n",
      "Epoch 13, CIFAR-10 Batch 5:  loss: 1.19474   accuracy: 0.5804\n",
      "\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss: 1.23517   accuracy: 0.5708\n",
      "\n",
      "Epoch 14, CIFAR-10 Batch 2:  loss: 1.23324   accuracy: 0.562\n",
      "\n",
      "Epoch 14, CIFAR-10 Batch 3:  loss: 1.18614   accuracy: 0.5938\n",
      "\n",
      "Epoch 14, CIFAR-10 Batch 4:  loss: 1.19146   accuracy: 0.5852\n",
      "\n",
      "Epoch 14, CIFAR-10 Batch 5:  loss: 1.18349   accuracy: 0.592\n",
      "\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss: 1.18839   accuracy: 0.5854\n",
      "\n",
      "Epoch 15, CIFAR-10 Batch 2:  loss: 1.27754   accuracy: 0.5442\n",
      "\n",
      "Epoch 15, CIFAR-10 Batch 3:  loss: 1.18033   accuracy: 0.596\n",
      "\n",
      "Epoch 15, CIFAR-10 Batch 4:  loss: 1.16889   accuracy: 0.597\n",
      "\n",
      "Epoch 15, CIFAR-10 Batch 5:  loss: 1.16932   accuracy: 0.5964\n",
      "\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss: 1.17661   accuracy: 0.5946\n",
      "\n",
      "Epoch 16, CIFAR-10 Batch 2:  loss: 1.18046   accuracy: 0.5894\n",
      "\n",
      "Epoch 16, CIFAR-10 Batch 3:  loss: 1.16113   accuracy: 0.5992\n",
      "\n",
      "Epoch 16, CIFAR-10 Batch 4:  loss: 1.15406   accuracy: 0.6068\n",
      "\n",
      "Epoch 16, CIFAR-10 Batch 5:  loss: 1.14822   accuracy: 0.6066\n",
      "\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss: 1.17459   accuracy: 0.5948\n",
      "\n",
      "Epoch 17, CIFAR-10 Batch 2:  loss: 1.16343   accuracy: 0.5872\n",
      "\n",
      "Epoch 17, CIFAR-10 Batch 3:  loss: 1.14858   accuracy: 0.6066\n",
      "\n",
      "Epoch 17, CIFAR-10 Batch 4:  loss: 1.15787   accuracy: 0.5964\n",
      "\n",
      "Epoch 17, CIFAR-10 Batch 5:  loss: 1.14658   accuracy: 0.6002\n",
      "\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss: 1.13602   accuracy: 0.6074\n",
      "\n",
      "Epoch 18, CIFAR-10 Batch 2:  loss: 1.19602   accuracy: 0.5792\n",
      "\n",
      "Epoch 18, CIFAR-10 Batch 3:  loss: 1.1202   accuracy: 0.6122\n",
      "\n",
      "Epoch 18, CIFAR-10 Batch 4:  loss: 1.15315   accuracy: 0.6062\n",
      "\n",
      "Epoch 18, CIFAR-10 Batch 5:  loss: 1.11086   accuracy: 0.6246\n",
      "\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss: 1.10776   accuracy: 0.6248\n",
      "\n",
      "Epoch 19, CIFAR-10 Batch 2:  loss: 1.16225   accuracy: 0.5952\n",
      "\n",
      "Epoch 19, CIFAR-10 Batch 3:  loss: 1.13037   accuracy: 0.6092\n",
      "\n",
      "Epoch 19, CIFAR-10 Batch 4:  loss: 1.12093   accuracy: 0.6158\n",
      "\n",
      "Epoch 19, CIFAR-10 Batch 5:  loss: 1.09503   accuracy: 0.6272\n",
      "\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss: 1.14371   accuracy: 0.603\n",
      "\n",
      "Epoch 20, CIFAR-10 Batch 2:  loss: 1.14173   accuracy: 0.6012\n",
      "\n",
      "Epoch 20, CIFAR-10 Batch 3:  loss: 1.11916   accuracy: 0.6148\n",
      "\n",
      "Epoch 20, CIFAR-10 Batch 4:  loss: 1.11453   accuracy: 0.6172\n",
      "\n",
      "Epoch 20, CIFAR-10 Batch 5:  loss: 1.10813   accuracy: 0.6226\n",
      "\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss: 1.10086   accuracy: 0.6226\n",
      "\n",
      "Epoch 21, CIFAR-10 Batch 2:  loss: 1.11618   accuracy: 0.6124\n",
      "\n",
      "Epoch 21, CIFAR-10 Batch 3:  loss: 1.08883   accuracy: 0.6252\n",
      "\n",
      "Epoch 21, CIFAR-10 Batch 4:  loss: 1.11676   accuracy: 0.6132\n",
      "\n",
      "Epoch 21, CIFAR-10 Batch 5:  loss: 1.08063   accuracy: 0.6316\n",
      "\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss: 1.10176   accuracy: 0.6188\n",
      "\n",
      "Epoch 22, CIFAR-10 Batch 2:  loss: 1.17623   accuracy: 0.588\n",
      "\n",
      "Epoch 22, CIFAR-10 Batch 3:  loss: 1.128   accuracy: 0.6092\n",
      "\n",
      "Epoch 22, CIFAR-10 Batch 4:  loss: 1.09707   accuracy: 0.6246\n",
      "\n",
      "Epoch 22, CIFAR-10 Batch 5:  loss: 1.06206   accuracy: 0.6362\n",
      "\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss: 1.08644   accuracy: 0.6264\n",
      "\n",
      "Epoch 23, CIFAR-10 Batch 2:  loss: 1.07705   accuracy: 0.6264\n",
      "\n",
      "Epoch 23, CIFAR-10 Batch 3:  loss: 1.06268   accuracy: 0.6256\n",
      "\n",
      "Epoch 23, CIFAR-10 Batch 4:  loss: 1.07064   accuracy: 0.627\n",
      "\n",
      "Epoch 23, CIFAR-10 Batch 5:  loss: 1.08504   accuracy: 0.63\n",
      "\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss: 1.07714   accuracy: 0.6286\n",
      "\n",
      "Epoch 24, CIFAR-10 Batch 2:  loss: 1.08836   accuracy: 0.6246\n",
      "\n",
      "Epoch 24, CIFAR-10 Batch 3:  loss: 1.06256   accuracy: 0.6308\n",
      "\n",
      "Epoch 24, CIFAR-10 Batch 4:  loss: 1.0596   accuracy: 0.6386\n",
      "\n",
      "Epoch 24, CIFAR-10 Batch 5:  loss: 1.06731   accuracy: 0.6338\n",
      "\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss: 1.05254   accuracy: 0.6348\n",
      "\n",
      "Epoch 25, CIFAR-10 Batch 2:  loss: 1.08108   accuracy: 0.619\n",
      "\n",
      "Epoch 25, CIFAR-10 Batch 3:  loss: 1.05086   accuracy: 0.6366\n",
      "\n",
      "Epoch 25, CIFAR-10 Batch 4:  loss: 1.07995   accuracy: 0.6278\n",
      "\n",
      "Epoch 25, CIFAR-10 Batch 5:  loss: 1.08921   accuracy: 0.626\n",
      "\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss: 1.06863   accuracy: 0.6306\n",
      "\n",
      "Epoch 26, CIFAR-10 Batch 2:  loss: 1.07689   accuracy: 0.6226\n",
      "\n",
      "Epoch 26, CIFAR-10 Batch 3:  loss: 1.03583   accuracy: 0.6362\n",
      "\n",
      "Epoch 26, CIFAR-10 Batch 4:  loss: 1.06975   accuracy: 0.6314\n",
      "\n",
      "Epoch 26, CIFAR-10 Batch 5:  loss: 1.06595   accuracy: 0.6372\n",
      "\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss: 1.05745   accuracy: 0.6314\n",
      "\n",
      "Epoch 27, CIFAR-10 Batch 2:  loss: 1.05376   accuracy: 0.6344\n",
      "\n",
      "Epoch 27, CIFAR-10 Batch 3:  loss: 1.04005   accuracy: 0.644\n",
      "\n",
      "Epoch 27, CIFAR-10 Batch 4:  loss: 1.04618   accuracy: 0.636\n",
      "\n",
      "Epoch 27, CIFAR-10 Batch 5:  loss: 1.02666   accuracy: 0.6496\n",
      "\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss: 1.02891   accuracy: 0.6442\n",
      "\n",
      "Epoch 28, CIFAR-10 Batch 2:  loss: 1.0201   accuracy: 0.644\n",
      "\n",
      "Epoch 28, CIFAR-10 Batch 3:  loss: 1.01755   accuracy: 0.643\n",
      "\n",
      "Epoch 28, CIFAR-10 Batch 4:  loss: 1.01856   accuracy: 0.646\n",
      "\n",
      "Epoch 28, CIFAR-10 Batch 5:  loss: 1.0407   accuracy: 0.6438\n",
      "\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss: 1.04615   accuracy: 0.6392\n",
      "\n",
      "Epoch 29, CIFAR-10 Batch 2:  loss: 1.02303   accuracy: 0.6424\n",
      "\n",
      "Epoch 29, CIFAR-10 Batch 3:  loss: 1.02631   accuracy: 0.6418\n",
      "\n",
      "Epoch 29, CIFAR-10 Batch 4:  loss: 1.01381   accuracy: 0.6476\n",
      "\n",
      "Epoch 29, CIFAR-10 Batch 5:  loss: 1.02418   accuracy: 0.657\n",
      "\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss: 1.02609   accuracy: 0.6468\n",
      "\n",
      "Epoch 30, CIFAR-10 Batch 2:  loss: 1.04474   accuracy: 0.6396\n",
      "\n",
      "Epoch 30, CIFAR-10 Batch 3:  loss: 1.00537   accuracy: 0.649\n",
      "\n",
      "Epoch 30, CIFAR-10 Batch 4:  loss: 1.01414   accuracy: 0.6518\n",
      "\n",
      "Epoch 30, CIFAR-10 Batch 5:  loss: 0.994812   accuracy: 0.6602\n",
      "\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss: 1.01097   accuracy: 0.6476\n",
      "\n",
      "Epoch 31, CIFAR-10 Batch 2:  loss: 1.00171   accuracy: 0.652\n",
      "\n",
      "Epoch 31, CIFAR-10 Batch 3:  loss: 1.00742   accuracy: 0.6558\n",
      "\n",
      "Epoch 31, CIFAR-10 Batch 4:  loss: 1.0215   accuracy: 0.648\n",
      "\n",
      "Epoch 31, CIFAR-10 Batch 5:  loss: 1.02934   accuracy: 0.649\n",
      "\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss: 0.997057   accuracy: 0.6536\n",
      "\n",
      "Epoch 32, CIFAR-10 Batch 2:  loss: 1.02705   accuracy: 0.645\n",
      "\n",
      "Epoch 32, CIFAR-10 Batch 3:  loss: 0.996397   accuracy: 0.6528\n",
      "\n",
      "Epoch 32, CIFAR-10 Batch 4:  loss: 1.01342   accuracy: 0.6492\n",
      "\n",
      "Epoch 32, CIFAR-10 Batch 5:  loss: 1.00954   accuracy: 0.655\n",
      "\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss: 1.00953   accuracy: 0.6486\n",
      "\n",
      "Epoch 33, CIFAR-10 Batch 2:  loss: 1.0061   accuracy: 0.6504\n",
      "\n",
      "Epoch 33, CIFAR-10 Batch 3:  loss: 1.02508   accuracy: 0.643\n",
      "\n",
      "Epoch 33, CIFAR-10 Batch 4:  loss: 0.992722   accuracy: 0.6594\n",
      "\n",
      "Epoch 33, CIFAR-10 Batch 5:  loss: 1.00924   accuracy: 0.6594\n",
      "\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss: 1.00846   accuracy: 0.651\n",
      "\n",
      "Epoch 34, CIFAR-10 Batch 2:  loss: 1.01204   accuracy: 0.645\n",
      "\n",
      "Epoch 34, CIFAR-10 Batch 3:  loss: 0.986447   accuracy: 0.663\n",
      "\n",
      "Epoch 34, CIFAR-10 Batch 4:  loss: 1.00057   accuracy: 0.6572\n",
      "\n",
      "Epoch 34, CIFAR-10 Batch 5:  loss: 1.00623   accuracy: 0.6616\n",
      "\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss: 0.989005   accuracy: 0.6558\n",
      "\n",
      "Epoch 35, CIFAR-10 Batch 2:  loss: 1.00997   accuracy: 0.648\n",
      "\n",
      "Epoch 35, CIFAR-10 Batch 3:  loss: 0.982023   accuracy: 0.6672\n",
      "\n",
      "Epoch 35, CIFAR-10 Batch 4:  loss: 0.969315   accuracy: 0.6636\n",
      "\n",
      "Epoch 35, CIFAR-10 Batch 5:  loss: 0.994209   accuracy: 0.6622\n",
      "\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss: 0.996606   accuracy: 0.6582\n",
      "\n",
      "Epoch 36, CIFAR-10 Batch 2:  loss: 0.98537   accuracy: 0.6596\n",
      "\n",
      "Epoch 36, CIFAR-10 Batch 3:  loss: 0.972314   accuracy: 0.6718\n",
      "\n",
      "Epoch 36, CIFAR-10 Batch 4:  loss: 0.997648   accuracy: 0.6578\n",
      "\n",
      "Epoch 36, CIFAR-10 Batch 5:  loss: 1.04652   accuracy: 0.6496\n",
      "\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss: 0.983571   accuracy: 0.6642\n",
      "\n",
      "Epoch 37, CIFAR-10 Batch 2:  loss: 0.979817   accuracy: 0.66\n",
      "\n",
      "Epoch 37, CIFAR-10 Batch 3:  loss: 0.975291   accuracy: 0.6668\n",
      "\n",
      "Epoch 37, CIFAR-10 Batch 4:  loss: 1.01629   accuracy: 0.6532\n",
      "\n",
      "Epoch 37, CIFAR-10 Batch 5:  loss: 0.969949   accuracy: 0.6686\n",
      "\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss: 1.00155   accuracy: 0.656\n",
      "\n",
      "Epoch 38, CIFAR-10 Batch 2:  loss: 0.961686   accuracy: 0.6678\n",
      "\n",
      "Epoch 38, CIFAR-10 Batch 3:  loss: 0.962819   accuracy: 0.6748\n",
      "\n",
      "Epoch 38, CIFAR-10 Batch 4:  loss: 0.985848   accuracy: 0.6584\n",
      "\n",
      "Epoch 38, CIFAR-10 Batch 5:  loss: 0.969291   accuracy: 0.671\n",
      "\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss: 0.958397   accuracy: 0.6704\n",
      "\n",
      "Epoch 39, CIFAR-10 Batch 2:  loss: 1.0078   accuracy: 0.6476\n",
      "\n",
      "Epoch 39, CIFAR-10 Batch 3:  loss: 0.949328   accuracy: 0.6744\n",
      "\n",
      "Epoch 39, CIFAR-10 Batch 4:  loss: 0.968171   accuracy: 0.6658\n",
      "\n",
      "Epoch 39, CIFAR-10 Batch 5:  loss: 0.977646   accuracy: 0.6634\n",
      "\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss: 0.989324   accuracy: 0.6632\n",
      "\n",
      "Epoch 40, CIFAR-10 Batch 2:  loss: 0.954216   accuracy: 0.6698\n",
      "\n",
      "Epoch 40, CIFAR-10 Batch 3:  loss: 0.982136   accuracy: 0.6684\n",
      "\n",
      "Epoch 40, CIFAR-10 Batch 4:  loss: 0.991188   accuracy: 0.6556\n",
      "\n",
      "Epoch 40, CIFAR-10 Batch 5:  loss: 0.971447   accuracy: 0.661\n",
      "\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss: 0.971854   accuracy: 0.6676\n",
      "\n",
      "Epoch 41, CIFAR-10 Batch 2:  loss: 0.975595   accuracy: 0.6604\n",
      "\n",
      "Epoch 41, CIFAR-10 Batch 3:  loss: 0.955587   accuracy: 0.674\n",
      "\n",
      "Epoch 41, CIFAR-10 Batch 4:  loss: 0.973604   accuracy: 0.66\n",
      "\n",
      "Epoch 41, CIFAR-10 Batch 5:  loss: 0.978627   accuracy: 0.6684\n",
      "\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss: 0.965747   accuracy: 0.6676\n",
      "\n",
      "Epoch 42, CIFAR-10 Batch 2:  loss: 0.976448   accuracy: 0.6664\n",
      "\n",
      "Epoch 42, CIFAR-10 Batch 3:  loss: 0.974334   accuracy: 0.6742\n",
      "\n",
      "Epoch 42, CIFAR-10 Batch 4:  loss: 0.958669   accuracy: 0.6678\n",
      "\n",
      "Epoch 42, CIFAR-10 Batch 5:  loss: 0.985588   accuracy: 0.663\n",
      "\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss: 0.94883   accuracy: 0.675\n",
      "\n",
      "Epoch 43, CIFAR-10 Batch 2:  loss: 0.946011   accuracy: 0.6718\n",
      "\n",
      "Epoch 43, CIFAR-10 Batch 3:  loss: 0.956566   accuracy: 0.6776\n",
      "\n",
      "Epoch 43, CIFAR-10 Batch 4:  loss: 1.00891   accuracy: 0.6584\n",
      "\n",
      "Epoch 43, CIFAR-10 Batch 5:  loss: 0.943205   accuracy: 0.674\n",
      "\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss: 0.971293   accuracy: 0.6664\n",
      "\n",
      "Epoch 44, CIFAR-10 Batch 2:  loss: 0.947941   accuracy: 0.6706\n",
      "\n",
      "Epoch 44, CIFAR-10 Batch 3:  loss: 0.952688   accuracy: 0.6744\n",
      "\n",
      "Epoch 44, CIFAR-10 Batch 4:  loss: 0.971106   accuracy: 0.665\n",
      "\n",
      "Epoch 44, CIFAR-10 Batch 5:  loss: 0.967504   accuracy: 0.6672\n",
      "\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss: 0.985113   accuracy: 0.6612\n",
      "\n",
      "Epoch 45, CIFAR-10 Batch 2:  loss: 0.966786   accuracy: 0.673\n",
      "\n",
      "Epoch 45, CIFAR-10 Batch 3:  loss: 0.938198   accuracy: 0.681\n",
      "\n",
      "Epoch 45, CIFAR-10 Batch 4:  loss: 0.98567   accuracy: 0.6652\n",
      "\n",
      "Epoch 45, CIFAR-10 Batch 5:  loss: 0.953183   accuracy: 0.6712\n",
      "\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss: 0.953524   accuracy: 0.671\n",
      "\n",
      "Epoch 46, CIFAR-10 Batch 2:  loss: 0.936458   accuracy: 0.6754\n",
      "\n",
      "Epoch 46, CIFAR-10 Batch 3:  loss: 0.955614   accuracy: 0.6724\n",
      "\n",
      "Epoch 46, CIFAR-10 Batch 4:  loss: 0.970943   accuracy: 0.6672\n",
      "\n",
      "Epoch 46, CIFAR-10 Batch 5:  loss: 0.950124   accuracy: 0.6726\n",
      "\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss: 0.973407   accuracy: 0.6586\n",
      "\n",
      "Epoch 47, CIFAR-10 Batch 2:  loss: 0.953948   accuracy: 0.6724\n",
      "\n",
      "Epoch 47, CIFAR-10 Batch 3:  loss: 0.943666   accuracy: 0.6794\n",
      "\n",
      "Epoch 47, CIFAR-10 Batch 4:  loss: 0.969751   accuracy: 0.6638\n",
      "\n",
      "Epoch 47, CIFAR-10 Batch 5:  loss: 0.947778   accuracy: 0.6704\n",
      "\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss: 0.975041   accuracy: 0.6686\n",
      "\n",
      "Epoch 48, CIFAR-10 Batch 2:  loss: 0.941227   accuracy: 0.6752\n",
      "\n",
      "Epoch 48, CIFAR-10 Batch 3:  loss: 0.964043   accuracy: 0.6708\n",
      "\n",
      "Epoch 48, CIFAR-10 Batch 4:  loss: 0.951439   accuracy: 0.6758\n",
      "\n",
      "Epoch 48, CIFAR-10 Batch 5:  loss: 0.962801   accuracy: 0.6756\n",
      "\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss: 0.940587   accuracy: 0.6766\n",
      "\n",
      "Epoch 49, CIFAR-10 Batch 2:  loss: 0.933597   accuracy: 0.6788\n",
      "\n",
      "Epoch 49, CIFAR-10 Batch 3:  loss: 0.941311   accuracy: 0.6794\n",
      "\n",
      "Epoch 49, CIFAR-10 Batch 4:  loss: 0.957659   accuracy: 0.676\n",
      "\n",
      "Epoch 49, CIFAR-10 Batch 5:  loss: 0.962173   accuracy: 0.6694\n",
      "\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss: 0.971516   accuracy: 0.6686\n",
      "\n",
      "Epoch 50, CIFAR-10 Batch 2:  loss: 0.956392   accuracy: 0.6674\n",
      "\n",
      "Epoch 50, CIFAR-10 Batch 3:  loss: 0.945617   accuracy: 0.6752\n",
      "\n",
      "Epoch 50, CIFAR-10 Batch 4:  loss: 0.949489   accuracy: 0.6718\n",
      "\n",
      "Epoch 50, CIFAR-10 Batch 5:  loss: 0.953363   accuracy: 0.6744\n",
      "\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss: 0.953234   accuracy: 0.6732\n",
      "\n",
      "Epoch 51, CIFAR-10 Batch 2:  loss: 0.921898   accuracy: 0.6834\n",
      "\n",
      "Epoch 51, CIFAR-10 Batch 3:  loss: 0.92792   accuracy: 0.6862\n",
      "\n",
      "Epoch 51, CIFAR-10 Batch 4:  loss: 0.963216   accuracy: 0.666\n",
      "\n",
      "Epoch 51, CIFAR-10 Batch 5:  loss: 0.933239   accuracy: 0.6798\n",
      "\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss: 0.950116   accuracy: 0.6774\n",
      "\n",
      "Epoch 52, CIFAR-10 Batch 2:  loss: 0.921345   accuracy: 0.6874\n",
      "\n",
      "Epoch 52, CIFAR-10 Batch 3:  loss: 0.937085   accuracy: 0.68\n",
      "\n",
      "Epoch 52, CIFAR-10 Batch 4:  loss: 0.975188   accuracy: 0.6616\n",
      "\n",
      "Epoch 52, CIFAR-10 Batch 5:  loss: 0.947025   accuracy: 0.671\n",
      "\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss: 0.975083   accuracy: 0.6646\n",
      "\n",
      "Epoch 53, CIFAR-10 Batch 2:  loss: 0.940739   accuracy: 0.6788\n",
      "\n",
      "Epoch 53, CIFAR-10 Batch 3:  loss: 0.939657   accuracy: 0.6796\n",
      "\n",
      "Epoch 53, CIFAR-10 Batch 4:  loss: 0.943152   accuracy: 0.6736\n",
      "\n",
      "Epoch 53, CIFAR-10 Batch 5:  loss: 0.94699   accuracy: 0.6718\n",
      "\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss: 0.975366   accuracy: 0.6676\n",
      "\n",
      "Epoch 54, CIFAR-10 Batch 2:  loss: 0.949117   accuracy: 0.6748\n",
      "\n",
      "Epoch 54, CIFAR-10 Batch 3:  loss: 0.942394   accuracy: 0.6786\n",
      "\n",
      "Epoch 54, CIFAR-10 Batch 4:  loss: 0.948256   accuracy: 0.6732\n",
      "\n",
      "Epoch 54, CIFAR-10 Batch 5:  loss: 0.941396   accuracy: 0.6766\n",
      "\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss: 0.936269   accuracy: 0.6814\n",
      "\n",
      "Epoch 55, CIFAR-10 Batch 2:  loss: 0.926842   accuracy: 0.6834\n",
      "\n",
      "Epoch 55, CIFAR-10 Batch 3:  loss: 0.923253   accuracy: 0.6834\n",
      "\n",
      "Epoch 55, CIFAR-10 Batch 4:  loss: 0.979544   accuracy: 0.6626\n",
      "\n",
      "Epoch 55, CIFAR-10 Batch 5:  loss: 0.942036   accuracy: 0.6804\n",
      "\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss: 0.926338   accuracy: 0.6804\n",
      "\n",
      "Epoch 56, CIFAR-10 Batch 2:  loss: 0.928373   accuracy: 0.675\n",
      "\n",
      "Epoch 56, CIFAR-10 Batch 3:  loss: 0.931415   accuracy: 0.685\n",
      "\n",
      "Epoch 56, CIFAR-10 Batch 4:  loss: 0.949724   accuracy: 0.6744\n",
      "\n",
      "Epoch 56, CIFAR-10 Batch 5:  loss: 0.959215   accuracy: 0.6768\n",
      "\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss: 0.965092   accuracy: 0.6658\n",
      "\n",
      "Epoch 57, CIFAR-10 Batch 2:  loss: 0.913824   accuracy: 0.6808\n",
      "\n",
      "Epoch 57, CIFAR-10 Batch 3:  loss: 0.913593   accuracy: 0.6872\n",
      "\n",
      "Epoch 57, CIFAR-10 Batch 4:  loss: 0.937522   accuracy: 0.6768\n",
      "\n",
      "Epoch 57, CIFAR-10 Batch 5:  loss: 0.941653   accuracy: 0.6816\n",
      "\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss: 0.973621   accuracy: 0.6648\n",
      "\n",
      "Epoch 58, CIFAR-10 Batch 2:  loss: 0.940062   accuracy: 0.678\n",
      "\n",
      "Epoch 58, CIFAR-10 Batch 3:  loss: 0.933832   accuracy: 0.679\n",
      "\n",
      "Epoch 58, CIFAR-10 Batch 4:  loss: 0.928859   accuracy: 0.6848\n",
      "\n",
      "Epoch 58, CIFAR-10 Batch 5:  loss: 0.929081   accuracy: 0.6784\n",
      "\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss: 0.967985   accuracy: 0.6752\n",
      "\n",
      "Epoch 59, CIFAR-10 Batch 2:  loss: 0.924488   accuracy: 0.6814\n",
      "\n",
      "Epoch 59, CIFAR-10 Batch 3:  loss: 0.951223   accuracy: 0.6742\n",
      "\n",
      "Epoch 59, CIFAR-10 Batch 4:  loss: 0.953026   accuracy: 0.675\n",
      "\n",
      "Epoch 59, CIFAR-10 Batch 5:  loss: 0.95837   accuracy: 0.6748\n",
      "\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss: 0.945907   accuracy: 0.6742\n",
      "\n",
      "Epoch 60, CIFAR-10 Batch 2:  loss: 0.94113   accuracy: 0.676\n",
      "\n",
      "Epoch 60, CIFAR-10 Batch 3:  loss: 0.921387   accuracy: 0.6858\n",
      "\n",
      "Epoch 60, CIFAR-10 Batch 4:  loss: 0.939045   accuracy: 0.6844\n",
      "\n",
      "Epoch 60, CIFAR-10 Batch 5:  loss: 0.915862   accuracy: 0.681\n",
      "\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss: 0.926428   accuracy: 0.6838\n",
      "\n",
      "Epoch 61, CIFAR-10 Batch 2:  loss: 0.921526   accuracy: 0.685\n",
      "\n",
      "Epoch 61, CIFAR-10 Batch 3:  loss: 0.937916   accuracy: 0.6786\n",
      "\n",
      "Epoch 61, CIFAR-10 Batch 4:  loss: 0.929014   accuracy: 0.6844\n",
      "\n",
      "Epoch 61, CIFAR-10 Batch 5:  loss: 0.92475   accuracy: 0.6788\n",
      "\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss: 0.950499   accuracy: 0.679\n",
      "\n",
      "Epoch 62, CIFAR-10 Batch 2:  loss: 0.92967   accuracy: 0.6782\n",
      "\n",
      "Epoch 62, CIFAR-10 Batch 3:  loss: 0.924948   accuracy: 0.682\n",
      "\n",
      "Epoch 62, CIFAR-10 Batch 4:  loss: 0.919907   accuracy: 0.6868\n",
      "\n",
      "Epoch 62, CIFAR-10 Batch 5:  loss: 0.946033   accuracy: 0.6802\n",
      "\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss: 0.929989   accuracy: 0.6842\n",
      "\n",
      "Epoch 63, CIFAR-10 Batch 2:  loss: 0.928949   accuracy: 0.6768\n",
      "\n",
      "Epoch 63, CIFAR-10 Batch 3:  loss: 0.926332   accuracy: 0.6842\n",
      "\n",
      "Epoch 63, CIFAR-10 Batch 4:  loss: 0.912983   accuracy: 0.6862\n",
      "\n",
      "Epoch 63, CIFAR-10 Batch 5:  loss: 0.925487   accuracy: 0.6826\n",
      "\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss: 0.938153   accuracy: 0.6798\n",
      "\n",
      "Epoch 64, CIFAR-10 Batch 2:  loss: 0.930606   accuracy: 0.677\n",
      "\n",
      "Epoch 64, CIFAR-10 Batch 3:  loss: 0.944792   accuracy: 0.6796\n",
      "\n",
      "Epoch 64, CIFAR-10 Batch 4:  loss: 0.925155   accuracy: 0.682\n",
      "\n",
      "Epoch 64, CIFAR-10 Batch 5:  loss: 0.922463   accuracy: 0.691\n",
      "\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss: 0.948628   accuracy: 0.6756\n",
      "\n",
      "Epoch 65, CIFAR-10 Batch 2:  loss: 0.914852   accuracy: 0.6828\n",
      "\n",
      "Epoch 65, CIFAR-10 Batch 3:  loss: 0.912788   accuracy: 0.6878\n",
      "\n",
      "Epoch 65, CIFAR-10 Batch 4:  loss: 0.934553   accuracy: 0.681\n",
      "\n",
      "Epoch 65, CIFAR-10 Batch 5:  loss: 0.927129   accuracy: 0.6838\n",
      "\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss: 0.923202   accuracy: 0.6844\n",
      "\n",
      "Epoch 66, CIFAR-10 Batch 2:  loss: 0.911807   accuracy: 0.6848\n",
      "\n",
      "Epoch 66, CIFAR-10 Batch 3:  loss: 0.928058   accuracy: 0.6818\n",
      "\n",
      "Epoch 66, CIFAR-10 Batch 4:  loss: 0.943324   accuracy: 0.6812\n",
      "\n",
      "Epoch 66, CIFAR-10 Batch 5:  loss: 0.951782   accuracy: 0.6766\n",
      "\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss: 0.917816   accuracy: 0.692\n",
      "\n",
      "Epoch 67, CIFAR-10 Batch 2:  loss: 0.917231   accuracy: 0.6856\n",
      "\n",
      "Epoch 67, CIFAR-10 Batch 3:  loss: 0.919613   accuracy: 0.6872\n",
      "\n",
      "Epoch 67, CIFAR-10 Batch 4:  loss: 0.921679   accuracy: 0.686\n",
      "\n",
      "Epoch 67, CIFAR-10 Batch 5:  loss: 0.933452   accuracy: 0.6814\n",
      "\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss: 0.982577   accuracy: 0.6646\n",
      "\n",
      "Epoch 68, CIFAR-10 Batch 2:  loss: 0.911982   accuracy: 0.6842\n",
      "\n",
      "Epoch 68, CIFAR-10 Batch 3:  loss: 0.95497   accuracy: 0.6762\n",
      "\n",
      "Epoch 68, CIFAR-10 Batch 4:  loss: 0.942832   accuracy: 0.6768\n",
      "\n",
      "Epoch 68, CIFAR-10 Batch 5:  loss: 0.935695   accuracy: 0.6812\n",
      "\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss: 0.932046   accuracy: 0.6878\n",
      "\n",
      "Epoch 69, CIFAR-10 Batch 2:  loss: 0.912129   accuracy: 0.6886\n",
      "\n",
      "Epoch 69, CIFAR-10 Batch 3:  loss: 0.916525   accuracy: 0.6922\n",
      "\n",
      "Epoch 69, CIFAR-10 Batch 4:  loss: 0.930492   accuracy: 0.6836\n",
      "\n",
      "Epoch 69, CIFAR-10 Batch 5:  loss: 0.927193   accuracy: 0.6906\n",
      "\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss: 0.926271   accuracy: 0.6838\n",
      "\n",
      "Epoch 70, CIFAR-10 Batch 2:  loss: 0.914975   accuracy: 0.6846\n",
      "\n",
      "Epoch 70, CIFAR-10 Batch 3:  loss: 0.912945   accuracy: 0.6888\n",
      "\n",
      "Epoch 70, CIFAR-10 Batch 4:  loss: 0.929979   accuracy: 0.6838\n",
      "\n",
      "Epoch 70, CIFAR-10 Batch 5:  loss: 0.899357   accuracy: 0.691\n",
      "\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss: 0.954582   accuracy: 0.6786\n",
      "\n",
      "Epoch 71, CIFAR-10 Batch 2:  loss: 0.911199   accuracy: 0.6894\n",
      "\n",
      "Epoch 71, CIFAR-10 Batch 3:  loss: 0.927002   accuracy: 0.683\n",
      "\n",
      "Epoch 71, CIFAR-10 Batch 4:  loss: 0.926388   accuracy: 0.683\n",
      "\n",
      "Epoch 71, CIFAR-10 Batch 5:  loss: 0.928074   accuracy: 0.6818\n",
      "\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss: 0.933806   accuracy: 0.6848\n",
      "\n",
      "Epoch 72, CIFAR-10 Batch 2:  loss: 0.908395   accuracy: 0.6898\n",
      "\n",
      "Epoch 72, CIFAR-10 Batch 3:  loss: 0.936414   accuracy: 0.6832\n",
      "\n",
      "Epoch 72, CIFAR-10 Batch 4:  loss: 0.910976   accuracy: 0.6936\n",
      "\n",
      "Epoch 72, CIFAR-10 Batch 5:  loss: 0.931308   accuracy: 0.6846\n",
      "\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss: 0.945517   accuracy: 0.6806\n",
      "\n",
      "Epoch 73, CIFAR-10 Batch 2:  loss: 0.918785   accuracy: 0.684\n",
      "\n",
      "Epoch 73, CIFAR-10 Batch 3:  loss: 0.956364   accuracy: 0.6786\n",
      "\n",
      "Epoch 73, CIFAR-10 Batch 4:  loss: 0.922479   accuracy: 0.6854\n",
      "\n",
      "Epoch 73, CIFAR-10 Batch 5:  loss: 0.931591   accuracy: 0.6822\n",
      "\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss: 0.938803   accuracy: 0.6852\n",
      "\n",
      "Epoch 74, CIFAR-10 Batch 2:  loss: 0.944945   accuracy: 0.678\n",
      "\n",
      "Epoch 74, CIFAR-10 Batch 3:  loss: 0.908276   accuracy: 0.6888\n",
      "\n",
      "Epoch 74, CIFAR-10 Batch 4:  loss: 0.916396   accuracy: 0.6884\n",
      "\n",
      "Epoch 74, CIFAR-10 Batch 5:  loss: 0.912188   accuracy: 0.6896\n",
      "\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss: 0.970536   accuracy: 0.6776\n",
      "\n",
      "Epoch 75, CIFAR-10 Batch 2:  loss: 0.909107   accuracy: 0.687\n",
      "\n",
      "Epoch 75, CIFAR-10 Batch 3:  loss: 0.927365   accuracy: 0.6828\n",
      "\n",
      "Epoch 75, CIFAR-10 Batch 4:  loss: 0.942008   accuracy: 0.6744\n",
      "\n",
      "Epoch 75, CIFAR-10 Batch 5:  loss: 0.952848   accuracy: 0.6736\n",
      "\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss: 0.910753   accuracy: 0.6978\n",
      "\n",
      "Epoch 76, CIFAR-10 Batch 2:  loss: 0.926349   accuracy: 0.6844\n",
      "\n",
      "Epoch 76, CIFAR-10 Batch 3:  loss: 0.952917   accuracy: 0.6804\n",
      "\n",
      "Epoch 76, CIFAR-10 Batch 4:  loss: 0.940466   accuracy: 0.6848\n",
      "\n",
      "Epoch 76, CIFAR-10 Batch 5:  loss: 0.970835   accuracy: 0.6736\n",
      "\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss: 0.930425   accuracy: 0.6848\n",
      "\n",
      "Epoch 77, CIFAR-10 Batch 2:  loss: 0.896947   accuracy: 0.6938\n",
      "\n",
      "Epoch 77, CIFAR-10 Batch 3:  loss: 0.930133   accuracy: 0.6884\n",
      "\n",
      "Epoch 77, CIFAR-10 Batch 4:  loss: 0.903786   accuracy: 0.6934\n",
      "\n",
      "Epoch 77, CIFAR-10 Batch 5:  loss: 0.931972   accuracy: 0.6848\n",
      "\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss: 0.957411   accuracy: 0.6804\n",
      "\n",
      "Epoch 78, CIFAR-10 Batch 2:  loss: 0.910331   accuracy: 0.6884\n",
      "\n",
      "Epoch 78, CIFAR-10 Batch 3:  loss: 0.916214   accuracy: 0.6864\n",
      "\n",
      "Epoch 78, CIFAR-10 Batch 4:  loss: 0.938726   accuracy: 0.6776\n",
      "\n",
      "Epoch 78, CIFAR-10 Batch 5:  loss: 0.938528   accuracy: 0.683\n",
      "\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss: 0.947583   accuracy: 0.679\n",
      "\n",
      "Epoch 79, CIFAR-10 Batch 2:  loss: 0.919911   accuracy: 0.688\n",
      "\n",
      "Epoch 79, CIFAR-10 Batch 3:  loss: 0.935395   accuracy: 0.691\n",
      "\n",
      "Epoch 79, CIFAR-10 Batch 4:  loss: 0.917362   accuracy: 0.6918\n",
      "\n",
      "Epoch 79, CIFAR-10 Batch 5:  loss: 0.943742   accuracy: 0.6836\n",
      "\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss: 0.935442   accuracy: 0.6872\n",
      "\n",
      "Epoch 80, CIFAR-10 Batch 2:  loss: 0.923422   accuracy: 0.6784\n",
      "\n",
      "Epoch 80, CIFAR-10 Batch 3:  loss: 0.944072   accuracy: 0.6852\n",
      "\n",
      "Epoch 80, CIFAR-10 Batch 4:  loss: 0.918291   accuracy: 0.69\n",
      "\n",
      "Epoch 80, CIFAR-10 Batch 5:  loss: 0.958411   accuracy: 0.6834\n",
      "\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss: 0.922884   accuracy: 0.683\n",
      "\n",
      "Epoch 81, CIFAR-10 Batch 2:  loss: 0.903351   accuracy: 0.6916\n",
      "\n",
      "Epoch 81, CIFAR-10 Batch 3:  loss: 0.936713   accuracy: 0.6816\n",
      "\n",
      "Epoch 81, CIFAR-10 Batch 4:  loss: 0.910347   accuracy: 0.6914\n",
      "\n",
      "Epoch 81, CIFAR-10 Batch 5:  loss: 0.942798   accuracy: 0.6846\n",
      "\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss: 0.944798   accuracy: 0.6792\n",
      "\n",
      "Epoch 82, CIFAR-10 Batch 2:  loss: 0.895834   accuracy: 0.6916\n",
      "\n",
      "Epoch 82, CIFAR-10 Batch 3:  loss: 0.934602   accuracy: 0.6852\n",
      "\n",
      "Epoch 82, CIFAR-10 Batch 4:  loss: 0.929894   accuracy: 0.6864\n",
      "\n",
      "Epoch 82, CIFAR-10 Batch 5:  loss: 0.962636   accuracy: 0.6732\n",
      "\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss: 0.948798   accuracy: 0.6804\n",
      "\n",
      "Epoch 83, CIFAR-10 Batch 2:  loss: 0.898075   accuracy: 0.69\n",
      "\n",
      "Epoch 83, CIFAR-10 Batch 3:  loss: 0.911891   accuracy: 0.6952\n",
      "\n",
      "Epoch 83, CIFAR-10 Batch 4:  loss: 0.933936   accuracy: 0.6874\n",
      "\n",
      "Epoch 83, CIFAR-10 Batch 5:  loss: 0.940762   accuracy: 0.6814\n",
      "\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss: 0.922116   accuracy: 0.6906\n",
      "\n",
      "Epoch 84, CIFAR-10 Batch 2:  loss: 0.919165   accuracy: 0.6858\n",
      "\n",
      "Epoch 84, CIFAR-10 Batch 3:  loss: 0.917618   accuracy: 0.689\n",
      "\n",
      "Epoch 84, CIFAR-10 Batch 4:  loss: 0.916661   accuracy: 0.6924\n",
      "\n",
      "Epoch 84, CIFAR-10 Batch 5:  loss: 0.94708   accuracy: 0.6768\n",
      "\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss: 0.952649   accuracy: 0.6784\n",
      "\n",
      "Epoch 85, CIFAR-10 Batch 2:  loss: 0.904705   accuracy: 0.6898\n",
      "\n",
      "Epoch 85, CIFAR-10 Batch 3:  loss: 0.926337   accuracy: 0.6858\n",
      "\n",
      "Epoch 85, CIFAR-10 Batch 4:  loss: 0.914386   accuracy: 0.694\n",
      "\n",
      "Epoch 85, CIFAR-10 Batch 5:  loss: 0.934874   accuracy: 0.6824\n",
      "\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss: 0.931668   accuracy: 0.683\n",
      "\n",
      "Epoch 86, CIFAR-10 Batch 2:  loss: 0.910384   accuracy: 0.6896\n",
      "\n",
      "Epoch 86, CIFAR-10 Batch 3:  loss: 0.909383   accuracy: 0.6978\n",
      "\n",
      "Epoch 86, CIFAR-10 Batch 4:  loss: 0.890494   accuracy: 0.7002\n",
      "\n",
      "Epoch 86, CIFAR-10 Batch 5:  loss: 0.935986   accuracy: 0.6892\n",
      "\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss: 0.959692   accuracy: 0.6784\n",
      "\n",
      "Epoch 87, CIFAR-10 Batch 2:  loss: 0.911807   accuracy: 0.6896\n",
      "\n",
      "Epoch 87, CIFAR-10 Batch 3:  loss: 0.933916   accuracy: 0.6932\n",
      "\n",
      "Epoch 87, CIFAR-10 Batch 4:  loss: 0.927197   accuracy: 0.6814\n",
      "\n",
      "Epoch 87, CIFAR-10 Batch 5:  loss: 0.924204   accuracy: 0.6828\n",
      "\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss: 0.931141   accuracy: 0.6866\n",
      "\n",
      "Epoch 88, CIFAR-10 Batch 2:  loss: 0.899852   accuracy: 0.6942\n",
      "\n",
      "Epoch 88, CIFAR-10 Batch 3:  loss: 0.940863   accuracy: 0.6848\n",
      "\n",
      "Epoch 88, CIFAR-10 Batch 4:  loss: 0.890516   accuracy: 0.6998\n",
      "\n",
      "Epoch 88, CIFAR-10 Batch 5:  loss: 0.936768   accuracy: 0.6862\n",
      "\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss: 0.935241   accuracy: 0.6854\n",
      "\n",
      "Epoch 89, CIFAR-10 Batch 2:  loss: 0.919911   accuracy: 0.689\n",
      "\n",
      "Epoch 89, CIFAR-10 Batch 3:  loss: 0.974798   accuracy: 0.6808\n",
      "\n",
      "Epoch 89, CIFAR-10 Batch 4:  loss: 0.906442   accuracy: 0.6934\n",
      "\n",
      "Epoch 89, CIFAR-10 Batch 5:  loss: 0.964492   accuracy: 0.6742\n",
      "\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss: 0.987225   accuracy: 0.672\n",
      "\n",
      "Epoch 90, CIFAR-10 Batch 2:  loss: 0.899411   accuracy: 0.6976\n",
      "\n",
      "Epoch 90, CIFAR-10 Batch 3:  loss: 0.949623   accuracy: 0.685\n",
      "\n",
      "Epoch 90, CIFAR-10 Batch 4:  loss: 0.917631   accuracy: 0.6954\n",
      "\n",
      "Epoch 90, CIFAR-10 Batch 5:  loss: 0.911434   accuracy: 0.6934\n",
      "\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss: 0.948254   accuracy: 0.6804\n",
      "\n",
      "Epoch 91, CIFAR-10 Batch 2:  loss: 0.901616   accuracy: 0.6968\n",
      "\n",
      "Epoch 91, CIFAR-10 Batch 3:  loss: 0.967841   accuracy: 0.6762\n",
      "\n",
      "Epoch 91, CIFAR-10 Batch 4:  loss: 0.941043   accuracy: 0.686\n",
      "\n",
      "Epoch 91, CIFAR-10 Batch 5:  loss: 0.920941   accuracy: 0.6906\n",
      "\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss: 0.951571   accuracy: 0.6828\n",
      "\n",
      "Epoch 92, CIFAR-10 Batch 2:  loss: 0.893312   accuracy: 0.6956\n",
      "\n",
      "Epoch 92, CIFAR-10 Batch 3:  loss: 0.91783   accuracy: 0.6946\n",
      "\n",
      "Epoch 92, CIFAR-10 Batch 4:  loss: 0.918105   accuracy: 0.6912\n",
      "\n",
      "Epoch 92, CIFAR-10 Batch 5:  loss: 0.945781   accuracy: 0.6816\n",
      "\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss: 0.997841   accuracy: 0.6662\n",
      "\n",
      "Epoch 93, CIFAR-10 Batch 2:  loss: 0.893923   accuracy: 0.6972\n",
      "\n",
      "Epoch 93, CIFAR-10 Batch 3:  loss: 0.933324   accuracy: 0.6906\n",
      "\n",
      "Epoch 93, CIFAR-10 Batch 4:  loss: 0.902032   accuracy: 0.6938\n",
      "\n",
      "Epoch 93, CIFAR-10 Batch 5:  loss: 0.983963   accuracy: 0.6718\n",
      "\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss: 0.951117   accuracy: 0.68\n",
      "\n",
      "Epoch 94, CIFAR-10 Batch 2:  loss: 0.900445   accuracy: 0.6952\n",
      "\n",
      "Epoch 94, CIFAR-10 Batch 3:  loss: 0.947138   accuracy: 0.6864\n",
      "\n",
      "Epoch 94, CIFAR-10 Batch 4:  loss: 0.921764   accuracy: 0.6888\n",
      "\n",
      "Epoch 94, CIFAR-10 Batch 5:  loss: 0.931091   accuracy: 0.6908\n",
      "\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss: 0.968037   accuracy: 0.6674\n",
      "\n",
      "Epoch 95, CIFAR-10 Batch 2:  loss: 0.886726   accuracy: 0.7032\n",
      "\n",
      "Epoch 95, CIFAR-10 Batch 3:  loss: 0.943987   accuracy: 0.6792\n",
      "\n",
      "Epoch 95, CIFAR-10 Batch 4:  loss: 0.902276   accuracy: 0.6954\n",
      "\n",
      "Epoch 95, CIFAR-10 Batch 5:  loss: 0.955577   accuracy: 0.6764\n",
      "\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss: 0.941627   accuracy: 0.6822\n",
      "\n",
      "Epoch 96, CIFAR-10 Batch 2:  loss: 0.895166   accuracy: 0.6998\n",
      "\n",
      "Epoch 96, CIFAR-10 Batch 3:  loss: 0.942629   accuracy: 0.6774\n",
      "\n",
      "Epoch 96, CIFAR-10 Batch 4:  loss: 0.909949   accuracy: 0.6954\n",
      "\n",
      "Epoch 96, CIFAR-10 Batch 5:  loss: 0.976472   accuracy: 0.6752\n",
      "\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss: 0.936049   accuracy: 0.682\n",
      "\n",
      "Epoch 97, CIFAR-10 Batch 2:  loss: 0.8961   accuracy: 0.6972\n",
      "\n",
      "Epoch 97, CIFAR-10 Batch 3:  loss: 0.930297   accuracy: 0.6886\n",
      "\n",
      "Epoch 97, CIFAR-10 Batch 4:  loss: 0.942024   accuracy: 0.6912\n",
      "\n",
      "Epoch 97, CIFAR-10 Batch 5:  loss: 0.941936   accuracy: 0.6878\n",
      "\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss: 0.971067   accuracy: 0.6744\n",
      "\n",
      "Epoch 98, CIFAR-10 Batch 2:  loss: 0.923099   accuracy: 0.6838\n",
      "\n",
      "Epoch 98, CIFAR-10 Batch 3:  loss: 0.932741   accuracy: 0.686\n",
      "\n",
      "Epoch 98, CIFAR-10 Batch 4:  loss: 0.932505   accuracy: 0.6924\n",
      "\n",
      "Epoch 98, CIFAR-10 Batch 5:  loss: 0.943971   accuracy: 0.689\n",
      "\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss: 0.921314   accuracy: 0.691\n",
      "\n",
      "Epoch 99, CIFAR-10 Batch 2:  loss: 0.901384   accuracy: 0.697\n",
      "\n",
      "Epoch 99, CIFAR-10 Batch 3:  loss: 0.927266   accuracy: 0.6922\n",
      "\n",
      "Epoch 99, CIFAR-10 Batch 4:  loss: 0.904295   accuracy: 0.6962\n",
      "\n",
      "Epoch 99, CIFAR-10 Batch 5:  loss: 0.950877   accuracy: 0.6844\n",
      "\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss: 0.946431   accuracy: 0.6824\n",
      "\n",
      "Epoch 100, CIFAR-10 Batch 2:  loss: 0.896092   accuracy: 0.6962\n",
      "\n",
      "Epoch 100, CIFAR-10 Batch 3:  loss: 0.939148   accuracy: 0.683\n",
      "\n",
      "Epoch 100, CIFAR-10 Batch 4:  loss: 0.91997   accuracy: 0.6908\n",
      "\n",
      "Epoch 100, CIFAR-10 Batch 5:  loss: 0.967008   accuracy: 0.6778\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.679296875\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAWJQAAFiUBSVIk8AAAIABJREFUeJzs3XecnFW9x/HPjx7AsKFXWToIiUCEiIlAVG4ERESEAEEp\nFpLQUW72UhRBcKMISEvCFQQxmiBFLhiIogEJVUJC6DVLNwik0Ft+94/fmXmefTK7O5ut2f2+X695\nzcw55znPmdmZ2d+cOcXcHRERERERgWW6ugEiIiIiIt2FgmMRERERkUTBsYiIiIhIouBYRERERCRR\ncCwiIiIikig4FhERERFJFByLiIiIiCQKjkVEREREEgXHIiIiIiKJgmMRERERkUTBsYiIiIhIouBY\nRERERCRRcCwiIiIikig4FhERERFJFBx3MTPb2My+aWajzOx/zKzOzI41swPM7HNmtmpXt7EpZraM\nme1rZpPM7BkzW2hmnrv8uavbKNLdmFlt4X1yRnuU7a7MbPfCYzi8q9skItKc5bq6Ab2Rma0OjAK+\nD2zcQvFFZvYYcCfwF+Dv7v5+BzexRekxXAsM7eq2SOczsyuBw1oo9jEwH3gdeJB4Df/R3Rd0bOtE\nRESWnHqOO5mZfQ14DPgZLQfGEH+j7Yhg+mbgWx3Xulb5Ha0IjNV71CstB6wJbA0cAowDXjazM8xM\nX8yXIoX37pVd3R4RkY6kf1CdyMwOBP4ALFvIWgg8DPwb+ADoB3wa2IZu+AXGzD4P7J1Leh74KfAA\n8FYu/d3ObJcsFVYBfgLsamZ7uvsHXd0gERGRPAXHncTMNiN6W/OB8SPAqcAUd/+4wjGrArsBBwD7\nAX07oanV+Gbh/r7u/lCXtES6i5OJYTZ5ywHrAEOA0cQXvpKhRE/ykZ3SOhERkSopOO48ZwMr5u7f\nBnzd3d9r6gB3f5sYZ/wXMzsW+B7Ru9zVBuZuNygwFuB1d2+okP4McJeZXQhMJL7klRxuZhe6+6zO\naODSKD2n1tXtaAt3v52l/DGISO/S7X6y74nMrA/w9VzSR8BhzQXGRe7+lruf7+63tXsDW2/t3O1X\nuqwVstRIr/URwFO5ZANGdk2LREREKlNw3Dl2BPrk7t/t7ktzUJlfXu6jLmuFLFVSgHx+IfnLXdEW\nERGRpmhYRedYt3D/5c48uZn1Bb4IbACsQUyamwvc5+4vLEmV7di8dmFmmxLDPTYEVgAagGnu/loL\nx21IjIndiHhcr6bjXmpDWzYAtgU2BWpS8pvAC8A9vXwps78X7m9mZsu6+yetqcTMtgM+A6xHTPJr\ncPc/VHHcisAXiJVi1gY+Id4Ls919dmva0ET9WwA7A+sD7wMvAfe7e6e+5yu0a0tge2At4jX5LvFa\nfwR4zN0XdWHzWmRmGwGfJ8awf4p4P70C3Onu89v5XJsSHRobEXNE5gJ3uftzbahzK+L5X5foXPgY\neBt4EXgaeMLdvY1NF5H24u66dPAFOAjw3OWWTjrv54BbgA8L589fZhPLbFkz9ezezPFNXW5PxzYs\n6bGFNlyZL5NL3w2YBiyqUM+HwKXAqhXq+wwwpYnjFgHXARtU+Twvk9oxDni2hcf2CTHefGiVdV9V\nOP6yVvz9f1449ubm/s6tfG1dWaj78CqP61PhOVm7Qrn86+b2XPoRREBXrGN+C+fdDvgT8E4zf5sX\ngROA5Zfg+RgM3NdEvR8TcwcGprK1hfwzmqm36rIVjq0BziS+lDX3mvwPcAWwUwt/46ouVXx+VPVa\nScceCMxq5nwfAX8DPt+KOm/PHd+QSx9EfHmr9JngwL3ALq04z/LAD4lx9y09b/OJz5w92uP9qYsu\nurTt0uUN6A0X4EuFD8K3gJoOPJ8Bv2jmQ77S5XagXxP1Ff+5VVVfOrZhSY8ttKHRP+qUdlyVj/Ff\n5AJkYrWNd6s4rgH4dBXP95FL8Bgd+BWwbAt1rwI8XjjuoCratEfhuXkJWKMdX2NXFtp0eJXHrVTh\neVirQrn86+Z2YjLrNc08lxWDY+KLyy+JLyXV/l0eosovRukcp1T5OvyQGHddW0g/o5m6qy5bOG4/\nYF4rX4+zWvgbV3Wp4vOjxdcKsTLPba089wXAMlXUfXvumIaUdizNdyLk/4YHVnGOtYiNb1r7/P25\nvd6juuiiy5JfNKyic8wg/jmXlnFbFfidmR3isSJFe/tf4LuFtA+Jno9XiB6lzxEbNJTsBvzTzHZ1\n93kd0KZ2ldaM/nW660Tv0rPEF4Ptgc1yxT8HXAQcYWZDgclkQ4qeSJcPiXWl++eO25jouW1ps5Pi\n2P33gEeJn60XEr2lnwYGEEM+Sk4ier7qmqrY3d8xs+FEr+RKKfkyM3vA3Z+pdIyZrQtcTTb85RPg\nEHd/o4XH0Rk2LNx3IohryQXEkoalY2aSBdCbApsUDzCzZYm/9f6FrHeJ9+SrxHtyM+CzZM/XAOBu\nM9vZ3ec21ygzO4FYiSbvE+Lv9SIxBGAHYvjH8kTAWXxvtqvUpvNYfPjTv4lfil4HVib+Fv1pvIpO\nlzOzTwF3EO/jvHnA/el6PWKYRb7txxOfaYe28nwjgAtzSY8Qvb0fEK+NgWTP5fLAlWY2092fbqI+\nA64n/u55c4n17F8nvkytlurfHA1xFOleujo67y0X4iftYi/BK8SGCP1pv5+7DyucYxERWNQUyi1H\n/JNeUCj/xwp1rkT0YJUuL+XK31vIK13WTcdumO4Xh5b8qInjyscW2nBl4fhSr9hfgM0qlD+QCFLz\nz8Mu6Tl34G5g+wrH7Q68UTjXXi0856Ul9n6ezlGx94r4UjKGxj/tLwIGVfF3HVlo0wPAChXKLUP8\nzJwve3oHvJ6Lf4/DqzzuB4XjnmmiXEOuzFu521cDG1YoX1sh7ezCueYSwzIqPW+bsfh7dEoLj6U/\ni/c2/qH4+k1/kwOB11KZNwvHnNHMOWqrLZvKD2PxXvI7iHHWi33GEMHlPsRP+jMKeWuSvSfz9V1L\n0+/dSn+H3VvzWgF+Wyi/EDiKwnAXIrj8FYv32h/VQv2358q+TfY5cQOweYXy2xC/JuTPMbmZ+vcu\nlH2amHha8TOe+HVoX2AS8Kf2fq/qoosurb90eQN6y4XomXq/8KGZv7xBBHqnEz+Jr7IE51iVxX9K\nPbGFYwax+DjMZse90cR40BaOadU/yArHX1nhOZtIMz+jEltuVwqobwNWbOa4r1X7jzCVX7e5+iqU\n36XwWmi2/txxkwvt+nWFMqcWyvyjueeoDa/n4t+jxb8n8SWrOESk4hhqKg/HqW9F+wbROEh8kgpf\nugrHLMPiY7z3bKb8tELZS1qof1sWD4zbLTgmeoPnFspfXO3fH1inmbx8nVe28rVS9XufmBybL/su\nMLiF+o8pHPM2TQwRS+Vvr/A3uJjm512sQ+PP1g+aOgcx96BU7iNgk1Y8Vyu15rnVRRddOuaipdw6\nicdGGd8mgqJKVgf2IibQ/BWYZ2Z3mtlRabWJahxGtjoCwK3uXlw6q9iu+4AfF5KPr/J8XekVooeo\nuVn2lxM94yWlWfrf9ma2LXb3m4lgqmT35hri7v9urr4K5e8BLsklfSOtotCS7xNDR0qOM7N9S3fM\nbAixjXfJf4ARLTxHncLMViJ6fbcuZE2osopZROBfrTqy4S4fA99w92Y30EnP01E0Xk3mhEplzewz\nNH5dPAWc2EL9jwL/3Wyr2+b7NF6DfBpwbLV/f29hCEknKX72/NTd72ruAHe/mOj1L1mF1g1deYTo\nRPBmzjGXCHpLViCGdVSS3wlylrvPqbYh7t7U/wcR6UQKjjuRu/+J+HlzehXFlyd6UcYDz5nZ6DSW\nrTkjCvd/UmXTLiQCqZK9zGz1Ko/tKpd5C+O13f1DoPiPdZK7v1pF/f/I3V47jeNtTzfmbq/A4uMr\nF+PuC4nhKR/mkn9rZp9Of68/ko1rd+A7VT7W9rCmmdUWLpub2RfM7L+Bx4BvFY6Z6O4zqqz/fK9y\nube0lF5+050/uPvj1RybgpPLcklDzWzlCkWL41p/kV5vLbmCGJbUEb5fuN9swNfdmNkqwDdySfOI\nIWHVOK1wvzXjjs9392rWa59SuP/ZKo5ZqxXtEJFuQsFxJ3P3me7+RWBXomez2XV4kzWInsZJZrZC\npQKp53HHXNJz7n5/lW36iFjmqlwdTfeKdBd/rbLcs4X7f6vyuOJkt1b/k7PwKTNbvxg4svhkqWKP\nakXu/gAxbrmkHxEUX0XjyW6/dPdbW9vmNvglMKdweZr4cjKWxSfM3cXiwVxzbm65SNnuNP5su64V\nxwL8M3d7eWCnCmV2yd0uLf3XotSLe20r29MiM1uLGLZR8i9f+rZ134nGE9NuqPYXmfRYH8sl9U8T\n+6pR7fvkicL9pj4T8r86bWxmR1dZv4h0E5oh20Xc/U7gTij/RPsFYlWFnYhexEpfXA4kZjpX+rDd\njsYzt+9rZZPuBUbn7g9k8Z6S7qT4j6opCwv3n6xYquXjWhzaklZH+AqxqsJORMBb8ctMBf2qLIe7\nX2BmuxOTeCBeO3n30rohCJ3pPWKVkR9X2VsH8IK7v9mKcwwu3J+XvpBUa9nC/U2JSW15+S+iT3vr\nNqL4VyvKVmtQ4f6dHXCOjjawcH9JPsM+k24vQ3yOtvQ8LPTqdystbt7T1GfCJBoPsbnYzL5BTDS8\nxZeC1YBEejsFx92Auz9G9Hr8BsDMaoifF08klpXKG21mV1T4ObrYi1FxmaFmFIPG7v5zYLW7zH3c\nTsct31xhM9uFGD/bv7lyzah2XHnJEcQ43E8X0ucDB7t7sf1d4RPi+X6DWHrtTmKIQ2sCXWg85Kca\nxeXi/lmxVPUaDTFKv9Lk/17FXydaUnEJvjYqDvupahhJN9MVn2FV71bp7h8VRrZV/Exw9/vN7FIa\ndzZ8JV0WmdnDxNC6fxITmqv59VBEOpGGVXRD7j7f3a8kej7OrFDk2AppNYX7xZ7PlhT/SVTdk9kV\n2jDJrN0np5nZV4nJT0saGEMr34up9+mcClk/dPeGNrRjSR3h7la4LOfua7j7lu4+3N0vXoLAGGL1\ngdZo7/HyqxbuF98bbX2vtYc1CvfbdUvlTtIVn2EdNVn1GOLXm3cL6csQY5WPJlafedXMppnZt6qY\nUyIinUTBcTfm4SfEh2jeV6o5vJWn0wfzEkgT4X5P4yEtDcBZwJ7AVsQ//ZXygSMVNq1o5XnXIJb9\nKzrUzHr7+7rZXv4l0NJ7ozu+15aaiXjN6I7Pa1XSZ/c5xJCcMcA9LP5rFMT/4N2JOR93mNl6ndZI\nEWmShlUsHS4Chufub2Bmfdz9vVxasadotVaeo/izvsbFVWc0jXvtJgGHVbFyQbWThRaTepiuAjao\nkD2UmLlf6ReH3iLfO/0x0Kedh5kU3xttfa+1h2KPfLEXdmnQ4z7D0hJwvwB+YWarAjsDXyTep4Np\n/D/4i8CtaWfGqpeGFJH219t7mJYWlWadF38yLI7L3LyV59iyhfqksr1ztxcA36tySa+2LA13YuG8\n99N41ZMfm9kX21D/0i6/Xu9ytLGXvigFLvmf/DdrqmwTWvverEZxDedtOuAcHa1Hf4a5+9vu/g93\n/6m7705sgX0aMUm1ZABwZFe0T0QyCo6XDpXGxRXH4z1C4/Vvi7PXW1Jcuq3a9Wer1RN+5q0k/w98\nuru/U+VxS7RUnpl9DqjPJc0jVsf4DtlzvCzwhzT0oje6t3D/yx1wjgdzt7dIk2irVWlpuLa6l8bv\nsaXxy1HxM6ctn2GLiAmr3Za7v+7uZ7P4kob7dEV7RCSj4HjpsFXh/tvFDTBSb1b+n8tmZlZcGqki\nM1uOCLDK1dH6ZZRaUvyZsNolzrq7/E+/VU0gSsMiDm7tidJOiZNpPKb2SHd/wd2nEmsNl2xILB3V\nG91WuH94B5zjntztZYD9qzkojQc/oMWCreTu/wEezSXtbGZtmSBalH//dtR79180Hpe7X1Pruhel\nx5pf5/kRd3+rPRvXgSbTeOfU2i5qh4gkCo47gZmtY2brtKGK4s9stzdR7g+F+8VtoZtyDI23nb3F\n3d+o8thqFWeSt/eOc10lP06y+LNuU77Nkv3sfRkxwafkInf/c+7+qTTuNd3HzJaGrcDblbs/A/w9\nlzTIzIq7R7bVxML9/zazaiYCHknlseLt4bLC/fPacQWE/Pu3Q9676VeX/M6Rq1N5TfdKzirc/327\nNKoTpPHw+VUtqhmWJSIdSMFx59iG2AK63szWbrF0jpntD4wqJBdXryi5isb/xL5uZqObKFuqfycW\n/8dyYWvaWKXngPymD1/qgHN0hYdztwea2W7NFTaznYkJlq1iZj+g8aTMmcDJ+TLpn+zBNA7Yf2Fm\n+Q0reoszCvf/18z2aE0FZraeme1VKc/dH6XxxiBbAue3UN9niMlZHeVyGo+3/gpwQbUBcgtf4PNr\nCO+UJpd1hOJnz1npM6pJZjaKbEMcgHeI56JLmNmotGNhteX3pPHyg9VuVCQiHUTBcedZmVjS5yUz\nu8HM9m/uA9TMtjGzy4BraLxj14Ms3kMMQPoZ8aRC8kVm9kszazTz28yWM7MjiO2U8//orkk/0ber\nNOwjv531bmb2GzP7spltUdheeWnqVS5uBXydmX29WMjM+pjZiUSPZl9ip8OqmNl2wAW5pLeB4ZVm\ntKc1jvNjGFcAJrdiK90ewd2n03gd6D7ESgCXmtkWTR1nZjVmdqCZTSaW5PtOM6c5lsZf+I42s4nF\n16+ZLWNmBxC/+PSjg9Ygdvd3ifbm5ygcB/w9bVKzGDNb0cy+ZmbX0vyOmPmNVFYF/mJm+6XPqeLW\n6G15DP8Ers4lrQL8zcy+W+yZN7O+ZvYL4OJCNScv4Xra7WUM8EJ6LXyjqfde+gz+DrH9e95S0+st\n0lNpKbfOtzyx+903AMzsGeAFIlhaRPzz/AywUYVjXwIOaG4DDHe/wsx2BQ5LScsAPwKONbN7gFeJ\nZZ52AtYsHP44i/dSt6eLaLy173fTpegOYu3PpcEVxOoRpYBrDeBGM3ue+CLzPvEz9CDiCxLE7PRR\nxNqmzTKzlYlfCvrkkke6e5O7h7n7tWY2HhiZkjYHxgGHVvmYeorTiR0ES497GeJ5H5X+Po8RExqX\nJ94TW9CK8Z7u/rCZjQHOyyUfAgw3s3uBF4lAciCxMgHEmNoT6aDx4O7+VzP7EfArsnV/hwJ3m9mr\nwGxix8I+xLj0AWRrdFdaFafkN8APgZXS/V3TpZK2DuU4htgoo7Q76Grp/GPN7H7iy8W6wC659pRM\ncvdxbTx/e1iJeC0cAriZPQXMIVtebj1gBxZfru7P7n5Tp7VSRCpScNw53iSC32IwChG4VLNk0W3A\n96vc/eyIdM4TyP5RrUjzAed0YN+O7HFx98lmNogIDnoEd/8g9RT/gywAAtg4XYreJiZkPVHlKS4i\nviyV/Nbdi+NdKzmR+CJSmpQ1wsz+7u69ZpJe+hL5bTN7CPgZjTdqaervU9TsWrnufn76AnMW2Xtt\nWRp/CSz5mPgy2NbtrJuV2vQyEVDmey3Xo/FrtDV1NpjZ4URQ36eF4m3i7gvT8KTricC+ZA1iY52m\nXEL0lHc3RkyqLk6sLppM1qkhIl1Iwyo6gbvPJno6vkT0Mj0AfFLFoe8T/yD2cfc9qt0WOO3OdBKx\ntNFfqbwzU8mjxAfyrp3xU2Rq1yDiH9m/iF6spXoCirs/AexI/Bza1HP9NvA7YIC731pNvWZ2MI0n\nYz5B5a3DK7XpfWKMcn6iz0VmtnU1x/ck7n4uMZHxAhZfD7iSJ4kvJbu4e4u/pKTluHal8bChvEXE\n+3Cwu/+uqka3kbtfQ6zvfC6NxyFXMpeYzNdsYObuk4n5Ez8lhoi8SuM1etuNu88nluA7hOjtbson\nxFClwe5+TBu2lW9P+xLP0b20/Nm2iGj/3u5+kDb/EOkezL2nLj/bvaXepi3TZW2yHp6FRK/vo8Bj\n7bGzVxpvvCsxS351IlCbC9xXbcAt1UlrC+9K/Dy/EvE8vwzcmcaEShdLE+MGEL/k1BBfQucDzwKP\nuvtrzRzeUt1bEF9K10v1vgzc7+4vtrXdbWiTEcMUtgXWIoZ6vJ3a9ijwuHfzfwRm9mnieV2H+Kx8\nE3iFeF91+U54TTGzlYDtiF8H1yWe+4+IidPPAA928fhoEalAwbGIiIiISKJhFSIiIiIiiYJjERER\nEZFEwbGIiIiISKLgWEREREQkUXAsIiIiIpIoOBYRERERSRQci4iIiIgkCo5FRERERBIFxyIiIiIi\niYJjEREREZFEwbGIiIiISKLgWEREREQkUXAsIiIiIpIoOBYRERERSRQci4iIiIgkCo5FRERERBIF\nxyIiIiIiiYJjEREREZFEwbGIiIiISKLgWEREREQkUXAsIiIiIpIoOBYRERERSRQci4iIiIgkvS44\nNrMGM3Mz272r2yIiIiIi3UuvC45FRERERJqi4FhEREREJFFwLCIiIiKSKDgWEREREUl6dXBsZqub\n2XlmNsfMPjCzl83sf81svWaOGWpm15vZv83sw3R9g5l9qZljPF1qzWwbM7vKzF40s4/M7M+5cmub\n2S/N7BEze8fM3k/l7jazM81s4ybqX8vMfm5mD5vZ2+nYR8zsbDNbvW3PkoiIiEjvYe7e1W3oVGbW\nAGwMfBv4Wbr9LrAssGIq1gDs6O7zCsf+DDg13XVgAbAaYCmt3t3/p8I5S0/yd4DxwMrAW8DywFR3\n/0YKfO8BSoH5J8BCoCZX/yh3H1+oewhwI1AKgj9Mx/ZJ918E9nD3J5t5WkRERESE3t1zfBEwD/iC\nu68CrArsC8wHaoFGQa6ZHUQWGF8MrO3u/YC1Ul0AdWZ2aDPnvBT4F9Df3fsSQfIPU95PiMD4GWBX\nYAV3X50IcvsTgfy/C23aGLiJCIx/A2ydyq8CbAfcCmwEXG9my1bzpIiIiIj0Zr2553gusK27v1HI\n/yFwLjDH3TdNaQY8BWwOTHL3gyvU+wfgYOB5YFN3X5TLKz3JzwHbuft7FY5/DNgGOMjdJ1f5WH4P\njAAudPfjK+SvANwPfBY4wN2vraZeERERkd6qN/ccX1YMjJPSGOBNzGyVdHt7IjCG6MGt5KfpemNg\n5ybKXFwpME4WpusmxzvnmVkf4IB097xKZdz9Q6AUEO9RTb0iIiIivdlyXd2ALvSvJtJfzt2uAd4B\ndkz3/+Puj1Y6yN2fNLOXgQ1S+XsrFLunmfZMAQYBY81sCyKovbeZYPpzwArp9n3RuV1RaezxRs2c\nW0RERETo3T3Hb1VKdPf3c3eXT9drpeuXad5LhfJF/2nm2LHA/xEB72jgH8DCtFLFyWZWUyif72Fe\np5lL31Rm5RbaLiIiItLr9ebgeEms2HKRZn3SVIa7f+Du+wK7AL8gep49d/8pM/ts7pDS326eu1sV\nl93b2HYRERGRHk/BcXVKPb6fbqHchoXyrebu97r7GHffBehHTPJ7geiN/k2u6Nx03c/M1l3S84mI\niIhIRsFxdR5M16uYWcXJdma2JTHeOF++Tdz9HXefBPwgJQ3MTRJ8APg43f5me5xPREREpLdTcFyd\nWcT6wwCnNFHmjHTdQCyf1ipp2bWmlCblGWkSnru/BVyX0k8zs3WaqXs5M1u1tW0SERER6W0UHFfB\nYzHo09Ldfc3sIjNbA8DM1jCzC4nhDwCn5dc4boVHzOwcM9upFChb2Jlsk5F/FXbtqwPeJCbn3W1m\n+5lZeVy0mW1uZicAjxOrW4iIiIhIM3rzJiBD3f32JsqUnpRN3L0hl57fPnoR2fbRpS8ZLW0f3ai+\nQpn5qS6IiXsLgE+RrZjxOvBld59dOG4nYm3m9VPSx+nYVWk8gXB3d7+j0rlFREREJKjnuBXc/TTg\ny8CNRLC6KvAGsQTbVyoFxq2wL/Bz4C7glVT3h8BsoJ7YzW928SB3/xexbfQY4G5iiboaYijGA8QS\ncTspMBYRERFpWa/rORYRERERaYp6jkVEREREEgXHIiIiIiKJgmMRERERkUTBsYiIiIhIouBYRERE\nRCRRcCwiIiIikig4FhERERFJFByLiIiIiCQKjkVEREREkuW6ugEiIj2Rmc0B+gINXdwUEZGlVS2w\n0N036cyT9tjg+JInb3SAERNWLadtXHshACPqzwFgXP3b5byxUzcDYMqE7QHoX3dO7rh1ARhSc0pc\nkx3Xf2yqv/6UlJIdN64mXae8UXWX51q4ako7JUsqtasm1VGfZQ1Ief3rGpcFGNcvtSHVNW7S9lnb\n7xsMwIbnXxV5Y7K2D6mL21f96XJDRNpb3z59+qy+zTbbrN7VDRERWRo9/vjjvPfee51+3h4bHPdb\ndA8AC3+zVzltr0H/B8Csbx4GwHN1Wd6ohmcBOOmoCCIvbTiunLf1sVcA8OGES+J+Ljj+6gqfBeCQ\n8yJAHcD0ct45UzePNsyM8hPnHF/O23hyBOO147Mg99L6C1Pj42r0hNwDqo/6R0yI8qPIgv7RKSCf\nna7H7vBkOW/f6fFl65MLF0TZl5fNVXoOIu3FzGqBOcBV7n54lzame2jYZpttVp8xY0ZXt0NEZKk0\ncOBAHnzwwYbOPq/GHIuIiIiIJD2251hEpKs98vICauv+0tXNEBHpEg31e3d1E5ZIjw2OH/351QBs\n/42NymmnDz8dgIen7QjAE32uLeet+e+7ALj1id0AeGf4nHJev9W2AeCWPQ4EYOG0h8p5N50ZaXfs\nHGnPTcsN1ThvFgAzjouxwB9+57vlvPXXiWEVv35ltXLagcfF7SlTYxjHJ2vnxkvv8AwAe82JoRCr\n5cZLzyaNNU5jlU+ZuqCcN3hYHFd7V9TVcP3Ucl7dtDTmmPxYaBEREZHeS8MqRKTdmVmtmU0ys9fN\n7H0ze8DMvlah3IpmVmdms83sXTNbaGZ3mtmBTdTpZnalmW1pZpPN7DUzW2Rmu6cym5rZZWb2jJm9\nZ2ZvmtnDZjbezNaoUOfBZjbNzOaldj5uZqeZ2Yod8sSIiEi312N7jm88Nnphh43NemYHnBe3bzxo\nIACDhz9TzvtBffSo1u4Xk2dGXHRWOW/cRTGprX/trwGoqX+znHco+wAw/fCYAPhEvy3LeV86Kia/\nPTFzLAAc/H7tAAAgAElEQVRTD8+Wn5g480gAFkzLfnLoe8UuAOyyQ0zq23aFS8p51/8+JgrOqJ8L\nwD3vblDOe+9X2wFwzIWHAzB26L/LeWv94RUAPjMujp/yVDZZb97vfxA3BiLSnjYG7geeA64GVgeG\nAzea2VfcfRqAma0ATAV2A54ALgFWBr4FTDaz7d39lAr1bwbcBzwFTAT6AAvNbD3gX8TyaVOA64CV\ngE2AbwMXA2+UKjGzy4EjgZeA64H5wOeBs4Avm9ke7v5xOz0nIiKylOixwbGIdJndgTPc/aelBDP7\nA3ArcDIwLSX/kAiMbwG+XgpEzeynRHD9P2Z2s7vfXah/CPDzYuBsZscSgfgJ7v7rQt4qwKLc/cOJ\nwPgGYIS7v5fLOwP4CXA00KieSsysqeUotm7pWBER6X56bHB86YTSWsZZ2sPDY8zvkHnRa3tTXTZu\n9+F0u/+sGCc8fdq65bzV5uwKwA82ifHIR41/tpw3cX50uw7baisAVhk/vJw384oYf7zXDdGTe9Pc\nr5bzBg+OSToj5uaWVtshfkkesuUUAA65fGQ567TLo10/fjPGDl/35hnlvE33jrWMx+0Xbb50/G7l\nvKFX/xGAU58cBsDGOzxYzjv/mnMR6QDPAz/LJ7j7VDN7Adg5l3wk4MBJ+R5ad3/NzM4CfgN8DygG\nx3OBn9K0xRbFdPd3CknHAx8DR+YD4+Qs4BhgBFUExyIi0rP02OBYRLrMLHf/pEL6i8AuAGb2KWBz\n4GV3f6JC2X+k6x0q5D3k7h9USP8/YvHuS8xsGDFk4y7gMXf3UiEzWxn4LPA6cIJZxT1wPgC2qZRR\n5O4VByalHuUdq6lDRES6DwXHItLe5jeR/jHZJODSZIBXmyhbSq+pkPfvCmm4+/NmtjNwBvBV4Jsp\n60UzO9fd0y479AMMWIsYPiEiIlLWY4PjAePj/+BdI7NhiYNr4hfSUWNj6MPs3LAKRkb5AWmr5+cn\nZcuoTUz/6uvTBL59BmWT/PpNi2ERH13xewAO3D873zX7xFDE46fGZL95Ndn/9A3vimEf++x/Vzlt\n2fnRnvcnxi/Pr/49a8PJb8eku35DY5m3d7bN2j76zFJjos6Js44s571655oA/G5k/KlPn5odd8/6\npUl9oxHpZKX1BtdtIn+9Qrk8r5AWGe6PA8PNbDmid/grwLHAr83sHXe/PFfnTHdXz66IiDTSY4Nj\nEem+3P0tM3sW2NTMtnD3pwtFhqbrB1kCaQzzDGCGmd0N/BP4BnC5u79tZo8C25rZ6u7+ZnN1tcV2\nG6zGjKV0EXwRkd6qxwbHA+qiF/VScpPuUs9q//qYUDfqqOOyA2qil3bh+DSpjWyZ1WW2in9ua06L\noYVP9JteztvwqFgi7bwffh+A0et8ppz3s4djYtzomjhP/aDsn+QTM2NDkn5Tty+nPT/5swBcvMJl\nALxz6+blvPpj/g+AsURdQ+qyHurnJ6Rfi+ujV3lAfbbM28Sj4nrUvCg/+6BZ5bw/j4ye7LP/hkhX\nuAI4G/ilme1fGqdsZmsCp+fKVCUNqXje3ecWstZJ1+/m0s4DLgeuMLPD3b3RUBAz6wds4u5LFJyL\niMjSq8cGxyLS7Z0L7AnsCzxkZlOIdY4PANYGfuHu05s5vugQ4GgzuwN4BphHrIm8DzHB7oJSQXe/\nwswGEmOKnjWzqcALxFJwmwC7Ar8FRiIiIr2KgmMR6RLu/qGZ7QGcRAS2xxKT9h4i1ir+Yyur/COw\nIvAFYpWIPsDLwCTgV+7+SOH8R5vZLUQA/BVi8t+bRJD8S+D3S/jQRERkKdZjg+PpaZjEuFza4DTU\nYsDMGFYxuzQcARg9L4ZffG1kTGZ7cn42BGLXQZF23n0x5GLjqYeV8876MCb33XZMTNY770fZOse3\npT6niRNi0t3X7ssm2G26XwxzmDX5jnLal3aOchdPjqEWJx2Z7WZ342MxBGLc0DTsIzesojw9cHwM\ny9h4hax9Lx75MgDP9Y+VtQaMyR5X3Sax9vHZiLSduzcQq0A0lb97hbT3ieXXzmmH+u8jds6rmrvf\nDNzcmmNERKRnW6blIiIiIiIivUOP7Tku9RhfmkubXeqcShPyxuV6X/un2481xBDHX9VnE/kWPhl5\nU1aOHtljPsx2upu4VnQP9904Jsqts97B5byvPR4T+A74XRy3YUN2vonTYnm3/XNLxp334zjn78ZH\n2i8/zHqODxge85PGDIse5Cnj/5LVRZQ/6IboCf/cMiuU8z762X+izReeB8DVX8we19cuvCZu7ImI\niIiIoJ5jEREREZGyHttz3H9+9NKOnp/1lE5MPcd966P3dVRdVn5AffS+nkeMHR5dlw2B3Pi16MEt\n9RyPOyrr7d14k9hPYGEa73vHw9mY4yMe6Q/Ah9NuBWD/tbPjNj8v9h6Y+lq2UtS2h24GwISZcZ4V\nxmRtP/errwBw8X6xdNzYsedlj7U2xijPeD7WZJu89VPlvNOv/BEAJ3/7BQC+fVTW6z16g6w9IiIi\nIqKeYxERERGRMgXHIiIiIiJJjx1WMa4+hkUMmZ/tglcaRjFxTmk4QTZsYXoq/5UdpgBwyrBdy3mP\n/S6GVYydGhPYxhz073LewpoZANy05XUA1J9/ejmvjjkAHJTqXv76q8p5D9fclfKySXfT62KS3TET\nYlOwfWqOLOe99ORkAC6+IiYTvjjnu+W8g4ZfDkCfzWNZ1nmnvFjOu7E+2jV93Zisd92Phpbz1jw8\nhoLsxMOIiIiIiHqORURERETKemzP8ajUW/vw/NzeAjWlvOgxnk62CcgtqYf5lKeid3jh0GxrjOl7\n/xOAcXWRN25MVuU+R0Ydzx8Ux89be2w5ry8xKfCUmTFJ75SDsp1w+94QvbaHTh1WTtsttWuvf8QS\ncAsnZD3bL6XNSSYMTRP4pmabh+z/8YkATFs18kbNy9pQ6i0fdVBMNHxu7x+X865/enkATkRERERE\nQD3HIiIiIiJlPbbnmFLn6ZhsubL+/aIXefTU6NFdMHb7ct5L8+L2AxvEkmm7zc3yHt1hcwBGjZwF\nQN+jsjHH02tiKbeNbogl0jYedn45r9/8yBuxQ2zZPOq+bHzxajNjE5CbhmU92/0uHBztHPsQAAPq\ns4ezz32nRh1pQ5Ibt7u1nLfz/vMAGPK31QG4uN8G5bwB46P+EbVxfdex65bzpgwr9T7PRkRERETU\ncywiIiIiUqbgWEREREQk6bHDKkaVbozNhi1MLOWNibT+R51SzhuXdtQ7oHZ/ABbWblbOGzAhbvef\nGZPvhkzIlod76aRPAOj7ZAyJ2Ou1bBjH9JlpObmhB0ad07L2nTP1EgCOeSobajFihzTZbtKWcV2b\n1TUqVnJj4x3i+tADbirnDVw5Hu3CUTFk4tKGbNjHwyOjrYPr4vE1zMomIfY9Mz3+rCoRERGRXk09\nxyKyVDGzBjNr6Op2iIhIz9Rje45Hp+tLa7Ke40vLeZE2qibrmb0nbQjy7OTofZ04MutVHlAfva3j\nJkeP7KjJ2TJqA54cAsC8YSmv5vKsEeMjbex9sQTciBvuKmedE4exMNc73L/+13FjQizlNn385uW8\nhanH+KAtY1Lg3hN/UM57fsODAbjpqKhr9ISsd7i0GUppGbvZ9bmNT7aM8/wQEREREQH1HIuIdJhH\nXl5Abdr5UkRElg4KjkVEREREkh47rKJ/GjIxIDd0YkDasW56v5Q2Plvzd2raLe/ZbWNt4n3um1LO\nGzUvjhuwfQyTGLFJdtz31o5hCwfusxUAn5yZteGyCbELXn19lJ8yM5vkN2SF6E2q3/6actqIfnF9\n6n6xIx/3fbWct3FN7Ni36rdvA2CXNU8t5y2ceUW0r1+0c0R9NpSkNKxiXOl5qLminDegrjR05E+I\ndCdmZsDRxNzazYA3gBuAU5sovyKx2eMhwObAx8BDwEXufk2F8gYcBxwFbFqo/yEAd69tz8ckIiJL\nhx4bHIvIUu0CInh9FbgM+AjYFxgErAB8WCpoZisAU4HdgCeAS4CVgW8Bk81se3c/hcYuIQLvV1L9\nHwJfB3YGlk/nq4qZzWgia+tq6xARke6jxwbHpZ7jS3O9qP1Lt9P16Jrs/+WlHAnA+mky2+Ax2XGk\nHudLJ0SdB9dnE96uf/cqAL7bEMcf9O4l5bzvTYve532HHg9A3bRsV7u96lYD4MW5WQ/1lLOGxXVN\n9FBPrD+7nPfifdGbvMPPojd53i1PlfMmpkmA4+pj2baHcxPySEu5pfl/jKvJlnm7NDc5T6S7MLMv\nEIHxs8DO7v5mSj8VmAasBzyfO+SHRGB8C/B1d/84lf8pcD/wP2Z2s7vfndK/SATGTwGD3H1+Sj8F\nuA1Yv1C/iIj0IhpzLCLdzRHp+uxSYAzg7u8D/1Oh/JGAAyeVAuNU/jXgrHT3e7nyh+Xqn58r/2ET\n9TfL3QdWuhC92CIispTpsT3HA8ZH7+kocr+mlsbYHhTX/XPjkWvTJhtbbRs9xtveML2cN29+9LCO\n/vAVAJ6+Yddy3sUfR8/s7Ekxrnh0w9xy3lfWjqXSbntnLwCuvGF4OW/2zOhpPuugrHmL3rkOgL0G\nRbtGDct6eT/3dIw17vfZQwGYNX7Zct70tFzdiH7rLva4isbletIHNFNOpAvtmK7vqJB3JzGeGAAz\n+xQxxvhld68UjP4jXe+QSyvdns7i7s3XLyIivY96jkWku1ktXc8tZrj7J8TkuWLZV5uoq5Res4T1\ni4hIL6PgWES6mwXpep1ihpktC6xRoey6xbLJeoVyAAtbUb+IiPQyPXZYxfQ0EW3ADrmhA2kHuY2G\nHwjA85OziWuDJ8USZ6OuHgPAovnZZLURkyNvxJMPAvCNfb5fzus7KCbbTZkZk+f61+UaUR9XY+qi\ng2qladkwiQ/TpL79V9iynLbRnr+KOoffAMC8rbLhGzeeuBsAw77zi2jnsdlp+o9Nu/LNiyETs8nO\nM65ffmIh9K/LPR/1aZjJWoh0Jw8SQyt2A54r5H2R3OeWu79lZs8Cm5rZFu7+dKH80FydJTOJoRVD\nKtT/edrxc3G7DVZjRv3e7VWdiIh0AvUci0h3c2W6PtXMVi8lmtlKwM8rlL8CMOCXqee3VH5N4PRc\nmZLf5epfLVd+BaDxt0kREel1emzP8ayHRgLw/LZHl9NeOvYTAIYMi17hupnPlPP2bYjrm4dFr/Lg\nkdlEvn5HxDJqq54XPbnbrrpBOW/6PlF+wI9nRcKw48p5s9MeHuPShLlx+V7lreK4h3cu/2/mnNfj\nnG8OfQ2Ak+8am+U1RO/TObUxaW9KfqOPtMTcqNQrPGpe7jxjUjvT8m4D6nN5NaU6bkGku3D3u8zs\nIuBY4BEzu5ZsneN5LD6++Fxgz5T/kJlNIdY5PgBYG/iFu0/P1X+HmV0G/AB41MyuS/XvQwy/eAVY\n1IEPUUREujH1HItId3Q8ERwvIHaxO5jY6OMr5DYAgfISbHuQ7Z53LLFc29PAIe4+pkL9o4CTgLeB\nkcTOerelevqSjUsWEZFepsf2HLN/bM7x/DI/KCc1HBLLm065NdZtq/ly1vs64j/RzXvddTGvZ0Ru\nmbPpDXH9pXX/DMDhh2UbfUz/btQxZVaM812w4cBy3qTvpuvxUebSj3I9wddEL/RNf9qtnLbp+rGC\nVMPhsTFI/y+eWM47/ZOVARj90hnRvrU+Xc67dLlo6xHLxS/Q918+vpw3sO5cAJ6o+1Hcz+/7kY7T\nmGPpbtzdgYvTpai2Qvn3iSERVQ2LcPdFwPnpUmZmWwCrAo+3rsUiItJTqOdYRHodM1vXzJYppK1M\nbFsNcEPnt0pERLqDnttzLCLStBOAg83sdmIM87rAl4ENiUH4f+q6pomISFfqscHxrgwDYNyW+V9Z\nbwVgVNr/6uGns2EOdRfGpLuFW5wHwNfWf7Kcd/HXYze7YSf/BoDVPs4m5A15PU2oWzWGWszbMZvk\n98+6GMOw7WaxmtQl/5uNaaibFxPkhozMJvAtP+MaAFYaGTvmvrF/tiTbe1uvCcDYm2LDr9MnlXfV\nZfrVqa71o/7j18895PVjR70nfhd5A3JLu80u7R54HiK9zd+AzwL/BaxO7Ir3FHAhcEEa1iEiIr1Q\njw2ORUSa4u5/B/7e1e0QEZHup8cGx5cMiiXPFoyfWk7bti4my217Q/SYzpuWTbqbucLmAEwcGRPe\na9/ONtS6/72Y/LbCGdHzfE7uOKbFClGz50Uv76jUOw3QMOgdAB5Oy6dNn7BKdtyc6I1umP9sOanP\nuOg5/uZRtQD89k93lfNW+8m90b75sWTcqOOOL+ctrL0WgDvGpuXaclOM+m+VepXTBMNxdVnvdcOc\nAxERERGRjCbkiYiIiIgkCo5FRERERJIeO6zi7JkxnGJAGl4BMGRQDKeYNX8rABZNy9Ykvvr8mKh2\nx5Z9ARizRbb473c/jvWRLxz2PgAbTlqnnFc3LNVR2nlufjZs4eKj4vY5/eL+RjPXLef1nxN5fRuy\noQ3r16Rj01CLjbbMhoQcv06sLHXHpHg8N43NhnYsJOqdl4ZOHJTbBW9cTayjvFFNDNEY07BZOW/A\nhHjMj+R37hMRERHpxdRzLCIiIiKS9Nie44fropd4Yn22dFnfkdEz+/CYtJtdTdbLe9eYuH32/54A\nwOmHnlXO2/b66K3tOzGuH550RTlvYkMcN3h+Wnat7shy3kYN0QN89sxoy15js57j52svB6B+clbX\nn2tjZ7s/jTwagPs///ty3uUzo+d3yOTUhnnZeXbd4VsAnFoTdfatn1vO6z8+2jdqQnoO5mQ78s2r\nPwURERERyajnWEREREQk6bE9x31TD+64+gvLabPnlHqKozd5YX3Wa7twcqRNT73JfWcOL+fZb2YA\nMPHj6B0ePDLbnGPw8Lh919g4fuPaIeW8Cdt/NW6cFMvCDZn/cjlv4ye3B6B26OZZG96PscyfeXlX\nANa5IevZHTR1DgB/HL9/elz7l/PqNonHMaU+2nxpTW6jj7TJyMPE5iTT88/H/OxxiIiIiIh6jkVE\nREREyhQci0i3YWa1ZuZmdmWV5Q9P5Q9vxzbsnuo8o73qFBGRpUePHVax17C9ARg7P5u4Vn//P+PG\nUTGUYcDQbILcrnNj6MQ+Q/8GwC/P/Go579DaKH/Ax6Qy2bCFEanOF2+dEvWQDVvYZ5MYJjEq3Z8+\nbe9y3oA09GHXle8op43dfSQAg+u/C8DOA5fN6lp1AwA2nhznm35UtgTcGOIx9u0XwyR2mbllOW/9\n4atF+QnxWH8w6HIyaZjJDt9BRERERNRzLCJLtxuAbdJ1t/PIywuorftLVzdDRERaocf2HJ+yw2AA\nXpz0z3Ja7eTYAGPE9tFbOyW3Yce3B8cEt2NOejmVzXpYjzlzGABbjI6l1aY0XFPOG1cfG28MSPeH\n1GWT3CYOmgXA9JroTR6Rmwy3cJNfA7DsaX3KaS8eEb27I6aeDcDo8a9nD2jDq1L9McFuXK7Xe/TI\nqHdUbfRGj5uf5dXNjLxTzvxsJEz7bjnv4flR17WILJ3cfQGwoKvbISIiPYd6jkWkWzKzrc3sz2b2\nppm9Y2bTzey/CmUqjjk2s4Z06Wtm56XbH+XHEZvZOmZ2uZnNNbP3zGyWmR3WOY9ORES6qx7bc7zb\nEU8CML1f1qk0amT01o4bGT2sE8nGDp/yZGyu8efP3A9AvxOz3uGpX/0EgGNOinHI02ceV86b2BDj\nfQekjUFmk/VGT6yJpdhGjY+e3Jcm57ZuHh7nHsUr5bTfTk1Ly42dDsBtB/+5nHf9jpcAcNOguD6o\nNusd7js/HtfsWdFTPWTe8eW8BWMfAqCO2ERk4vxsy+z+NdnjEOlmNgHuAR4BJgDrAcOBW8zsEHef\nXEUdKwD/AFYH/gosBOYAmNkawN3ApsD0dFkPGJ/KiohIL9Vjg2MRWartCpzr7ieXEszsYiJgHm9m\nt7j7whbqWA94DNjN3d8p5P2cCIwvcPcTK5yjamY2o4msrVtTj4iIdA8aViEi3dEC4Mx8grs/AEwE\naoD9qqznh8XA2MyWB0YAbwFnNHEOERHppXpsz3HfkTGcYGJNNkFur2GRdvacqQCMrsuGGLw4KHaX\n2/T+7QCYPjYbtnDmz94H4OrvxLCIDcdk5xk3MoZRzB4f15dOyHbduzltsjd4ZCy7Nnb81HLe9H4x\nbGPjk9Ypp538nZga9/bIqOOepx8u542Z/3g8rjm3AjBk/qrZ40pDLG5Ou+EtqM3tfNcvhnaMaYgd\n8u4an+26N7uuNARkBCLdzIPu/laF9NuBw4AdgKtaqON9YHaF9K2BlYE704S+ps5RFXcfWCk99Sjv\nWG09IiLSPajnWES6o7lNpJe++a1WRR2vubtXSC8d29I5RESkF+qxPcfnbBk9rBePzCbITd+q1Buc\nllabnC2ttiAtf7bP67Hk2R6bPlTOe3/aPgD84IrodR13fTaRr47orZ1eFxPz9sptAjJ4frRhtZHP\nAjBvVtaWmyaliXwTsklxrxzzGQDe2zz+Zy88coNy3mN7xuS80e/ExLr8YMtx6Xp2TdQ/ri47z+j6\nUk9x/L+/tD7rcR7QL+vlFulm1mkivfQmrmb5tkqBcf7Yls4hIiK9kHqORaQ72tHMPlUhffd0PbMN\ndT8BvAtsb2aVeqB3r5C2RLbbYDUa6vduuaCIiHQbCo5FpDtaDfhxPsHMPkcMkF9AG3bEc/ePiEl3\nn6IwIS93DhER6aV67LCK6RPS8IGabIhBf2KIwWrDYyhDw9hnynn9iDWMJ86JnfFmbZnN9Tno5ZhT\ns9HU3QCYV7dbOW9e2mVvw8kxdOLgOdeV826eFRPwZs+LIQ0T67MhFBsdFLc3nJlNkBvy+5j49+0N\nPg3AS88MLeetUhvDNQ7+cfRCzc4NFxlyVKo3DaHoX5N7IlJaaeLguFxW7dA0rGLovoh0M/8Evmdm\ng4C7yNY5XgY4qopl3FpyCvBl4IQUEJfWOR4OTAG+3sb6RURkKdVjg2MRWarNAUYC9el6ReBB4Ex3\nn9rcgdVw99fNbDBwDrAP8DngSWAU0ED7BMe1jz/+OAMHVlzMQkREWvD4448D1Hb2ea3yZG4REWkL\nM/sAWBZ4qKWyIh2stCHNE13aCpHQmtdjLbDQ3TfpuOYsTj3HIiId4xFoeh1kkc5S2sVRr0XpDpaG\n16Mm5ImIiIiIJAqORUREREQSBcciIiIiIomCYxERERGRRMGxiIiIiEiipdxERERERBL1HIuIiIiI\nJAqORUREREQSBcciIiIiIomCYxERERGRRMGxiIiIiEii4FhEREREJFFwLCIiIiKSKDgWEREREUkU\nHIuIVMHMNjSzK8zsFTP7wMwazOwCM+vXynpWT8c1pHpeSfVu2FFtl56nPV6PZna7mXkzl5U68jFI\nz2Bm3zKzi8zsTjNbmF47v1/Cutrlc7atluvMk4mILI3MbDPgbmBt4EbgCWBn4Hjgq2Y22N3fqKKe\nNVI9WwL/ACYBWwNHAHub2S7u/lzHPArpKdrr9Zjz0ybSP25TQ6W3OA34LPA28BLxmdZqHfC6XmIK\njkVEWnYp8YF9nLtfVEo0s/OAE4GzgZFV1HMOERif7+4n5eo5Dvh1Os9X27Hd0jO11+sRAHc/o70b\nKL3KiURQ/AywGzBtCetp19d1W5i7d8Z5RESWSma2KfAs0ABs5u6LcnmfAl4FDFjb3d9ppp5VgP8A\ni4D13P2tXN4y6Ry16RzqPZaK2uv1mMrfDuzm7tZhDZZexcx2J4Ljie5+aCuOa7fXdXvQmGMRkeZ9\nKV3/Nf+BDZAC3LuAlYHPt1DPLkAf4K58YJzqWQT8Nd0d2uYWS0/WXq/HMjMbbmZ1ZnaSme1pZiu2\nX3NFqtLur+u2UHAsItK8rdL1U03kP52ut+ykeqR364jX0STg58CvgCnAC2b2rSVrnsgS6VafjwqO\nRUSat1q6XtBEfim9ppPqkd6tPV9HNwL7ABsSv2psTQTJNcBkM9uzDe0UaY1u9fmoCXkiIm1TGq/Z\n1gkc7VWP9G5Vv47c/fxC0pPAKWb2CnARMYH0lvZtnsgS6dTPR/Uci4g0r9RjsVoT+X0L5Tq6Hund\nOuN19BtiGbft02QokY7WrT4fFRyLiDTvyXTd1Fi3LdJ1U2Pl2rse6d06/HXk7u8DpUmjqyxpPSKt\n0K0+HxUci4g0r7Rm53+lJdfKUq/aYOA94N4W6rk3lRtc7I1L9f5X4XwilbTX67FJZrYV0I8IkF9f\n0npEWqHDX9etoeBYRKQZ7v4sscxaLXB0IfunRM/a7/Jrb5rZ1mbWaJcod38buDqVP6NQzzGp/qla\n41ia016vRzPb1Mw2KNZvZmsCv013J7m7dsmTdmNmy6fX42b59CV5XXdoO7UJiIhI8ypsa/o4MIhY\nk/gp4Av5bU3NzAGKmytU2D76fmAbYF/gtVTPsx39eGTp1h6vRzM7nBhbfAex+cKbwKeBvYhxnw8A\ne7j7/I5/RLI0M7NvAN9Id9cFhgHPAXemtNfd/UepbC0wB3je3WsL9bTqdd2RFByLiFTBzDYCziS2\nd16D2LHpz8BP3f3NQtmKwXHKWx34CfHPZD3gDWJFgB+7+0sd+Rik52jr69HM+gM/BAYC6xMTnt4C\nHgWuASa4+4cd/0hkaWdmZxCfaU0pB8LNBccpv+rXdUdScCwiIiIikmjMsYiIiIhIouBYRERERCRR\ncNwMM/uUmZ1nZs+a2Ydm5mbW0NXtEhEREZGOoe2jm3c98JV0eyExm/c/XdccEREREelImpDXBDPb\nFngE+AjY1d07ZeFpEREREek6GlbRtG3T9WwFxiIiIiK9g4LjpvVJ1293aStEREREpNMoOC4wszPS\ngulXpqTd0kS80mX3Uhkzu9LMljGzY8zsfjObn9K3L9S5g5n93sxeNLMPzOx1M5tqZvu30JZlzewE\nM5ttZu+Z2X/M7GYzG5zyS22q7YCnQkRERKTX0YS8xb0NzCV6jvsSY47zu7LkdwwyYtLevsAnxO5C\njW3MIUYAACAASURBVJjZD4BxZF9E5gM1wH8B/2VmvwcOd/dPCsctT2yfuGdK+pj4e+0NDDOzg5b8\nIYqIiIhIJeo5LnD3c919XeD4lHS3u6+bu9ydK/5NYovD0UBfd+8HrEPsKY6ZfYEsML4W2CiVqQFO\nBRw4FPifCk05jQiMPwFOyNVfC9wK/Kb9HrWIiIiIgILjtloVOM7dx7n7uwDu/pq7L0z5ZxHP8V3A\nQe7+UirztrufA9SncmPMrG+pUjNbldjzHuDH7v5rd38vHfs8EZQ/38GPTURERKTXUXDcNm8AV1TK\nMLPVgaHp7s+LwyaSscD7RJC9Vy59GLBKyruweJC7fwSct+TNFhEREZFKFBy3zQPu/nETeTsQY5Id\nuKNSAXdfAMxId3csHAswy92bWi3jzla2VURERERaoOC4bZrbLW+tdL2gmQAX4KVCeYA10/WrzRz3\nSgttExEREZFWUnDcNpWGShStuAT1WhVltLWhiIiISDtTcNxxSr3KfcxsrWbKbVgon7+9XjPHrb+k\nDRMRERGRyhQcd5yZZL27QysVMLPVgIHp7oOFYwG2TytXVPLFNrdQRERERBpRcNxB3P1NYFq6O8bM\nKj3XY4CViI1HpuTS/wq8k/KOLh5kZssBJ7Zrg0VEREREwXEHOx1YRKxEMcnMNoRYx9jMTgHqUrn6\n3NrIuPtbwPnp7s/M7Fgz65OO/TSxocgmnfQYRERERHoNBccdKO2mN5oIkA8AXjCzN4ktpM8mJt5N\nJNsMJO8sogd5OWKt4wXp2OeJNZGPzJX9oKMeg4iIiEhvouC4g7n7BGAn4A/E0myrAguAvwEHuPuh\nlTYIcfcPgb2JnfIeIQLsT4CbgF3JhmxABNsiIiIi0kbmrhXBlkZm9mXgNuB5d6/t4uaIiIiI9Ajq\nOV56nZyu/9alrRARERHpQRQcd1NmtqyZXWtmX01LvpXStzWza4FhwEfEeGQRERERaQcaVtFNpeXa\nPsolLSQm562c7i8CRrn7ZZ3dNhEREZGeSsFxN2VmBowkeoj7A2sDywP/Bv4JXODuDzZdg4iIiIi0\nloJjEREREZFEY45FRERERBIFxyIiIiIiiYJjEREREZFEwbGIiIiISLJcVzdARKQnMrM5QF+goYub\nIiKytKoFFrr7Jp150h4bHF99wC8cYKVrdiqn/fG/TwDg3HOvAuDRP369nDfjxVUB+NEy+wPw+cMH\nlvPWfG0wACe/fDQAK557dDlv0aX9ALjghv8D4Lx9v1TO+/n78wCYcv4lAJx2zf3lvGfv/Flc73dB\nOe36Xx8GwFqbvhpt+fRV5bynlo/ljC/aL9Lm7fFkOW/LiVHXlOHR9rs//1g57+5r6ho99tPOJ8v7\n/BgAHjroIUNE2lvfPn36rL7NNtus3tUNERFZGj3++OO89957nX7eHruU203PmwPsec3O5bQ/3hAB\n4j+IIHSd49Yt5929yzEArLRLBL5rzv1VOe/NKbsBcOyy/wDgdxPnlfP+dlMEzsP/O4LjZZ57sZz3\nfx9F/VMOjOD43IbrynkzFkUAO+PcunLal4jA+sDXIjg+dvAl2QO6Jq7WfCDadf7f/lPOGvH47QDs\ntUHcv+uax8t5e70Q51zwowji5y+zUfaYr4lAe7m5Co5F2puZzdhxxx13nDFjRlc3RURkqTRw4EAe\nfPDBB919YMul24/GHItIt2Jmx5nZY//P3p2HV1Vd/x9/LyxCHYoEC1IMpApWhTqBDMGRX6uCVov6\nRRCc2lqplSAKVSsqKCoKioBStBMyyFAtVVtwqDg1ESg4VcUKWCRShAqKI4i6f3+sfc+5hgQCBhJu\nPq/n4bnJXufss6/PNWxW1t7bzD41s2Bml1b3mEREpPbI2bIKEdn5mFlPYDTwAnAHsAGYW62DEhGR\nWiVnJ8c3POevC0lrjtd/4WUOzzf38oUuD6Q1wIMeGA5A1/sPAmDdwLR04idntgJgzq/8+7HFnyex\nv152FgD7zlgCQN+mVySxIy7xeuJhzfx5C59Nyypm9/DyjXWd9kzaOi/3sb66i5d4PN/ioSQ2a4CX\ngrw6ujcA1/RfmcQe/OHPAHi/n9ceXzNgThJbeNnB/l47nw7Anzu/k8TmxJKTExCpMU7JvIYQ/lut\nI6kCr6xYR8GVf6vuYYjINlo2/OTqHoJUA5VViEhN8h2AXJgYi4jIzilnM8e9+nsWdcMDZyRt9eNC\nvLGjPgTgnLrp4rQu/4gL6Tr5IrhVv0rzqfv+8UQA3ipYDMDMdj9PYm/c6Yva5tzqO18suTXNODf5\n0r9uO8oX3627LV0cOOs5zxz3arpH2tY8LsB7ztvmdLwziY1+wLPIk87cCEBxp3Qh34yrfQzzJvii\nuwZf3pfE/vwrf19vJG+1eRIbNsMz2cocS3UzsyHAdVnfJyuFQwgWv38a6AkMA7oC+wA/DSFMiPc0\nBQYDJ+OT7HXAs8CNIYRNVsWZWQNgKHAmsDe+5do9wF+ApcC9IYTzq/SNiohIjZezk2MR2ak8FV/P\nB1rgk9ay8vD644+APwNfAqsAzOy7wD/wSfEcYCqQD/wfcLKZnRFC+GumIzOrH687Aq9vngI0AK4G\njt6agZtZRdtRHLg1/YiISM2Qs5Pj+nGbsvoj07riWQPjPsXLvW1Qq1OS2B7jPH96Wj3fBm14l2eT\n2PP9/fqucZ/ivo1bJbHBr3uW95P3Gvj9+7dJYi828Exz7wFeVzzz8jQTXDLjIwBWnZmOuW0zrw+e\nM8qf06v4kiTW/Bnvq+t8fw9dnkn3Wu7Xy8fQ4plCABp8mW7lNrr3FAB63v9YfG66D/PM+V5fzduI\nVKsQwlPAU2Z2HNAihDCknMu+D0wCfhJC+LxMbDw+MR4cQrgx02hm44BngHvNrEUI4aMYGoRPjKcB\nZ4e4p6WZ3Qg8X1XvS0REdj6qORaRncVnwMCyE2Mz2xevDloO3JodCyGU4FnkPOD0rNB5eOb5qpC1\n2XsIoRTfJaPSQghty/sDvL41/YiISM2gybGI7CyWhRBWl9N+eHx9NoSwsZz4nOzrzOxbwP7AihDC\nsnKu/8fXHaiIiOy8crasohAvTRjW6bykrct0XxDX7czMSXR3J7GGr7zi953vf4/Wv21YEjv6R/53\n70E/9YV4Rw7okcRKZrwAQJN+3vcBG19KYi16+GK4hy/2v6/H7v/PJLa0nS+Gu2JKetTz+h5xK7Yi\nH1/f4S8nsRsunuZjKPbnzLo/3ZLtgM/9ukErvHxjVc90Qd4erbyvjsvrAvDTIel/j6lt0j5EdgIV\nfWAbxNeVFcQz7XvF12/F11UVXF9Ru4iI1ALKHIvIzqKis+7Xxdd9Kog3LXPdB/G1SQXXV9QuIiK1\nQM5mjtuuuAWAEdyVtHUp9azpYHyx3j3PXJTEni6+AIDiZn79o4fWS2Lnd/eDPfqe4gvx6q9It1/b\n+NTeALx9q/+9fOHDf0pi+/717wC8e6s/96cdGiWx58+4HIDrGjdN2lZ0HwzAxB6+oO6M09It4344\noT4A/z3Us9bD+CiJtVjhibGJGw8DYGbjNHu9vnlLAJb38vd1xPyDk1jJ/f6qLc5lJ/dCfD3KzL5R\nzmK94+Pr8wAhhA/M7E2gwMwKyimtOKqqBtamWQMW6hABEZGdijLHIrJTCyG8DTwOFACXZsfMrANw\nNvAeMDMrNBH/+XezmVnW9fll+xARkdolZzPHIlKr9AWKgRFmdgKwgHSf4y+BC0IIH2ZdfyvwY/xQ\nke+Z2WN47XIPfOu3H8f7RESklsnZyfGcUV7K0H1k2tZtgO8N3HXGkQDslXViXZ21vuBtXtxb+PAp\njyaxJsX+W9tzRy8BYGFpwyR2wMZYYtHM+zykblr2OLGOH0t3Tb7vJ9yAtISicL4vlLtmVLpArniZ\nL+CbdZbvaTw2q+zjOyf9AoATT/M+l/4+LbnosvJn/p7n+vu7cODZSezVqb7Ir+X9PubC7mlJyAGj\nMjtWJdvCiuyUQghvmlk7/IS8bsBxeG3xI/gJef8sc/2nZnY8cD1+Qt4A4D/ATfipej8mrU0WEZFa\nJGcnxyKy8wkhHFdBu5XXXuaaFcAvtuJZ7wNF8U/CzC6MXy7a5CYREcl5OTs5Lunor2075ydt/UZ5\nBrfx1PMBWF3wYBLrMN4zq8Nu9d+8Du5xWxL759X3AFBcx++/YHqacZ7AlQA0OP09AArP+F0Se+gy\nv65FgS+mW3d5eqrd2GJfIzS2fZoBHtHMs8m9mvviuQ0r0vfz8rhZABw72hfUzVrROokN7tkcgAuP\neQiAvb5IT8F7e+GpADRp74sQj1iYZr3nTPcsd6tDlDmW2sfMvhNC+G+ZtnzgGuBz4K/l3igiIjkt\nZyfHIiJb8ICZ1QUWAu/jC/pOAXbDT85bsZl7RUQkR+Xs5LhraczSzk23cpvznGd+23b235becux+\nSezcY/x8gZ6dPfv6x7pphnXpjD8DcE9Tr9d9vln9JPbgEf8HwA/a+dZvMyalz3voUN8M5IZONwHQ\nr1v629slxRsAaNA/rQ/u9aUf5tHidj8g5OhJadZ7Vv6TAMzu5WMetjwdX4/OvpVblxVpZjrjhqM9\ne33pBq9tvuCWNKu8bhf91lhqtUnAOcAZ+GK8j4B5wJ0hhD9X58BERKT65OzkWERkc0II44Bx1T0O\nERGpWbTPsYiIiIhIlLOZ48E9vIRiTscjk7b6zXw3p1V3FgIw9Ie/SWKvLvMShkEDfeuzt595M4mt\n6nQfACOaeknC9249LIk9tHeBx2728ogJdR9IYl2e8xKLETN8MVzhipIk1nq0l0LM6P50Or5S38pt\nRAc/iW9m+7R04tUm/uw92r8CwBtj00PAunTykon1K/w53b9Mt4wbNsBjF/b32Krp6VZu3fbNlGFk\nn40gIiIiUnspcywiIiIiEuVs5njdfF9YN6jOQ0lbl/meRb3qBV8Mx6/uTGPsCcDGjasBGLehWxK7\nY+N6AI489N8A9J11dBKb09Gzr11i1vblH56TxL645u8AHNbjLwBsuP8HSez8Sd5Hl2XpQR+te30E\nwD+v/CYAUwfel8RKOm4E4ICNntHere8ZSewH63zMHUfNBqDb8qyDReKBJxc80xKAmbu8k8QKZ8wB\n/BgxEREREVHmWEREREQkkbOZ43Pr+pHKfUvTQznu6O2Z2NNG+yFau1x0RxK7otjrgy8sOB2AfivS\n7Gv/17oC0GXuLQDMaJ6eRDu+7qUAPLjC2z6J28UB1O/kNcNNOiwH4NLi95LYyNP861U90rzt1PmD\nASic5gd9tJ2f9tVllD/7msO9pnnggLR2uMfVPQGYM8Nrhwdn1Vkz1zPardv7YSCDi+9NQuufi++x\nKyIiIiKCMsciIiIiIglNjkVEREREopwtq2i2YDEAh3X8U9L28mpfnHfk4b6o7Z5lpUmsV38vfXj1\nGF/otuSivyexkksaAdCl+XwAZq6Yk8Te6P0CAG2neNvL+81IYsUFJwKwtLGXdhz9+N5JbOIoL2/Y\n98GhSdtbBV728ey4QwD4zyPpFmuN+3vZx6g/dPYxzE8XGl64wPu6qYGXTuw7Oo0NyvdT8Nb/2Us8\n1j2bLuRbOMMXE36/6xhERERERJljEdnJmNkyM1tW3eMQEZHclLOZ4+sf9u3a5p57U9J2xeJPAHj0\nbs+U9qJuEpvTybPIG57xgzqmNkuzyk2e9W3Q6nfyBXxzzrwyvS8uams9zxfWdVtRmMZmeub41Y3T\nABj7ZboAsHX+CQAUNk0XDPa+piMADy72rdnOGJJmdMc3beexxg2873M+TGIT6k8BoOEjPoZv904X\n+V04dbiP/QFf5Ldkn3RB3rrOtyAiIiIiqZydHIuIVLdXVqyj4Mq/Vfcwtsqy4SdX9xBERKqVyipE\nRERERKKczRyfc4wvyOs6sH3S9vfxjQEYVzwIgMIVP0tiY6f6CXRTS30f4ZnNT09ie630U+WWrDwc\ngKUb0/2KX93oJRdv4YvbCpvdlcRGPe4n4t1z7XTv594rkliPzl6OMZh0gdwtDx8LwHpiiUezB5LY\nuDe9PKRFU3/2OU0OSWL19zgNgCd5C4DfDLgkiZV08j2Puw2IJR7907KKewbG0pH0P4NIjWBmBvwS\n+AWwP7AGmAlcXcH19YABwNlAS+Bz4CVgbAhhRjnXG1AEXATsV6b/lwBCCAVV+Z5ERGTnkLOTYxHZ\nqd2BT15XAvcAG4HTgA7ArsBnmQvNbFfgUeBY4HXgLmA34ExgupkdFkL4dZn+78In3v+N/X8GnAq0\nB+rG54mISC2Us5Pjm//tdX63tF2ctLU87REA3up1DQDn5Ken4C2d4Nu7rT/cF66NvTmtE/xo2tN+\nTfu/AlC8/KMk9uSZnlWe2M8zui3ys/5OnesvrZf7grlVLVolobfO8fvOXXBP0tZ3o2eY9xzn2eQG\nXz6axB6dVN+fnb8SgL3HnZLGGnqGev/hPq76A/6QxOYM9L7qj/L32vzWdKFhi2aeyf6MgYjUFGZW\niE+MlwLtQwhrY/vVwJNAU4i/JnGX4xPj2cCpIYTP4/VDgfnAVWb21xBCSWw/Gp8YvwF0CCG8H9t/\nDfwd+E6Z/rc03oUVhA6sbB8iIlJzqOZYRGqaC+LrjZmJMUAIYT1wVTnX/wQIwGWZiXG8fjVwQ/w2\nu3go86/iGzMT43j9ZxX0LyIitUjOZo5fbOPZ3vsn7Ze0TbzWM7Gjm3vb69e8ksTGd/4UgBanP+av\nA7oksQZ9fSu2D8/YB4Cu76xJY196JpeOfn3JqEVJ7MIC//t4+ExPIe9xa0kS63Wzb9e2akyahea5\n1wB4eokfFnJ8x98noXMH+tZtP/3JcQD84PLVSeziWd8B4LsD/NndbkvrrH/b3se1ntfi+2uaxAq/\nSOujRWqQI+Lr0+XEnsXriQEwsz3xGuMVIYTXy7k+c2LP4Vltma//Uc71c7P7r4wQQtvy2mNG+Yjy\nYiIiUnMpcywiNU2D+LqqbCCE8AW+eK7stSsr6CvTvtc29i8iIrWMJsciUtOsi69NygbMbBegUTnX\n7lNBX03LXAfwwVb0LyIitUzOllXUm3EQALv0+mfSVv9ALyOYu8wXz30x7dYkNuW0vgC8WLcNAA9O\nrZfEmjzrZRVNBjYHYPyxfZLYq+18qzSK46l5lx+cxEY084V467/0Ldm6DdgjiQ2e4ifXzWn/WDro\ny33x3KkLbgPg4kPTso+13/HFfKOPPAyA36/bkMRaF/hJd21X+2+Qb4/bygH0bVcMwIWdvXzj9FHD\nk9isHvdlrkKkBnkeL0c4FnizTOxosn5uhRA+NLOlwH5m1iqEsLjM9cdn9ZnxAl5acVQ5/XekCn8u\ntmnWgIU6VENEZKeizLGI1DQT4uvVZpaXaTSz+sDN5Vz/B8CAETHzm7l+b+CarGsyJmb13yDr+l2B\nmxARkVotZzPHrUcvAWD8gjRrM/5B33Fpl39PBmDJaUuS2FOfFABw6vM9AZh3w31J7MjTfVu3PVou\nA+DZlyclscMv88V2g5t5Vnh250uTWK8Zvu7n3fVdAaj/zoAktue4ZwG4/frOSduB13gGt+PV3leD\ncWn2unfnGwFYesUYALoMuDOJrevkC/BeiZnpXr86NYkd3cwzx2P7e/Z6UOmeSez54vRQEpGaIoRQ\nbGZjgX7AK2Z2P+k+x++xaX3xSKBrjL9kZrPwfY7/D2gM3BpC+EdW/0+b2T3Az4FXzeyB2P+P8PKL\n/wJfbse3KCIiNZgyxyJSE/XHJ8fr8FPseuEHffyArANAINmC7Yekp+f1w7drWwycHUIo71+BvwAu\nAz7C64rOxvc4/iHwLdK6ZBERqWVyNnM8c5cHAVjF+qRt3eW+RdqFBR8D8OHFzZLY0d17A9Dg3VkA\nHPu9tFb5qotbAPCvq2YDcOKxjZNYv/p+mMew/l7v2/X2W5LYq9N867h1Z3iN86CCnyexyS95hvrU\n436UtN00scD7PMt3hqr/0vFJ7O19vKb5gH387/m3+6cZ4PM7ng3AzXn+b515n9yexGZtaAjAu7HO\n+u22aTa6wdSLAPiMtxGpSUIIAbgz/imroJzr1+MlEZUqiwghfAmMin8SZtYK2ANYVN59IiKS+5Q5\nFpFax8z2MbM6Zdp2w4+tBpi540clIiI1Qc5mjkVENuNSoJeZPYXXMO8D/D9gX/wY6j9V39BERKQ6\n5ezkeGz70wBY3T9dINfzTC8/qH+Mb5l2ZsO0/OCh03YFoLDH9/z1kRuT2ClPDAPgX+f4qbMz5/8i\nvW+4J5jeWP0bAGbc9kA6hhleVtHyaF8w36PuPUns7d96icXCE9NFgZf289/kjrvIT7+rX5z21bz/\nywCMaOzlEV2WvpfE3u3uC/FevdgXDp5fkG75+tODfOHfhnN+B8Dgp/dOYqN7p9u6idQyjwOHAicA\nefipeG8AY4A7YlmHiIjUQjk7ORYRqUgI4Qngieoeh4iI1Dw5OzluMtUzq6tXH5m0tZ7vi9OOblwf\ngO6Pp7E7jvZscr86nqFdXzAniX3x3iAAOhzqv2k94PDDk9hhj/j2qbfP9OvnFi9MYm1X+KEjF670\nxYF1fv+TJHbaAD+k5P0JtyVtK5Z5pvnoG/3Qkbw2yRasjN/7aADqz/DnNGmfLtb7bX9fBPj+Ss9w\nL8yqpPzn8z7WwSv8EJBbxqUHkfT4c75/sSA91ERERESkNtOCPBERERGRSJNjEREREZEoZ8squpV6\n+UCPK9NFbTNH+8K4fqedAMD+p9ydxN74qb8+vNsyAFocky7ka9LfyxVaFywGYELv85JY2xfuBWDd\ncN93eOQpBUlsYbHvCvX38x4HoPmUaUls448u93F2yE/aLr3Y9x0+8IfdAVjZI10wNyvf92T++X4v\nADCo+fwk1n1f3wO5xT/8NLypu3yUxGbjp/OR720PDUxjzWY+hIiIiIiklDkWEREREYlyNnPcZYBn\ndPfgkKRtdjPPIne58zUAJjZNs8ozbt4AwG5X+/XXzExPoDtkvWdkxzY+FYA7xr2SxD5q1sb7XvYY\nAOt7FCexvS7zjPOqSfsBULdVXhLbe4Ev3JvcdnHSlpfnh3vt1d63nHvht6uT2LQVnu1eOCCO+bmD\nkljby/x13f0DAOgRFwACLG3nCwyXLPP75ky5JIl1P82fU/qxzjsQERERAWWORUREREQSOZs5XnW/\nv06dmW5ddsTMlQC0HuhZ2Au735fEuv2oFICPZ60HYOOpzZPY+jaeYb2wkx+o0aVjyyTW9leeoT6h\nrm+H9ljdT9NBtPD/vMXFnqmuX5yO5fbePwPgtMbpISDrdtkIQNNHdvH3cMWJSWxmZ68PHvuFH/Dx\n6oy0r1kzvOb4iBW+Nd36GYuS2AUbfSu3OU291rjfwMFJrHvWeEREREREmWMRERERkYQmxyIiIiIi\nUc6WVVx70ToADvlhw6Rt+cr2AEwd7d9fcXFaVvDq1CsAOHVpZwDeXr0yie217Hd+TdveAFxwa9bJ\nesd5ucJ13X2btrn3pqfgPXv3twFofZqXRLQd1TmJndrZxzV4eXrS3YRjfIu45rO8jGPkP4YlsSPw\nE/9GnOGlF/VIt3Kr39xLO/o28/fz6vzS9D9Es38C0OWZLgBs6JVuQzfo8tjHE+sQqQnMrAD4D3Bv\nCOH8Slx/PvBH4IIQwoQqGsNxwJPA0BDCkKroU0REdh7KHIuIiIiIRDmbOT7svXcBGHnhL5K2jpcU\nAtAgZlNnD3w5ifXftwSAXnF7t313SQ/IaN7dF9SNPNMzrWPb35DEPooHbqya6Qvspta/IIltbHQd\nALsNawHAVWu/SGLfPu42AAaPOj9p+/NoX4D32Cme5f3jMb9OYmNf6gtASekcv6/Hk0lsdg/f1q3e\ngF8C0KXzGUms1zM+1uaXeZ/rv1GYPq+OL87rj8hOayYwF1i5pQurwysr1lFw5d+26zOWDT95u/Yv\nIlLb5OzkWERyXwhhHaC6IBERqTI5Ozm+4Bmv0a3fLK3N7bHAv14/zo9svrDVLUls5MRGAAz6lW+t\nNvaqtB557+P8cI3WvTwL+3bxXUlsRCd/ndnCj4p+t9c7SWx2D98q7uPdfg7AopvrJ7FTGvrhIU1+\ncmPS9t9R3m/Tjl4zPHtOWttcvOJ/AIw/ww8nKZmxTxJrMNCz3deM8ozx+uZzktgcL7OmywyvS75j\nVHooSucVsR77+mMRqWnM7EBgOHAMUA94Abg+hPBY1jXnU07NsZkti18eAgwBTgeaATdm6ojNrAlw\nE3AK8C3g38Ao4K3t9qZERKTGy9nJsYjs1L4LPAe8AtwNNAXOAmab2dkhhOmV6GNXYA6QBzwGfIAv\n9sPMGgElwH7AP+KfpsD4eK2IiNRSmhyLSE10DDAyhDAo02Bmd+IT5vFmNjuE8MEW+mgKvAYcG0L4\nuEzsZnxifEcIYUA5z6g0M1tYQejArelHRERqhpydHDfY10sUZnVPyyp6col/cetwAA7nl0nszLlv\nAjCijpdc/HxMfhJ7uIcvtms92tsWFqQn1w2+3UsgCld4X0denG6/1rj/pQD02M8X/k2acngS6zLD\n6x0mPrNn0jbsOd9mrd/lPva/jlqcxL785goApvXykoniXpcksQ35Pq71K3xx4LAe7ZPYqh7e18wz\nfXz7Nk+3ctu44U1Eaqh1wPXZDSGEBWY2BTgP6A7cW4l+Li87MTazukBv4EO85KKiZ4iISC2krdxE\npCZ6PoTwYTntT8XXw8uJlbUeeLmc9gOB3YAX44K+ip5RKSGEtuX9AV7fmn5ERKRmyNnM8dhmfqhH\nt07p4rk/rvSMca8BnnDq84euSaz3/fUAmJN/PgCzv0gzznU3+jZv/Zb7dmglz6aL9bqN9G3UOses\nb5PSNGu7oYdneR8a6dnlvv3eS2ILz/Ds86DSB5K2YT28j4WdfJxdmqXvp8uZ/jrnfs8OdzvztSQ2\n8HLPIrcZfRBlrYvbus2e7+/5yIsuT2Jz+74EQKdN7hKpdqsqaM+seG1QiT5WhxBCOe2Ze7f0WD5F\nhgAAIABJREFUDBERqYWUORaRmqhJBe2ZbVoqs31beRPj7Hu39AwREamFcjZzLCI7tSPMbM9ySiuO\ni68vfI2+Xwc+AQ4zswbllFYct+kt26ZNswYs1CEdIiI7ldydHN8eXx9IyypmH+0L1wrjfsLv1/ld\nEjt3gZ9mN7ZOLH2YcUUSe+vOugAUz/B9i6cWLEpiF1zyKACzVvieyROz1vHUaxYX1JX69XvXSU/d\nu/CdEwDoXqdp0rZXXDNfGBfU1R+ZrjeatcL7Le7h35d8WZr1Zr3UYmHcf7nt3OFJpPVoH9cBi30f\n5l9em/6yYE5xF0RqqAbAtUD2bhXt8IV06/CT8bZJCGFjXHR3Ib4gL3u3iswzRESklsrdybGI7Mye\nAX5mZh2AYtJ9jusAF1ViG7ct+TXw/4BL44Q4s8/xWcAs4NSv2T9AwaJFi2jbtm0VdCUiUvssWrQI\noGBHP9fKX68iIrLjmVkBflDHvcAtlH9C3qNZ15/PZk7ICyEUbOZZ++An5P0I2AM/Ie8OYBnwJDA0\nc5reNr6XDcAuwEvb2ofIdpbZi1s7q0hNdSjwRQih3o58qCbHIiLbQeZwkLitm0iNo8+o1HTV9RnV\nbhUiIiIiIpEmxyIiIiIikSbHIiIiIiKRJsciIiIiIpEmxyIiIiIikXarEBERERGJlDkWEREREYk0\nORYRERERiTQ5FhERERGJNDkWEREREYk0ORYRERERiTQ5FhERERGJNDkWEREREYk0ORYRERERiTQ5\nFhGpBDPb18z+YGb/NbMNZrbMzO4ws4Zb2U9evG9Z7Oe/sd99t9fYpXaois+omT1lZmEzf+pvz/cg\nucvMzjSzsWb2rJl9ED9Pk7exryr5eVyRb1RFJyIiuczM9gdKgMbAg8DrQHugP3CSmXUOIaypRD+N\nYj8HAHOAacCBwAXAyWbWKYTw5vZ5F5LLquozmmVoBe2ff62BSm02GDgU+Ah4G//Zt9W2w2d9E5oc\ni4hs2Tj8B3FRCGFsptHMbgcGADcCfSvRz034xHhUCOGyrH6KgNHxOSdV4bil9qiqzygAIYQhVT1A\nqfUG4JPiJcCxwJPb2E+VftbLYyGEr3O/iEhOM7P9gKXAMmD/EMKXWbE9gZWAAY1DCB9vpp/dgf8B\nXwJNQwgfZsXqxGcUxGcoeyyVVlWf0Xj9U8CxIQTbbgOWWs/MjsMnx1NCCH224r4q+6xvjmqORUQ2\nr0t8fSz7BzFAnOAWA7sBHbfQTyfgm0Bx9sQ49vMl8Fj89vivPWKpbarqM5ows7PM7Eozu8zMuppZ\nvaobrsg2q/LPenk0ORYR2bzvxdc3Kogvjq8H7KB+RMraHp+tacDNwG3ALGC5mZ25bcMTqTI75Oeo\nJsciIpvXIL6uqyCead9rB/UjUlZVfrYeBH4E7Iv/puNAfJK8FzDdzLp+jXGKfF075OeoFuSJiHw9\nmdrMr7uAo6r6ESmr0p+tEMKoMk3/Bn5tZv8FxuKLSmdX7fBEqkyV/BxV5lhEZPMymYgGFcS/Vea6\n7d2PSFk74rP1O3wbt8PiwieR6rBDfo5qciwisnn/jq8V1bC1iq8V1cBVdT8iZW33z1YIYT2QWUi6\n+7b2I/I17ZCfo5oci4hsXmYvzhPilmuJmEHrDHwKzN1CP3PjdZ3LZt5ivyeUeZ5IZVXVZ7RCZvY9\noCE+QX53W/sR+Zq2+2cdNDkWEdmsEMJSfJu1AuCXZcJD8SzaxOw9Nc3sQDP7yulPIYSPgEnx+iFl\n+rkk9v+o9jiWrVVVn1Ez28/MmpXt38z2Bv4Yv50WQtApebJdmVnd+BndP7t9Wz7r2/R8HQIiIrJ5\n5RxXugjogO9J/AZQmH1cqZkFgLIHKZRzfPR84CDgNGB17Gfp9n4/knuq4jNqZufjtcVP4wctrAWa\nA93wGs8FwA9DCO9v/3ckucbMfgz8OH67D3Ai8CbwbGx7N4QwMF5bAPwHeCuEUFCmn636rG/TWDU5\nFhHZMjPLB67Hj3duhJ/E9BdgaAhhbZlry50cx1gecB3+l0RTYA2++v/aEMLb2/M9SG77up9RM/s+\ncDnQFvgOvrjpQ+BVYAZwdwjhs+3/TiQXmdkQ/GdfRZKJ8OYmxzFe6c/6No1Vk2MREREREaeaYxER\nERGRSJNjEREREZFIk2MRERERkUiT46/JzM43s2BmT23DvQXxXhV+i4iIiNQAmhyLiIiIiETfqO4B\n1HIbSY9CFBEREZFqpslxNQohrAAO3OKFIiIiIrJDqKxCRERERCTS5LgcZrarmfU3sxIze9/MNprZ\nKjN7yczuMrNOm7n3R2b2ZLzvIzOba2a9Kri2wgV5ZjYhxoaYWX0zG2pmr5vZp2a22symmtkBVfm+\nRURERGo7lVWUYWbfAB4Djo1NAViHH0/YGDgkfv1cOfdegx9n+CV+5Obu+Hnf95lZkxDCHdswpHrA\nk0BH4DNgPfBtoCdwqpl1DSE8sw39ioiIiEgZyhxv6mx8YvwJcA6wWwihIT5JbQFcArxUzn2H4meG\nXwM0CiHsBewD3B/jN5tZ3jaM5xf4hPw8YI8QQgPgcOB5YDdghpk13IZ+RURERKQMTY431TG+Tgwh\nTA4hrAcIIXwRQlgeQrgrhHBzOfftBVwXQhgWQng/3rMKn2D/D6gPnLIN42kA/DyEMDGEsDH2+yJw\nIrAGaAL8chv6FREREZEyNDne1AfxtelW3rce2KRsIk6uH43fttmG8bwF3FdOv+8Cd8dvz9yGfkVE\nRESkDE2ONzU7vp5mZg+Z2elm1qgS970WQvi4gtiK+Lot5Q9PhxAqOkHv6fjaxsx23Ya+RURERCSL\nJsdlhBCeBq4FPgd+BDwAvGtmi8xspJm1quDWDzfT7fr4WncbhrSiErFd2LaJt4iIiIhk0eS4HCGE\nG4ADgKvwkogP8MM6LgdeM7Nzq3F42ay6ByAiIiKSSzQ5rkAI4T8hhOEhhJOAPOB44Bl8+7txZtZ4\nBw3lO5uJZeqivwDe2wFjEREREclpmhxXQtyp4il8t4mN+P7F7XbQ44+tROyVEMJnO2IwIiIiIrlM\nk+MytrCw7TM8Swu+7/GOUFDeCXtxz+Sfx2//tIPGIiIiIpLTNDne1EQz+6OZnWhme2YazawAuBff\nr/hT4NkdNJ51wG/NrE88vQ8zOwSvhf42sBoYt4PGIiIiIpLTdHz0puoDZwHnA8HM1gG74qfRgWeO\nL4r7DO8IvwGOAyYBvzOzDcC3YuwT4P9CCKo3FhEREakCyhxv6krgV8AjwJv4xHgXYCnwR+CIEMKk\nHTieDfhiwOvxA0F2xU/cmxbH8swOHIuIiIhITrOKz5eQ6mRmE4DzgKEhhCHVOxoRERGR2kGZYxER\nERGRSJNjEREREZFIk2MRERERkUiTYxERERGRSAvyREREREQiZY5FRERERCJNjkVEREREIk2ORURE\nREQiTY5FRERERKJvVPcARERykZn9B/gWsKyahyIisrMqAD4IIXx3Rz40ZyfHBcXzAkDp4qVJWz67\nA1Aavy/N+3iTGI32iN9nydsjfvGR37d20+flJ9dkWfvRV5+35qNN7+PNrK+/2n/2c/LXfLVPvhJL\n3wdAfjnjK1m8KvaZjuGsxf767MGzbdM7RORr+tY3v/nNvIMOOiivugciIrIzWrRoEZ9++ukOf27O\nTo5FJLeY2VPAsSGESv9jzswC8HQI4bjtNa7NWHbQQQflLVy4sBoeLSKy82vbti3PP//8sh393Jyd\nHOfneSY4v1WTrNavZnczmVoAWnqstNHmei0nO5zYvRJt5dy/Zp/ky0yGmbXvxC+yMsIxG5y/JpNd\nTmMli70tv8y12fJbxf8eXdP7CuN9vFXO0EVERERqoZydHIuIAAcBn1TXw19ZsY6CK/9WXY8XEalW\ny4afXN1D2CaaHItIzgohvF7dYxARkZ1Lzk6OS+MCudLs0oS4LCa/kZcYFOalJRclmfvKK6tIFtLF\nsojNXpPKb/TVMorScsoqStZs0kT+2kw5RtaCwbh4LlM6kd8ofV5hXiyryJReZJVckOk/b1X8Ir2v\nebmlICI7npmdCvQHDsb/T10DLAamhxDGlbn2G8CvgAuA5sBq4D7gmhDCZ2Wu3aTm2MyGANcBxwMt\ngEuBA4EPgb8Cvw4hvIOIiNRK2udYRKqVmf0ceBCfGD8M3AbMAr6JT4DLug/oBzwL/Ab4FJ8s372V\njx4AjAdeAu4A/h2fV2Jm397qNyIiIjkhZzPH+TG7W5rdGDPGpeVtu7a5hXiNKrEQr5z7Szdtqtxz\nMxnutWlmNz/vq7Hsbd4y7yc/s3Xc4qzxJlnlTF9p5ri4MuMT2f4uAj4DDg0hrM4OmNne5Vy/P9A6\nhLA2XnM1PsE918yu2oqsb1egQwjhhaznjcIzycOBn1amEzOraDuKAys5DhERqUGUORaRmuBzYGPZ\nxhDCu+Vce0VmYhyv+RiYgv88a7cVz5yUPTGOhgDrgLPNrN5W9CUiIjkiZzPHxHri/KxDLygvY5zI\n1CH7d5XK+pJ9WMhXDxiBr263VuH9eZvW/SZ1xVlHB5SWec2WZJHXZO7Lrn+O7zmOpXTxx5vGRKrX\nFLyU4lUzmw48DRSHEP5XwfULymnL/K/RcCue+3TZhhDCOjN7ETgW3+nixS11EkJoW157zCgfsRXj\nERGRGkCZYxGpViGE24HzgOVAETATWGVmT5rZJpngEML75XTzeXzdZSsevaqC9kxZRoOt6EtERHKE\nJsciUu1CCBNDCB3xKvyTgd8DxwCPmlnj7fTYJhW0Z07mWbednisiIjVYzpZVNO/gf++V8HjSlt8h\nU0aQKWVI/25MyyO87KAwa5uzzO9rM1uzlealCafCxd5H87iwrnDNqk3uS0o8shb2lSZbv2WXOcRn\n5sUSiKzyiLIlGtnbwpW2yryHuDBvcXpdZos61pR972kJiUhNEbPCs4BZZlYH+AlwNPDAdnjcscDE\n7AYzawAcBqwHFn3dB7Rp1oCFO+km+CIitZUyxyJSrczspLh3cVmZjPH2OuHuHDM7vEzbELycYmoI\nYcN2eq6IiNRgOZs5/uulhQAMLuqUtP24lWdfR8bvv3JYRsu45dkSz/zm56Wxkkzmt1UmM5tmnMfM\nG+SvfboDMK3D/kmsdLH3Wdgqk7XNzhzH17WbLsgrjm3Ts8b3lcNM2PxCvsIO+yVthYtjJnttpp99\n0uuVOZaaYRqw3sz+ASwDDM8WHwksBP6+nZ47Gyg2sxnASuCo+GcZcOV2eqaIiNRwyhyLSHW7EngO\n39nhYvwgjrrAFcDxIYRNtnirIqPi8w4jPSVvAlBYdr9lERGpPXI2czzuzhMAeP3EtAZ4YfCM74K4\npVv2flAlcYH6+Fm/BKClpRnnVUP89dxYVzyzQ3rf1HvvA+Dvex0FwJg+hUksf65X/NaLh48sz8r+\nlsa64PK2ZhsRY4OyjrcuXrvp8dSJzBZuMTPdckznJDQozzPaSzgHgJ6Nlqbji7XKfFBx1yLbWwhh\nPH5S3ZauO24zsQn4xLZsu22hz3LvExGR2kuZYxERERGRSJNjEREREZEoZ8sqOset0s6eckjSdtMk\nL2+ox8sAFH/lDi95WEdTAO7uky5qo6OXZpy02O97skP3JHTwhv8A8P2WfwFgddf0vnqTvVRjVdzC\n7S9r0tKI4laZEou0dKIkjitTmlE666W0r7XxuriwbkTW+QXT4zZtA+M1JzV8Lon1nO19rms0AoBz\n572cxAqzSixERERERJljEallQghDQggWQniqusciIiI1T85mjifN9QzpZ2PeSdoe6eavqzr4Yr0J\nbY9KYoPe7QHAuSecCUDrOy5OYvfmXe5fFMXzAvqfk8Sa/96X9c0s6gPAAef9KYkdHw8dyWwBt+vi\nNHPcMtlaLd3e7e7Fnsm9er/zAPj+1PQAk6K4VVzJPM8Yr+uQZpyvb+VtRWMeA2Dh5HvS93X9tQDM\nv+tWf+6AEUlsyvWZLPL9iIiIiIgyxyIiIiIiiZzNHF/b2+uLS+al26cNi8cx9+zgNb0LHs46WvoL\nPyhrUJFnZLu/e18SW/dHPyirsHg+ANd3+2ESmzb3Xb+vsx/+0aVb+rx6az2jWy8ettGyVRpr2SEe\nOpK1LdzV7X03q2+39T6vn5xmqGf18drp0j6+Pdz7Yw9NYn3u8/f6iPUFoN3QdOxjWr0JwJdT/PCP\nUe+lGedBsd55ShEiIiIigjLHIiIiIiIJTY5FRERERKKcLau4ZbaXDIzpmrUILpY3jMnz8obCx9ok\nsdVzZwMw7TlfIFd0aroFXOGkW/yadrsAkP/vDel97b4FQM82ft+THXZP78ucXBcX5K1emzXANX7d\nmD7pc056wxcPDm4+CIDzlo9JYl3MS0Aun+8L/85ampZojGnp7/GJVl6WcWGrkUns8DUz/bWwNQDt\nJqVlFdMXx8WAKqsQERERAZQ5FhERERFJ5Gzm+NdTfdHd09eWJG0jYqZ0wVD//viiE5LYeQ0PA2D5\nc/46/YMXkti/mkwAIP/JXwDQ8qc3JbFh940G4O4n/PuWswuTWOlYX5DXupVnaw/OyhyXxkNH9rT9\nk7bWa3zx3LRD4+EcfdKDPq4+xw8XWY1ngg/ummaAT5rtGePBb9wGQNHYlmnsgO/7uIb+GIDOswcm\nsTbz0vcvIiIiIsoci4iIiIgkcjZznJ/nWeJlHX+TtN13gLc9Yn4s86fd0qzy+2fNA+Avv5wKwPhn\n08Olx/zTD9L4VivfWu3gYWld8Qs/86/fH++Z3J6XzU5i04b681q39NdpWZnj1vHrC7NqomfGGuWR\nP/uXvwebmsR6FpzmX4z6FIDSk/+exOqN9We/0cEzzg8MTbPDq4d6JrvefM9CHz05K1Pd0Z/99smI\niIiICMoci0gtZGYFZhbMbEJ1j0VERGoWTY5FZLvQBFRERHZGOVtWcX5D387skf33SdoeOTJuXdbQ\nyxDOu+TEJDZiVT0ANoz2k+cGtXgoidW79k4A1j/tJQpvLE1LIY4+qQUAd4/uAsCsPulCueVTfJu2\nWbHEY3n2grz49beWpGUOLd94DoDfdvUT+Ha9Pi3tWH29b+VWNMlLQhr3nJvE8nv71m+XvO9bwZXM\nTU/Waxa3tFt36Qgfy3V9k9jdY5YiItvPKyvWUXDl37Z43bLhqm0SEakplDkWEREREYlyNnO8sJFn\nRf/9+xuTtttGXQHAoDaeJd7zkIZJbObDvh0aEz1Du2DxO0nsevsPALfs828Adv9Dul1b4+Z+uMb7\nfWYA8K8/pZngEUM9y8sazxyXkmacS9f6Arn8DvslbW1P9D3mBv32KQDuO+PXSeykfd4HYO+3fMHf\n1MwBHsC5UzxzTEd/z9OGpAsNF6z1TPH3/+3/Drr2uouT2KxHHgDg5P6IVCkzGwJcF789z8zOywpf\nACwDngSGArPitZ2AhsB3QwjLzCwAT4cQjiun/wnAeZlry8TaA5cDRwF7A2uBfwG/CyHM2MK46wB3\nAP2AmcDZIYT1lXzbIiKSA3J2ciwi1eopYC+gP/AS8Jes2IsxBj4hvgr4B/AHfDL72bY+1MwuBH4D\nfAE8BCwGGgPtgIuBCifHZlYfmAycAdwFFIUQvqzEMxdWEDpwqwYvIiI1Qs5Ojv/e7TEAPr78laTt\nx/v5IRuTB38CwMxWTZPYosmdATjrb551nfVeugXcgbGO+JYx/QD44JBDk9jei722ecSQyQA8MfDO\nJDZzcqw/LvJrWrZKs8SZreY6j01roht386zujO+f4tf/6/dJbM8lnoQ7r45ntNuOSbdyG7Q4jrlV\nJ/9+cprZLi7yDPU3z7obgOlvTEpiLcd5lvsuRKpWCOEpM1uGT45fDCEMyY6b2XHxyxOAviGEu7/u\nM83sYGAc8AFwdAjh1TLxfTdzbx7wINAZuDKEcMvXHY+IiOyccnZyLCI7hRerYmIc/QL/mXZD2Ykx\nQAjh7fJuMrMWwCPA/sA5IYQpW/PQEELbCvpdCByxNX2JiEj10+RYRKrT/Crsq2N8nb3Zq77qe8Bz\nwO5A1xDCE1U4HhER2Qnl7OR4dTcvGTio7ulJ2//uehSAdrcfAMD+S9Lyg5IO/tpzsS9qe7JVel+3\nN7w8ouUsL70oGjMzieXleVnE3dN8zc7KveslsUO73wzAuR194V/Lny9KYsc3+rOPpSD9u7joNi95\neHD3jwHo1LBlen2/Pv5FQz89r93q55LYzI4BgHqtfAu4wm7pdnKDFvt2chuvP9OfMfaxJLbr2kyJ\nxa2IVJN3tnxJpWXqmFdsxT0HAHl4HfTzVTgWERHZSWkrNxGpTmELsYr+Ab9XOW3vx9dmW/H8h4Ff\nA4cBT5jZ3ltxr4iI5KCczRwvyPPX48d0T9qu2rMXAGctbQxAgzXp9SO7+gK5Yl4GoN289ICMEaP9\ndfBYfx00e2ASK+nj2eeP9vNM8End0u3a/oIvhiu9zjO0hVMuSGIt53tJ5ILD0oX5x//spwCs7edb\nsw0/9/4kNvMiT7CN630CANNuGZnEZh2wOwBj/uIni0w/Ps1Qr9z9dwDUaTEEgN/3PTyJ3fwn8y+6\nIrI9fBFfd9nG+98D8ss2mtku+GS2rLn4rhRdgdcr+5AQws1m9ikwCnjSzH4QQli1bUP+qjbNGrBQ\nB3yIiOxUlDkWke3lPTz723wb758PNDezE8q0DwZalHP9b4DPgWvizhVfsbndKkIId+AL+loDT5vZ\nd7ZxzCIispPL2cyxiFSvEMJHZjYPONrMpgBvkO4/XBkjgROBB81sOn6YRyHwXXwf5ePKPO81M7sY\nGA+8YGYP4vscN8Izyh8Cx29mvOPNbD3we+AZM+sSQlheybGKiEiOsBA2V/K386rzXp0AsLxRut6n\nyd+8jGD+CxcCcM8J56Y3tPQFfN1b+bdLGqWhJXP9N6wlreJCtw5pImtah8x9vsdw97x03+IlU/yE\nvGEn+cl3pRySxPKHngNAu67p4rmSxV6SsbyR99G85UtJrBAv++g5z0soaPVx2lcc14KW8dS8Vul7\nfpIhAHSL93/r0cZJbPCV/hvrgvefN0S2AzNriZcrFOKn3xllTsgruwdymftPBa4F2gAfA48DV+An\n61V0Ql4nYCBwNF6b/C7wMn5C3v3xmgLgP8C9IYTzy9zfC5iIL+zrEkJ4cxvf+8IjjjjiiIULKzoj\nRERENqdt27Y8//zzz1e0Zeb2osyxiGw3IYQlwI8qCG/xH2UhhIcoP9N8fvxT3j3P4afcba7fZRU9\nP4QwFZi6pbGJiEhuyt3J8Vrfsmz6rJKk6ZHXRgHw3DXTAej8+eNJ7Ky4eO4gP+iOE/qkXZX28azr\nmX1itjYv3Q5tbiOPvZDnGeQxS9L7uuMZ3Z5j/Tkj46l4ACVrvI/mvdPrS4Z4vHHMMN/YIb1+r7F9\nATh7rC8UXLI2zRwPi9vQ3TiPr9wPcH0jv/7a9r413eN1023bHlhwE+CFoSIiIiKiBXkiIiIiIomc\nzRyfM9Ln/adOviFpG77St0Z9/JjYUDQiiU1b7JnjtXhGd2H7NKucH7yGt/gNv6ZzqzSje26eZ2lv\ni1vHDWq/fxKrd7b3dUArb1uYdd/ARl473DkvzQDvGjPNsyaPB+Dgyen72Xu+1zn/32TfMm5DVoa6\nqJ/3f+x1Pr7vZ21D1yZ+/Ujcqm5CvXFJ7InXbvMv0rNGRERERGo1ZY5FRERERCJNjkVEREREopwt\nq3j8Yd99qU7PQ5O2IY3OB+DAa1YCcFb/xUmsZLGfjDet6xAAdr9vfFZvXpJwTJ6XQjQfm5YtHFvk\n5Q7ndvJyh0WNdkti5y37DQB3X+4n6m0sSs8luL2PbydX+mG6kP/m8fUAGPbGAB/TvHSbt3GFfwRg\nQ9drAGiSl47uqv9422vX++q+/wvpe7536NL43r8NwBF3pKfjPnzdr+JXpyIiIiIiyhyLiIiIiCRy\nNnN81lrPBDMv3dZsr0dvAaD7X48CoGjvD5NY36UzAfj+e75grds/v5vE/jL2Or+vyDPNJfPSjHOD\nlo/49QsuA+DGy/smsWuLhgNwVKcJAOQ/mmac6eiL51iwe9I0Mm471+ZvnvX+vNuQJLZxgWd8p+f5\n6xWZ9wecfZ33f+O3nwNgTvvOSWzJSb6F28yTfVzdG9+fxG4MIxERERGRlDLHIiIiIiJRzmaOR0yO\nWdoOq5K25nlew1vU7wcAXHhA1gkcdfy6DXvHmuE6DyShMSd75vhfp8TYmo+S2CuT7wVg4BSvL84c\n7gEwrZ9nZpvEwz/ajRmSxErWxBrnuWkGeES/SQDs9bpngI9/Iz2KurSfH0HNYu+/eF66/1rnMV73\nvCpuI7ew6BfpfWv8COq73/O9304akz6v8XUxw7wBEREREUGZYxERERGRhCbHIiIiIiJRzpZVlMyK\n5Q2z0rblHXyLs5Fxa7bCPucksenzvNxgxPyJHluanmY3ZqyfpLcgbttWMmtm2mkrfymMa/RGtEpL\nGorm+QK74q5+H7PTrdlS6SK9ktmDALgozxfktRuaLpjbMMXHteFLL4UYcV26XVvztft526zDAJjY\nOy3tyI+n7hXN87KPJekBfjz5nAHwk3JGJSIiIlIbKXMsIl9hZk+ZWdgBzykws2BmE7b3s0RERCor\nZzPH+TETXLI2zcyOHONZ1GktPaPL7DSWv8a/bjtmsn/fId0Cbvp1/nVhV7+vsFFWn/38+rPGeEq2\ntEO64I3ZnsEdgWeAS7OyxMRDPIreOCFpanuSX1d07W0ArH7kz+n1i33B4LTZH3tfeWkWujS+j/y1\nfnDJmKI0I77hpCEAzOzm4+ucl76vJb19ASD/6IeIiIiI5PDkWES22bnAblu8SkREJAfl7OR4Q8tz\nASju2idpKzrJM6X3DvFt0Trvnx6WMfG5WEccM6353Qal9/WOtcZjvPY4v1WatW3Xz7PDpa38vrOG\nppngksXdAZg2K9YzT5mUxDpndmLLyvKO6efHTJde+xcAFjzyUhI7vlHmKz80pGhMWvej/tPEAAAg\nAElEQVRcPMYz2qXzvN65++j06OuiMV47vaS9v44Y+5skVjov04cyx5IKISyv7jHkildWrKPgyr8B\nsGz4ydU8GhERqQzVHIvUAmZ2vpk9YGZvmtmnZvaBmRWbWZ9yrt2k5tjMjov1wUPMrL2Z/c3M1sa2\ngnjNsvingZndaWYrzGy9mb1mZkVmZpUc6wFmNtzMFpjZ/8xsg5m9ZWb3mNm+5VyfPbbD4tjeN7NP\nzOxpMyus4DnfMLOLzWxu/O/xiZm9YGaXmJl+NoqI1FL6C0CkdvgNUAA8A9wBTANaAJPM7Iat6KcT\n8CxQH/gDcC/wWVZ8V+DvwInxGb8F9gJGA3dW8hmnA32BUmAqMBZ4DfgZ8E8za1bBfe2Akji23wF/\nBY4CnjCz72VfaGZ1Y/yuOL77gHvwn4lj4/sSEZFaKGfLKpbs7+UNS1qmJ8lNn+JbnY04e4g3rE2v\nH7HYSx/y8dPvSmenW7nlt/fSicbt/b5S0lP3RjQ8HYDucXFbO9Kt3Ijbpk3r7QvmlvQ+PQldNCsu\n7mu5Omnr+zffwq1oiC/M697n4iS2YKyPp2SMl4uUrilJYkWz/NkjJ/t7aDslLblY3jKe/Le/v/cl\n16Z91pvyqH+Rvh3JXW1CCEuzG8xsV2A2cKWZjQ8hrKhEPycAfUMId1cQbwq8GZ+3IT7nOuCfwMVm\nNj2E8MwWnjEJGJW5P2u8J8TxDobs/9ESJwMXhBAmZN1zETAe6A9cnHXt1fgE/k7g0hDCF/H6XfBJ\n8k/M7P4QwoNbGCtmtrCC0IFbuldERGoeZY5FaoGyE+PY9hmeOf0G8P8q2dWLm5kYZ1yVPbENIawF\nMtnpCyox1hVlJ8ax/THgVXxSW57i7Ilx9Afgc6B9piGWTFwCvAMMyEyM4zO+AC4HAtAbERGpdXI2\nc/zbI48C4IL90nLDu9/1MsrujeLitCVZ2dchvoitNGZ0S6b0TWJjHvFFbGOWxPnFmnSeURj7at5v\nCABt2TOJlfIOABta7gPA9JOGJrFpDeNBHa3SMXfv7du0tZ3smeARWdWgg7r5oR97XeLPmdOnexIr\nmf1c/Cpu99YhXTDYs4Nn0Evj+HrGVwBml1uKKTnIzJoDV+CT4ObAN8tcUlGpQlnztxD/HC9tKOup\n+Hr4lh4Qa5N7A+cDhwINgV2yLvmsnNsAFpRtCCFsNLNVsY+MA4BGwGJgcAWl0J8CB21prPEZbctr\njxnlIyrTh4iI1Bw5OzkWEWdm++GT2oZ4vfBjwDrgC7wO+TygXiW7e2cL8XezM7Hl3NegEs+4HbgU\nWAk8CqzAJ6vgE+YWFdz3fgXtn/PVyXVm75dWwHWbGccem4mJiEiOytnJccsiz/bOJD1II3+sfz1t\ncjxSeXFamzttiGdrSyZ7W+bAD4AlRZ5F3tDQM62Z+l2Akb29jHFEzMju1TvdRm3iFP97N7+VFx8/\neVKaVZ7e0bO8Z5Ge59yzm/ffs4NnkPOzioGnzfNM87Szva64qGE69onXZr72+zZcl5ZjfrPE38eH\nkx7399LqX0mssKVvC8fqJ5Ccdhk+IbygbNmBmfXCJ8eVtaWT8/Y2s13KmSDvE1/Xbe5mM2sMFAGv\nAIUhhA/LGe/XlRnDzBDC6Zu9UkREah3VHIvkvsyq1AfKiR1bxc/6BlBevc5x8fWFLdy/H/5z6bFy\nJsb7xvjX9TqeZe4Yd60QERFJaHIskvuWxdfjshvN7ER8e7SqdrOZJWUaZpaH7zAB8Mct3Lssvh4V\nd47I9LEHvi3c1/5tVwjhc3y7tqbAGDMrW3+NmTU1s4O/7rPaNGvAsuEn6wAQEZGdSM6WVeRzCPDV\nbdc29PNt0JYMidu1dX0siY2Ii+CKrvOyiCXts+5r74vh2nX1RfIb1qZllxfN8+3XlvT2Uob8vJFJ\nrGSWlzIM7OZ9DeyYljAWzfLnTDw7PTWvbXv/zXPbhl5+MXJe1rqmNV4CcdZkbxvU9ZB07Gvi8+b5\niXrFHdNT+vJn+3g2nOSnATZutE8SW5Lnr5XdpkB2WuPwXSL+ZGYP4DW8bYCTgBnAWVX4rJV4/fIr\nZvYQUBc4E5+IjtvSNm4hhHfMbBrQE3jRzB7D65R/CKwHXgQO20wXlXUDvtivL/AjM5uD/3dpjNci\nd8a3e3utCp4lIiI7kZydHIuICyG8bGbHA8OAbvj/9y/hh228T9VOjj8DfgDchE9w98b3PR6OZ2sr\n46fxnrOAXwL/Ax4CrqX80pCtFnex+DHQB1/kdwq+AO9/wH+Aa4ApX/MxBYsWLaJt23I3sxARkS1Y\ntGgR+MLxHcpC2NL6GhGRLTOzZQAhhILqHUnNYGYb8F0yXqrusYiUkTmg5vVqHYVI+bI/nwXAByGE\n7+7IAShzLCKyfbwCFe+DLFJdMqc66rMpNVFN+HxqQZ6IiIiISKTJsYiIiIhIpLIKEakSqjUWEZFc\noMyxiIiIiEikybGIiIiISKSt3EREREREImWORUREREQiTY5FRERERCJNjkVEREREIk2ORUREREQi\nTY5FRERERCJNjkVEREREIk2ORUREREQiTY5FRERERCJNjkVEKsHM9jWzP5jZf81sg5ktM7M7zKzh\nVvaTF+9bFvv5b+x33+01dsl9VfH5NLOnzCxs5k/97fkeJDeZ2ZlmNtbMnjWzD+JnafI29lUlP4e3\n5BtV2ZmISC4ys/2BEqAx8CDwOtAe6A+cZGadQwhrKtFPo9jPAcAcYBpwIHABcLKZdQohvLl93oXk\nqqr6fGYZWkH7519roFJbDQYOBT4C3sZ/5m217fA5r5AmxyIiWzYO/4FcFEIYm2k0s9uBAcCNQN9K\n9HMTPjEeFUK4LKufImB0fM5JVThuqR2q6vMJQAhhSFUPUGq1AfikeAlwLPDkNvZTpZ/zzbEQQlX0\nIyKSk8xsP2ApsAzYP4TwZVZsT2AlYEDjEMLHm+lnd+B/wJdA0xDCh1mxOvEZ/7+9e4+zuir3OP55\n1EEFdQDBI6PBhHjBLE3MxCtG3irNl5l2sePl2CtTK9NK61jisWP3zNKyMrXME1pWagZa3lOpxIQU\nvACCyWDOiDMog4Cyzh/rWfzWbPZcmNkzA3u+79fL12J+67fXb+1hOzw8PGuten+GssfSJZX6fPr9\n9wKHhBCs1yYsA5qZTSIGxzeEEE5aj9dV7HPeFao5FhHp2Lu8vTP/gQzgAe6DwGBgv07GmQhsCTyY\nB8Y+zhrgTv/y0B7PWAaSSn0+1zKzE83sAjM718yOMrPNKzddkW6p+Oe8IwqORUQ6tqu3T7fT/4y3\nu/TROCK53vhcTQW+BnwH+CPwnJkd373piVREn/78VHAsItKxWm9b2ulP14f20TgiuUp+rm4BjgZ2\nJP4rx27EIHkocKOZHdWDeYr0RJ/+/NSCPBGRnkn1mT1dwFGpcURyXf5chRAuK7n0FPAlM2sAfkBc\nUDqtstMTqYiK/vxU5lhEpGMpI1HbTv82Jff19jgiub74XF1N3MZtL1/8JNLX+vTnp4JjEZGOPeVt\ne7VsO3vbXi1cpccRyfX65yqE8BqQFpEO6e44Ij3Qpz8/FRyLiHQs7cl5uG+5tpZn0Q4AVgAzOhln\nht93QGn2zcc9vOR5Il1Rqc9nu8xsV2AYMUBu6u44Ij3Q65/znIJjEZEOhBDmE7dZqwfOKum+mJhJ\n+0W+t6aZ7WZmbU6BCiG8Clzv908pGedsH/8O7XEs66NSn08zG2tmO5SOb2YjgGv9y6khBJ2SJ73G\nzGr887lTfr07n/MezUOHgIiIdKzMsaVzgXcS9yR+Gtg/P7bUzAJA6WEKZY6P/hswHng/8KKPM7+3\n349Ul0p8Ps3sFGJt8X3EwxaWAqOB9xDrPB8BDgshNPf+O5JqYmbHAsf6l9sDRwALgAf8WlMI4XN+\nbz3wLLAohFBfMs56fc57NGcFxyIinTOzNwH/QzzeeVviiUy/By4OISwtubdscOx9w4GLiH9YjAJe\nIu4A8JUQwvO9+R6kevX082lmbwXOAyYAdcQFTq8ATwA3AT8OIazq/Xci1cbMphB/5rVnbSDcUXDs\n/V3+nPdozgqORUREREQi1RyLiIiIiDgFxyIiIiIiTsFxO8xsoZkFM5u0nq+b4q+7rndmBmY2yZ+x\nsLeeISIiIjIQKTgWEREREXEKjiuviXiSy5L+noiIiIiIrJ/N+nsC1SaEcAVwRX/PQ0RERETWnzLH\nIiIiIiJOwXEXmNloM7vazP5lZq+Z2bNm9m0zqy1zb7sL8vx6MLN6MxtvZj/3MVeb2e9L7q31Zzzr\nz/yXmf3UzHbsxbcqIiIiMqApOO7cOOKxmf8FDAUC8Wzv84BHzGxUN8Y8yMf8T+KxnG3OqvcxH/Fn\n1PszhwKnA48Cbc4cFxEREZHKUHDcuW8DLcBBIYStgSHEY1+biIHzz7sx5g+BvwNvDSFsAwwmBsLJ\nz33sJuD9wBB/9sHAMuA73XsrIiIiItIRBced2xw4KoTwF4AQwpoQwi3ACd5/mJkduJ5jvuhjPu5j\nhhDCfAAzOwg4zO87IYRwawhhjd/3APE88S169I5EREREpCwFx527KYQwr/RiCOEe4CH/8vj1HPOK\nEMKKdvrSWDP8GaXPnQfcuJ7PExEREZEuUHDcuXs76LvP273Xc8yHO+hLY93XwT0d9YmIiIhINyk4\n7tziLvSNXM8xGzvoS2M1dOG5IiIiIlJBCo57xrr5ujf66bkiIiIi0gEFx52r66AvbePWUSZ4faWx\nuvJcEREREakgBcedO6QLfY9W8HlprIO78FwRERERqSAFx5070czGll40s4OBA/zLX1fweWmsif6M\n0ueOBU6s4PNERERExCk47twqYJqZ7Q9gZpuY2dHAb7z/TyGEByv1MN9P+U/+5W/M7H1mtok/+wBg\nOrCyUs8TERERkYKC4859DhgGPGhmrwCvArcSd5WYB5zcC8882cceCdwGvOrP/gvxGOnzOnitiIiI\niHSTguPOzQP2Aa4hHiO9KbCQeITzPiGEJZV+oI/5DuC7wCJ/ZgvwM+I+yPMr/UwRERERAQsh9Pcc\nREREREQ2CMoci4iIiIg4BcciIiIiIk7BsYiIiIiIU3AsIiIiIuIUHIuIiIiIOAXHIiIiIiJOwbGI\niIiIiFNwLCIiIiLiFByLiIiIiDgFxyIiIiIibrP+noCISDUys2eBbYCF/TwVEZGNVT2wLITw5r58\naNUGx3b0LgGA1iw5/r6xsX35HbFdvnfRd8qbYrtkUGwbNy/6Vm8a21qL7dzB6z5wtbVtAWrXxHZQ\n8DGzb3dD+nV2f92q2B5yX2y3f3vRd+22sR3l9xyztOibuo2POajt8wDetjq2i2piO7m56DupCYCw\neGI2CRGpkG223HLL4ePHjx/e3xMREdkYzZ07lxUrVvT5c6s2OGbmlrEds7K49tLs2N71bGzDM0Xf\n0UfEdu7+sa3NAsxLF8f2G3WxHfJG0bfcA+e1QXEeZ/oYNSVt/uumTYtrn305ti943wNbr3t/evai\nQUVfq48xZE3JXDLjX4vtiNeLay3V+9svfc/M6oFngZ+HEE7p18lsGBaOHz9++MyZM/t7HiIiG6UJ\nEybw6KOPLuzr56rmWERERETEKXUoItJLHl/cQv0Ft/f3NEQGnIVff29/T0E2YtUbHO82PrbfWlxc\nu8trjr/ZGttTZxV9WzwX29v9/s8cU/SdMCa2rV6/m2qJgbVlFGvLJLJv6WpPzNd4KcPg7HVD/P6W\nbKjzX4ht83/E9qCa0ncFzT7+9G2Kaw1eVjHy9XXnN3eL2E5eFtt9WtcdS0REREQAlVWISC8ws3oz\nm2pmTWb2mpk9YmbvK3Pf5mZ2gZnNNrNWM1tmZg+Y2QntjBnM7Doz28XMbjSzF81sjZlN8nvGmtlP\nzGyema0ws6Vm9k8zu8rMti0z5ofN7B4ze9nnOdfMLjSzzUvvFRGRgaF6U4fv2im2L29fXLvKM8A7\nLYntv99U9G2yVWwf8d0czp1W9P350NheMSq2R75W9M3zzGyNZ5BbskV3TSXf3nxB3mBfWDc+WyC3\n80uxHb17bG/M/u6Sss5pAV9zmUV3q/zayGzMtEtFg7fN2ZgtPtaW6w4l0gNjgL8BC4DrgeHAicAt\nZvbuEMI9AGY2CLgDOAR4ErgSGAwcD9xoZnuFEL5UZvydgL8CTwM3ED/By8xsFPB34vZpfwRuBrYA\n3gx8DLgCeCkNYmY/A04Dngd+CzQD+wGXAJPN7LAQQvY/U3lm1t6Ku906e62IiGx4qjc4FpH+MgmY\nEkK4OF0ws/8DpgOfB+7xy+cRA+NpwDEpEDWzi4nB9RfN7A8hhIdKxj8Q+Fpp4GxmnyIG4ueEEC4v\n6RsCrMm+PoUYGP8O+GgIYUXWNwW4CDgLaDOOiIhUv+oNjr9/VGz3zf4ldZrXDI/2ZNBX8z7PqB7o\nmeDfzS/6/tvrkE/y7dOaslTrOM8iPzwktvk2ast9zLTdW12WhKr1zPH3Xiyu3e5/dt/iWeyGLMtb\n623ayq01e46/LYZ63+Bsq7kR3rm23llbGkuvWwR8Nb8QQrjDzJ4D9s0un0bc7/DcPEMbQnjRzC4B\nrgZOB0qD438DF9O+dTbFDCEsL7n0GeB14LQ8MHaXAGcDH6ULwXEIYUK5655R3rtcn4iIbLiqNzgW\nkf7yWAjhjTLX/wVMBDCzrYFxwOIQwpNl7r3b27eX6ZsVQlhZ5vqtwKXAlWZ2BLFk40FgTghhbU2T\nmQ0G9gSagHPMyv6FcSUwvlyHiIhUNwXHIlJpze1cf51iEXD6t5Al7dybrg8t0/dCuReEEBaZ2b7A\nFOBI4Djv+peZfTuE8H3/ehjxn1BGEssnRERE1qre4HinvWJ7TnYCXVoQN96TTn/ISidu8z+r0wK7\nw3ct+g72k/GO8e3epmflDo2+0K3Fs0/5cdVrSyxCydfARC9zuPqfxbVrfIHgM1ut+35qSlqyxX3p\nhLz0/pqz95zKL0Z6ecXQbJu31nxLOpE+lTYx3L6d/lEl9+VCmWuxI4S5wIlmthkxO/xu4FPA5Wa2\nPITws2zMf4QQVPYgIiJtVG9wLCIbrBDCK2Y2HxhrZjuHkJ/lDoBvEcOj3Rz/dWAmMNPMHgLuB44F\nfhZCeNXMngDeYmbDQwhLu/k2OrXHDrXM1GEEIiIbleoNjsd5dnh2tnguLVxLi9bPqiv6vun3pa3O\nGrIDONJhHtP9X3iv/FfRd5GP8Vv/elU2hxp/TjrwY/Wgou9iT1599qXi2gEl2e6hHWR222wL5/eN\nLLPrVIv/Fqe3syibQ7OXha6z+6tIn7gG+F/gW2b2gVSnbGYjgC9n93SJl1QsCiH8u6TLT9UhOwGH\n7wI/A64xs1NCCG1KQcxsGPDmEEK3gnMREdl4VW9wLCIbum8DRwHvB2aZ2R+J+xx/ENgO+GYI4S/r\nMd5HgLPM7D5gHvAycU/ko4kL7L6XbgwhXGNmE4AzgflmdgfwHHEruDcDBwPXAmf06B2KiMhGR8Gx\niPSLEMIqMzsMOJcY2H6KuGhvFnGv4l+t55C/AjYH9iduobYlsBiYCnwnhPB4yfPPMrNpxAD43cTF\nf0uJQfK3gF92862JiMhGrHqD47QwrjbbUSotTmv09tJsj+F9HoztkV7usHu2g9Tot8b25mGxvW1Z\n0VfTENsJ28T2/tqiL5VjLPdv8zHZyXo/XRDbJ0cW1z7orx3k82vOfnvSfsqpdGL2FkVfWp+UTsgb\nkpVjpD2Pa8vsgZxO21NZhVRACGEhHWykHUKYVObaa8Tt1y6twPh/JZ6c12UhhD8Af1if14iISHXb\npPNbREREREQGhurPHI/JVsg1+bWRM2K7+BtF31W+pVqrf0suyhbrfcS3S93nw7H90qii71uetZ33\nbGyPyLaAe3irtmOOyw7p+oRnjjfbqbi20BcF1nrmN8/yTnw1tmlhXWO2XVvpzlarN1n312kBX55V\nnpctzhMRERERZY5FRERERJLqzRynrGtTFv/fPS+2K/1QrEefKPq2mRjbz3kmd0K269Oy2bG9zjOt\nHzyu6Pvo6NjetCi2D2cHfp1a77/wLO/9Q4q+z28d20fqi2tzfPyU5R2XnZA7YYW/B69tHpJli0d5\ndjwlgpvLHHwyaE3brwEWldn6TURERGQAU+ZYRERERMQpOBYRERERcdVbVrHaywdGZ6UDl98d2/N9\ncdvYg4u+h98d25P3ju1LjxV9ly2M7WTfJvXT2TZqp3uJxQljY/uFpqLvAV+A1+jf5lTWAfCcb9v2\n0IjiWlosN8bLKfLFhHMHx7bBaycGZ+URE73kIp3q17B50Ze2r0sn/zVnfx/KKkBERERERJljERER\nEZG1qjhz7HH/IdmhWHsNj+3BH49tc0vRt4cvxHuXb+FWv7Dom+2L56Z7FvZt/yj6btgntnu/JbbX\nDi/6RviiwKGrY7vs0aJvyuGxfSjLAE/wTPNgzyDPGFz0jfIMeI2PWVN0MdkPJUnv+RvZFm1LUjbZ\n25HZb3m6lu0+JyIiIjKQKXMsIiIiIuKqN3N8ZWNsFzQU187aP7a1zbGd+GzRd5vX7e7sp89e/HzR\nl7ZUW+1bpNVmJ9gun+W/8Mzx9K2zOfgYC66P7S+zFO0n/SCRfbIt48b7EdEzPWM8K6ttrk0HiHim\nuTWbwyN+eEidZ6jvz46pTlu4DfWMc0P2W96qvxuJiIiI5BQdiYiIiIg4BcciIiIiIq56yyp+92Js\nDxlaXKtJq9i8DGFWtnhutG/BNsVLLc5eXPR9y0sSGv31ddnfKWY8Gdt7/fUN2el0u3w/tiO93OH4\nzxR9Q3zMcdl2bbN9Xmnrt7qsLy3ES6UQ6QRAgN/6tnAjfdHe8mx+8zZve//gbH6sQUREREQKyhyL\nyIBnZveaWej8ThERqXbVmzme7AvW9skOAfnlM7Fd7ovbPvZ00XfkgtjesTS2F67IBvO/Q8xLGedt\nsuf4WB//eWyfybaO2+q52G7hGeSbsyz2B16JbWOWyZ3rWd5az+jWZnNPWeG0/drqbEFeykynvsbs\nt3WuL+pLC/L2y7LRLfq7kYiIiEiueoNjEZF+9vjiFuovuL2/p9FjC7/+3v6egohIn1HqUEQ2Kma2\nr5ndaGaLzWylmS0xszvN7ITsnlPM7GYzW2BmK8xsmZk9aGYnlYxV7+UUh/jXIfvv3r59ZyIisiGo\n3szxCI/7Jy8prn18ZGxneBnC2+YWfRcujO0oLz8YlB9Bl37t365F2UK2Ji+LWOAL85Y8UfQdcHps\nn98rtvkiunSaXVP2W1DnpRO1JYvvcmlb5FGri2urvFSyxUst6lZmY/p9aQ/lQfkcstIMkY2AmX0c\n+BHwBnAr8AywHbAPcCZwk9/6I2AOcD+wBNgWeA9wvZntGkL4st/XDFwMnAKM8V8nC3vxrYiIyAaq\neoNjEakqZrY78ENgGXBQCOGJkv4dsy/3CCHML+kfBEwDLjCzq0IIi0MIzcAUM5sEjAkhTOnGvGa2\n07Xb+o4lIiL9r3qD47N8Idqvsj8vp+4S2yd8Mdw5M4q+hoXe+qK4umyh3Gofqzlti5Zljsd4ZvbW\nYX7hwKLv2YNju5N//aHmoi9tt5YlgPmEbwd3l5+yNz1bwDfXs7yN3uZZ6Bafazr9rjbrO3pZbId6\nVnrW5ohspD5J/Jl1SWlgDBBCeD779fwy/avM7ErgXcBk4Be9OFcREdlIVW9wLCLVZj9vp3V2o5mN\nBs4nBsGjWbu5+Vo7VGpSIYQJ7cxhJrB3pZ4jIiJ9o3qD4+3rYnt9ln1t8LrbVI98ZGvRd49nW6d7\n1rbNARn+bWr1DO3qrBZ4hGdkd/f7P579mbvSDxm5xmuAT1xa9KXs8Lwsk5u2W0v1wbOzbddare0c\n8u3aUm1yqiHOa5VTTXPKUOfbt5WraRbZcKX/mRd3dJOZjQX+BgwDHgDuBFqIdcr1wMmA/glFRETK\nqt7gWESqTapL2gF4soP7ziUuwDs1hHBd3mFmHyYGxyIiImUpdSgiG4u0SOCoTu4b5+3NZfoOaec1\nbwCY2abt9IuIyABRvZnjJV5i+Fy2Jdt4L1M4/4XY/umfRd8IL2l4ZpB/nZ0kO9L/DpEW0eUL8mr8\nvrQILi+TeNV/ncoXbqtd93Ut2Z/FU31R3x/8tL4rshPybvR/UW7YdN3XlZZHjMnKMdJc0/35DnUN\n+RciG7wfAWcAXzazO0IIc/JOM9vRF+Ut9EuTgNuy/iOA09sZ+yVvRwPPVmrCe+xQy0wdoCEislGp\n3uBYRKpKCGGOmZ0JXAX8w8xuIe5zvC1xn+NXgEOJ272dCvzazG4m1ijvARxJ3Af5xDLD3wV8EPit\nmf0RWAEsCiFc37vvSkRENjTVGxyP8axrQ/YWl/uCtQ/9ObZ/mVf0vbh7bAd5RjfPxpYudGvJxhy3\num3fEeOKvpM92zvSs8pztyj6Uqa5OcsAz/ZsdzpYJC3MgyKLnLZwq8my1xPToR8+97dlh4CkypmU\nJW5UJY1svEIIPzWzx4HPETPDxwJNwGzgar9ntpkdCnyVePDHZsAs4Dhi3XK54Phq4iEgHwK+4K+5\nD1BwLCIywFRvcCwiVSmE8DDwgU7ueYi4n3E56xwNGUJ4A/iS/yciIgNY9QbHKbNam9UOr/aM7Ple\nXnhDln2d7lnbmpShzTKsq7wOOe38tjwbM2Vr67zOty6rE07Z61RfXO645pHZ/Slz/HnfDm6/5UXf\nDM8YDy2pcQY4ssXn6X3zBmcP2KTt3JuyTHVeOy0iIiIi2q1CRERERCRRcCwiIiIi4qq3rKLRF6DV\nrVn32m5vie0xi4q+ef73hBpv7x9W9B3nC+OGeAlEXgqxyO9v9tKLsxuLvoley/CIlzmsLlOOkUvl\nFzcPXbcvLSYc5c9ekm3DNsefXZdO1svKKkb6gsGsQmMtnZAnIiIi0oaiIxERERERV72Z47Rt2pjW\n4tpQz7r+6NDY7pC9/a/eF9tr/yO20/Yq+k7yswbu8ExzXbawLh3cMWf/2L78jkIlgcYAAA3nSURB\nVKLvmz6HtD1cTZlscZ6FbvVFfYs8E3zzNkXfniv8F54Jz7dye84PGxnpr2/N5jfLF/mNWbnuHJr1\ndyMRERGRnKIjERERERGn4FhERERExFVvWUUqTWhYVVxLewT/sja2kw8q+m70a/d7KcOO9UXffL/2\nTS/ROHVZ0TfDyxY22zO2R2SvG+fPrilTVpEWww3NyirW3uclE0uy35463594qO9vvLroYnnay9jb\nfA/kVF6S9mFuc/Kf/m4kIiIiklN0JCIiIiLiqjdznBalzd6iuDYunQ7nmdkfb1/0jdk2tkf4dmgz\nsq3S9vOs8AeHx/bspUXfSs8mXzEitrNWFn01Pod0Ml6bE+n8WnN2Yl36dcr85ifqNfhvVco0j3+t\n6Ktd1fb+uiytnJ6ZMun5AsCG7D2KiIiIiDLHIiIiIiJJ9WaO0yEb87K3ONizyEM92zsuq0du9Szq\noiF+T1a3O923axvr7S1ZtvcxHyNlYfOsbZNna1NNcG32d5FU+5sfDJIyvyP92U3Z3FN98Cq/Nj7L\nUA/2bHCL31OXZYfH+Pzmpe3esvc1uMzWciIiIiIDmDLHIiIiIiJOwbGItGFm95pZr/+zgpnVm1kw\ns+t6+1kiIiJdVb1lFWlbtJasBGJJS2yHvhrb/IS4Fi9pGOFlB7VZbFDjZQoNXppQmy2UW+3fwt29\nfKE2W3TX5KUWLankIhtzuY/RlM0hvbbRx2yz1ZrPa/+HY5uXhEzz0/lSqUVTNr+0uC+VWqzOvh95\niYWIiIiIVHFwLCLd9Z/A4P6eRDV4fHEL9RfcXrHxFn79vRUbS0REyqve4LimzL8Kp63VDloR2+m1\nRV/K2qaFdWOyBW9pUVuqQqnJs6+ekR1RJgubsrar/J7G/Nvtz8t3Uyvd8m3ciqKv2d/PkS/FduxV\nRd9rp8X2oYNj25ptX9daMmZNnvVW5ljWFUJ4rr/nICIi0l9UcywyAJjZKWZ2s5ktMLMVZrbMzB40\ns5PK3LtOzbGZTfL64Clmtq+Z3W5mS/1avd+z0P+rNbMrzGyxmb1mZnPM7NNmZqXPameuu5jZ183s\nETNrNLOVZrbIzH5iZjuWuT+f214+t2YzazWz+8xs/3aes5mZnWlmM/z70Wpm/zCzs81MPxtFRAao\n6s0cl3Oi1xzv9Xe/kP2ZOcP/FbnRU7n5McuN/mf6BN8CrmVQ0Ze2Q0uHjeRHN6dsbco8D8nm0uDZ\n5yFZDNJY8ttRk9UvpzFe3Du2Xxxb9P3Vs8gf8Xrk64cVfYP8Pf7Wrw1+eza/LsUqUh1+BMwB7geW\nANsC7wGuN7NdQwhf7uI4E4EvAn8BrgFGAFkBPIOAPwNDgan+9QeAy4FdgbO68IzjgDOAe4CHfPy3\nAKcDR5vZPiGExWVetw/wBeBh4GpgtD/7LjPbK4TwVLrRzGqA24AjgKeA/wNeAw4FfgC8E/hYF+Yq\nIiJVZmAFxyID1x4hhPn5BTMbBEwDLjCzq9oJOEsdDpwRQvhxO/2jgAX+vJX+nIuAvwNnmtmNIYT7\nO3nG9cBl6fXZfA/3+V4IfLLM694LnBpCuC57zSeAq4DPAGdm9/43MTC+AjgnhPCG378p8BPgNDP7\nTQjhlk7mipnNbKdrt85eKyIiGx7906HIAFAaGPu1VcCVxL8kT+7iUI91EBgnX8wD2xDCUuAS//LU\nLsx1cWlg7NfvBJ4gBrXlPJgHxu4a4HVg33TBSybOBl4APpsCY3/GG8B5xFOEPtrZXEVEpPpUb+Y4\nLW7LyxzuWh7bu0fH9qSsrCBtdVaT/r6Qn1yXFuL5tcFZuUNp6URtdjrdIi+1SOURo/KSC79vUfZb\nkO5LVRsjsvkN8b6ZXpux+J1F3zUvAGBvuTHOfHj2ujm/ie3p42N79QVF3wk7IwODmY0GzicGwaOB\nLUtu2aGLQ/2tk/7XiaUQpe719u1l+trw2uSPAqcAewLDgGwVbJsyjtwjpRdCCKvN7N8+RrILsazk\nGeDCdkqhVwDjO5urP2NCueueUd67K2OIiMiGo3qDYxEBwMzGEoPaYcADwJ1AC3Hz7HrgZGDzLg73\nQif9TXkmtszrasv0lfoucA6xNvoOYDExWIUYMI9p53XN7Vx/nbbB9bbe7gxc1ME8turCXEVEpMpU\nb3CcFtTlh3KkTC4HxuayLAE1/pXYTk4HhGTfmnRwRhqzbnXRN89jivEr1n3ePH9eWojXlH+7PX7I\nDylZ5CnjPX1eu2cxxqJ0oIhnr097qegbHO8L13vc0fJq0feJptg2esLvli8WfZf79+H1K5Cqdi4x\nIDy1tOzAzD5MDI67qrOT80aY2aZlAuTtvW3p6MVmth3waeBxYP8Qwitl5ttTaQ6/CyEcV4HxRESk\nilRvcCwiyThvby7Td0iFn7UZcRuYB0quT/L2H528fixxLcSdZQLjHb2/p54kZpn3M7OaEMLqzl7Q\nXXvsUMtMHdwhIrJR0YI8keq30NtJ+UUzO4K4PVqlfc3M1pZpmNlw4g4TANd28tqF3h7oO0ekMbYC\nfkoF/kIfQniduF3bKOD7ZlZaf42ZjTKz3Xv6LBER2fhUb+Y4LchrzcocRpacEtecnSQ3z/8c/oT/\ni2tNdjrdbK+LSIv18gV5ozzp1Orfyrzqsa4kIbUoK+ts9GfPyE7pTYsH3+YLBwdnr5/si/fneOnF\n89ncd/e53zbU55I9Z6rPuTGOaQtmr+0Ky5/wX6msosr9kLhLxK/N7GZiDe8ewJHATcCJFXzWEmL9\n8uNmdivxDMjjiYHoDzvbxi2E8IKZTQU+BDxmZncS65QPI+5D/BiwVwXmeQlxsd8ZxL2T7yZ+X7Yj\n1iIfQNzubU4FniUiIhuR6g2ORQSAEMJsMzsU+Crx4I/NgFnEwzaaqWxwvAp4N3ApMcAdQdz3+OvE\nbG1X/Je/5kTioSGNwK3AVyhfGrLefBeLY4GTiIv83kdcgNcIPAt8Gbihh4+pnzt3LhMmlN3MQkRE\nOjF37lyIC8f7lIXQ2foaEZHOmdlCgBBCff/OZMNgZiuJu2TM6u+5iLQjHVTzZL/OQqR9ewJvhBC6\nuqNSRShzLCLSOx6H9vdBFulv6XRHfUZlQ9XBCaS9SgvyREREREScgmMREREREaeyChGpCNUai4hI\nNVDmWERERETEKTgWEREREXHayk1ERERExClzLCIiIiLiFByLiIiIiDgFxyIiIiIiTsGxiIiIiIhT\ncCwiIiIi4hQci4iIiIg4BcciIiIiIk7BsYhIF5jZjmZ2jZk1mNlKM1toZt8zs2HrOc5wf91CH6fB\nx92xt+YuA0MlPqNmdq+ZhQ7+26I334NULzM73sx+YGYPmNky/zz9sptjVeTncXs2q8QgIiLVzMx2\nAh4CtgNuAZ4E9gU+AxxpZgeEEF7qwjjb+ji7AHcDU4HdgFOB95rZxBDCgt55F1LNKvUZzVzczvXX\nezRRGcguBPYEXgWeJ/7sW2+98Flfh4JjEZHO/ZD4g/jTIYQfpItm9l3gs8D/Amd0YZxLiYHxZSGE\nc7NxPg1c7s85soLzloGjUp9RAEIIUyo9QRnwPksMiucBhwD3dHOcin7Wy9Hx0SIiHTCzscB8YCGw\nUwhhTda3NbAEMGC7EMLyDsYZAjQCa4BRIYRXsr5N/Bn1/gxlj6XLKvUZ9fvvBQ4JIVivTVgGPDOb\nRAyObwghnLQer6vYZ70jqjkWEenYu7y9M/9BDOAB7oPAYGC/TsaZCGwJPJgHxj7OGuBO//LQHs9Y\nBppKfUbXMrMTzewCMzvXzI4ys80rN12Rbqv4Z70cBcciIh3b1dun2+l/xttd+mgckVK98dmaCnwN\n+A7wR+A5Mzu+e9MTqZg++Tmq4FhEpGO13ra005+uD+2jcURKVfKzdQtwNLAj8V86diMGyUOBG83s\nqB7MU6Sn+uTnqBbkiYj0TKrN7OkCjkqNI1Kqy5+tEMJlJZeeAr5kZg3AD4iLSqdVdnoiFVORn6PK\nHIuIdCxlImrb6d+m5L7eHkekVF98tq4mbuO2ly98EukPffJzVMGxiEjHnvK2vRq2nb1trwau0uOI\nlOr1z1YI4TUgLSQd0t1xRHqoT36OKjgWEelY2ovzcN9ybS3PoB0ArABmdDLODL/vgNLMm497eMnz\nRLqqUp/RdpnZrsAwYoDc1N1xRHqo1z/roOBYRKRDIYT5xG3W6oGzSrovJmbRfpHvqWlmu5lZm9Of\nQgivAtf7/VNKxjnbx79DexzL+qrUZ9TMxprZDqXjm9kI4Fr/cmoIQafkSa8ysxr/jO6UX+/OZ71b\nz9chICIiHStzXOlc4J3EPYmfBvbPjys1swBQepBCmeOj/waMB94PvOjjzO/t9yPVpxKfUTM7hVhb\nfB/xoIWlwGjgPcQaz0eAw0IIzb3/jqTamNmxwLH+5fbAEcAC4AG/1hRC+JzfWw88CywKIdSXjLNe\nn/VuzVXBsYhI58zsTcD/EI933pZ4EtPvgYtDCEtL7i0bHHvfcOAi4h8So4CXiKv/vxJCeL4334NU\nt55+Rs3srcB5wASgjri46RXgCeAm4MchhFW9/06kGpnZFOLPvvasDYQ7Co69v8uf9W7NVcGxiIiI\niEikmmMREREREafgWERERETEKTgWEREREXEKjkVEREREnIJjERERERGn4FhERERExCk4FhERERFx\nCo5FRERERJyCYxERERERp+BYRERERMQpOBYRERERcQqORUREREScgmMREREREafgWERERETEKTgW\nEREREXEKjkVEREREnIJjERERERH3/2BxqfZRWW6YAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xea037f0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
